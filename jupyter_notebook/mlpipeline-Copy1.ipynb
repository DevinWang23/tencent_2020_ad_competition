{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mlpipeline\n",
    "Data reformat , preprocess, feature engineering, train, evaluate model and generate submission \n",
    "- <a href='#1'>1. feature_engineering</a> \n",
    "- <a href='#2'>2. train</a> \n",
    "- <a href='#3'>3. predict</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "from time import time\n",
    "from datetime import timedelta, datetime\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from dinglingling import wx_reminder\n",
    "import torch\n",
    "\n",
    "sys.path.append('../')\n",
    "import conf\n",
    "from mlpipeline import (\n",
    "    feature_engineering_pandas,\n",
    "    train,\n",
    "    predict,\n",
    ")\n",
    "from utils import (\n",
    "    check_columns,\n",
    "    check_nan_value,\n",
    "    load_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.width',100)\n",
    "sns.set(rc={'figure.figsize':(20,10)})\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "warnings.simplefilter('ignore', np.RankWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "SCKEY = 'SCU92138T03d57ff9d4b08ced24c2cceb440cd3bd5e843242680de'  # used for reminding when feature engineering or model training completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def __dummy():\n",
    "    pass\n",
    "\n",
    "@wx_reminder(SCKEY=SCKEY, remind_started=True)  \n",
    "def feature_engineering_wrapper(params):\n",
    "        \"\"\"\n",
    "        wrapper for feature engineering func \n",
    "        for reminding when it completes\n",
    "        \"\"\"\n",
    "        train_fe_df, test_fe_df = feature_engineering_pandas(**params)\n",
    "        \n",
    "        return train_fe_df, test_fe_df\n",
    "    \n",
    "@wx_reminder(SCKEY=SCKEY, remind_started=True)  \n",
    "def train_wrapper(params):\n",
    "        if params['is_eval']:\n",
    "            _,_ = train(**params)\n",
    "        else:\n",
    "            if params['model_type'] == 'neural':\n",
    "                model = train(**params) \n",
    "                return model\n",
    "            else:\n",
    "                model, scaler = train(**params) \n",
    "                return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5G\t../data/ad_id_window_150_dim_128_sg_1_hs_0_iter_10_embedding.bin\r\n",
      "1.5G\t../data/ad_id_window_150_dim_128_sg_1_hs_0_iter_10_embedding.npy\r\n",
      "249M\t../data/ad_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_test_fe_df.feather\r\n",
      "244M\t../data/ad_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_train_fe_df.feather\r\n",
      "63M\t../data/ad_id_window_150_dim_128_sg_1_hs_0_iter_10_vocab.pkl\r\n",
      "29M\t../data/advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_embedding.bin\r\n",
      "29M\t../data/advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_embedding.npy\r\n",
      "195M\t../data/advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_test_fe_df.feather\r\n",
      "195M\t../data/advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_train_fe_df.feather\r\n",
      "1008K\t../data/advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_vocab.pkl\r\n",
      "437M\t../data/creative_advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_test_fe_df.feather\r\n",
      "412M\t../data/creative_advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_train_fe_df.feather\r\n",
      "1.7G\t../data/creative_id_window_150_dim_128_sg_1_hs_0_iter_10_embedding.bin\r\n",
      "1.7G\t../data/creative_id_window_150_dim_128_sg_1_hs_0_iter_10_embedding.npy\r\n",
      "250M\t../data/creative_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_test_fe_df.feather\r\n",
      "245M\t../data/creative_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_train_fe_df.feather\r\n",
      "71M\t../data/creative_id_window_150_dim_128_sg_1_hs_0_iter_10_vocab.pkl\r\n",
      "3.9G\t../data/creative_id_window_150_dim_300_sg_1_hs_0_iter_10_embedding.bin\r\n",
      "3.9G\t../data/creative_id_window_150_dim_300_sg_1_hs_0_iter_10_embedding.npy\r\n",
      "250M\t../data/creative_id_window_150_dim_300_sg_1_hs_0_iter_10_neural_test_fe_df.feather\r\n",
      "245M\t../data/creative_id_window_150_dim_300_sg_1_hs_0_iter_10_neural_train_fe_df.feather\r\n",
      "71M\t../data/creative_id_window_150_dim_300_sg_1_hs_0_iter_10_vocab.pkl\r\n",
      "3.9G\t../data/creative_id_window_150_dim_300_sg_1_hs_1_embedding.npy\r\n",
      "3.9G\t../data/creative_id_window_150_dim_300_sg_hs_embedding.bin\r\n",
      "71M\t../data/creative_id_window_150_dim_300_sg_hs_vocab.pkl\r\n",
      "5.2G\t../data/creative_id_window_150_dim_400_sg_1_hs_0_embedding.bin\r\n",
      "5.1G\t../data/creative_id_window_150_dim_400_sg_1_hs_0_embedding.npy\r\n",
      "250M\t../data/creative_id_window_150_dim_400_sg_1_hs_0_neural_test_fe_df.feather\r\n",
      "245M\t../data/creative_id_window_150_dim_400_sg_1_hs_0_neural_train_fe_df.feather\r\n",
      "71M\t../data/creative_id_window_150_dim_400_sg_1_hs_0_vocab.pkl\r\n",
      "413M\t../data/download\r\n",
      "28M\t../data/label_round_one_df.feather\r\n",
      "12K\t../data/product_category_window_150_dim_128_sg_1_hs_0_iter_10_embedding.bin\r\n",
      "12K\t../data/product_category_window_150_dim_128_sg_1_hs_0_iter_10_embedding.npy\r\n",
      "87M\t../data/product_category_window_150_dim_128_sg_1_hs_0_iter_10_neural_test_fe_df.feather\r\n",
      "99M\t../data/product_category_window_150_dim_128_sg_1_hs_0_iter_10_neural_train_fe_df.feather\r\n",
      "4.0K\t../data/product_category_window_150_dim_128_sg_1_hs_0_iter_10_vocab.pkl\r\n",
      "1.2G\t../data/raw_test_df.feather\r\n",
      "1.2G\t../data/raw_train_round_one_df.feather\r\n",
      "13G\t../data/tencent_2019_ad_competition_data\r\n",
      "732M\t../data/test\r\n",
      "2.3G\t../data/test_fe_df.feather\r\n",
      "2.1G\t../data/train_fe_df.feather\r\n",
      "637M\t../data/train_preliminary\r\n"
     ]
    }
   ],
   "source": [
    "! du -sh ../data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='1'> 1.feature_engineering</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-25 16:17:06,548 - mlpipeline.feature_engineering.feature_engineering - INFO - feature_engineering_pandas开始\n",
      "2020-05-25 16:17:06,550 - mlpipeline.feature_engineering.feature_engineering - INFO - is_train: True, is_neural_network: True\n",
      "2020-05-25 16:17:06,551 - mlpipeline.feature_engineering.feature_engineering - INFO - _load_preprocessed_data开始\n",
      "2020-05-25 16:17:07,702 - mlpipeline.feature_engineering.feature_engineering - INFO - _load_preprocessed_data已完成，共用时0:00:01\n",
      "2020-05-25 16:17:15,820 - mlpipeline.feature_engineering.feature_engineering - INFO - _generate_emb_for_sparse_feat开始\n",
      "../mlpipeline/feature_engineering/feature_engineering.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sparse_feat_seq_df[sparse_feat] = sparse_feat_seq_df[sparse_feat].astype(str)\n",
      "2020-05-25 16:23:07,078 - mlpipeline.feature_engineering.feature_engineering - INFO - sparse_feat: product_category, method: w2v, emd_dim: 128, window: 150, min_count: 1, workers: 20, sg: 1, hs: 0, smaple: 6e-05, negative: 0, alpha: 0.03, min_alpha: 0.0007, iter: 10\n",
      "2020-05-25 16:31:01,291 - mlpipeline.feature_engineering.feature_engineering - INFO - _generate_emb_for_sparse_feat已完成，共用时0:13:45\n",
      "2020-05-25 16:31:01,294 - mlpipeline.feature_engineering.feature_engineering - INFO - _build_vocab开始\n",
      "2020-05-25 16:32:26,697 - mlpipeline.feature_engineering.feature_engineering - INFO - ../data/product_category_window_150_dim_128_sg_1_hs_0_iter_10_vocab.pkl has been built\n",
      "2020-05-25 16:32:27,689 - mlpipeline.feature_engineering.feature_engineering - INFO - _build_vocab已完成，共用时0:01:26\n",
      "2020-05-25 16:32:27,692 - mlpipeline.feature_engineering.feature_engineering - INFO - _build_emb_matrix开始\n",
      "2020-05-25 16:32:27,695 - mlpipeline.feature_engineering.feature_engineering - INFO - product_category_window_150_dim_128_sg_1_hs_0_iter_10_embedding has been saved into ../data/product_category_window_150_dim_128_sg_1_hs_0_iter_10_embedding\n",
      "2020-05-25 16:32:27,696 - mlpipeline.feature_engineering.feature_engineering - INFO - _build_emb_matrix已完成，共用时0:00:00\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "2020-05-25 16:38:39,871 - mlpipeline.feature_engineering.feature_engineering - INFO - _get_label开始\n",
      "2020-05-25 16:38:40,313 - mlpipeline.feature_engineering.feature_engineering - INFO - _get_label已完成，共用时0:00:00\n",
      "2020-05-25 16:38:40,710 - mlpipeline.feature_engineering.feature_engineering - INFO - train_fe_df with shape (900000, 5) has been stored in ../data/product_category_window_150_dim_128_sg_1_hs_0_iter_10_neural_train_fe_df.feather\n",
      "2020-05-25 16:38:40,957 - mlpipeline.feature_engineering.feature_engineering - INFO - test_fe_df with shape (1000000, 2) has been stored in ../data/product_category_window_150_dim_128_sg_1_hs_0_iter_10_neural_test_fe_df.feather\n",
      "2020-05-25 16:38:41,332 - mlpipeline.feature_engineering.feature_engineering - INFO - feature_engineering_pandas已完成，共用时0:21:35\n"
     ]
    }
   ],
   "source": [
    "# feature engineering\n",
    "params = {\n",
    "    'train_preprocessed_data_filename':'raw_train_round_one_df.feather', \n",
    "    'test_preprocessed_data_filename':'raw_test_df.feather', \n",
    "    'train_fe_save_filename': 'neural_train_fe_df.feather',\n",
    "    'test_fe_save_filename': 'neural_test_fe_df.feather',\n",
    "    'emb_method':'w2v',\n",
    "    'max_df':0.9,  # param for tf_idf\n",
    "    'min_df':3,  # param for tf_idf\n",
    "    'emb_dim':128,  \n",
    "    'window':150,  \n",
    "    'sparse_feat': 'product_category',  # advertise_id, product_category\n",
    "    'min_count':1, \n",
    "#     'sample':6e-5, \n",
    "#     'negative':0,  \n",
    "    'hs':0, \n",
    "#     'alpha':0.03,\n",
    "#     'min_alpha':0.0007,\n",
    "    'iter_':10,\n",
    "    'workers':20,\n",
    "    'sg':1,\n",
    "    'num_processes': 40,\n",
    "    'is_train':True,\n",
    "    'is_neural_network':True\n",
    "}\n",
    "\n",
    "train_fe_df, test_fe_df = feature_engineering_wrapper(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat train features by user_id\n",
    "creative_id_train_fe_df = pd.read_feather(os.path.join(conf.DATA_DIR, 'creative_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_train_fe_df.feather'))\n",
    "advertiser_id_train_fe_df = pd.read_feather(os.path.join(conf.DATA_DIR, 'advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_train_fe_df.feather'))\n",
    "creative_advertiser_id_train_fe_df = creative_id_fe_df.merge(advertiser_id_fe_df[['user_id','advertiser_id']],how='left',on='user_id')\n",
    "creative_advertiser_id_train_fe_df.to_feather(os.path.join(conf.DATA_DIR, 'creative_advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_train_fe_df.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat test features by user_id\n",
    "creative_id_test_fe_df = pd.read_feather(os.path.join(conf.DATA_DIR, 'creative_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_test_fe_df.feather'))\n",
    "advertiser_id_test_fe_df = pd.read_feather(os.path.join(conf.DATA_DIR, 'advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_test_fe_df.feather'))\n",
    "creative_advertiser_id_test_fe_df = creative_id_test_fe_df.merge(advertiser_id_test_fe_df[['user_id','advertiser_id']],how='left',on='user_id')\n",
    "creative_advertiser_id_test_fe_df.to_feather(os.path.join(conf.DATA_DIR, 'creative_advertiser_id_window_150_dim_128_sg_1_hs_0_iter_10_neural_test_fe_df.feather')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-23 22:08:34,450 - mlpipeline.train - INFO - train开始\n",
      "2020-05-23 22:08:34,452 - mlpipeline.train - INFO - using_fe_df: neural_train_fe_df.feather, use_label: age, is_eval: True, model_type: neural, model_name: bilstm_attention, use_log: False, use_std: False, use_cv: True, n_splits: 2\n",
      "2020-05-23 22:08:35,210 - mlpipeline.train - INFO - _train_pipeline_neural开始\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:576: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "2020-05-23 22:08:42,254 - mlpipeline.train - INFO - 模型参数: {'model_name': 'bilstm_attention', 'num_classes': 10, 'sparse_feat': 'creative_id', 'embed': 'embedding_creative_id_window_150_dim_300_sg_hs_w2v.npy', 'vocab_path': 'creative_id_window_150_dim_300_sg_hs_w2v_vocab.pkl', 'dropout': 0.3, 'required_improvement': 1000, 'num_epochs': 20, 'batch_size': 128, 'learning_rate': 0.001, 'hidden_size': 256, 'use_pad': True, 'max_seq_len': 110, 'seed': 1234, 'init_method': 'xavier', 'num_layers': 1, 'bidirectional': True}\n",
      "2020-05-23 22:08:42,257 - mlpipeline.train - INFO - _get_cv_folds开始\n",
      "2020-05-23 22:08:42,258 - mlpipeline.train - INFO - _get_cv_folds已完成，共用时0:00:00\n",
      "2020-05-23 22:08:42,603 - utils.utils - INFO - build_dataset开始\n",
      "2020-05-23 22:08:44,408 - utils.utils - INFO - ../data/creative_id_window_150_dim_300_sg_hs_w2v_vocab.pkl has been loaded\n",
      "2020-05-23 22:09:46,375 - utils.utils - INFO - build_dataset已完成，共用时0:01:04\n",
      "2020-05-23 22:09:46,377 - mlpipeline.train - INFO - Loading data...\n",
      "2020-05-23 22:09:46,378 - utils.utils - INFO - build_iterater开始\n",
      "2020-05-23 22:09:46,379 - utils.utils - INFO - build_iterater已完成，共用时0:00:00\n",
      "2020-05-23 22:09:46,380 - utils.utils - INFO - build_iterater开始\n",
      "2020-05-23 22:09:46,380 - utils.utils - INFO - build_iterater已完成，共用时0:00:00\n",
      "2020-05-23 22:09:46,381 - mlpipeline.train - INFO - Time usage:0:00:00\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "2020-05-23 22:09:51,640 - utils.utils - INFO - init_network开始\n",
      "2020-05-23 22:09:51,642 - utils.utils - INFO - init_network已完成，共用时0:00:00\n",
      "2020-05-23 22:09:51,643 - utils.utils - INFO - neural_train开始\n",
      "2020-05-23 22:09:51,646 - utils.utils - INFO - Epoch [1/20]\n",
      "2020-05-23 22:10:57,139 - utils.utils - INFO - Iter:      0,  Train Loss:   2.2,  Train Acc: 14.06%,  Val Loss:   2.3,  Val Acc: 11.48%,  Time: 0:01:05 *\n",
      "2020-05-23 22:12:09,310 - utils.utils - INFO - Iter:    100,  Train Loss:   1.6,  Train Acc: 30.47%,  Val Loss:   1.6,  Val Acc: 34.38%,  Time: 0:02:18 *\n",
      "2020-05-23 22:13:20,926 - utils.utils - INFO - Iter:    200,  Train Loss:   1.6,  Train Acc: 38.28%,  Val Loss:   1.5,  Val Acc: 37.93%,  Time: 0:03:29 *\n",
      "2020-05-23 22:14:31,782 - utils.utils - INFO - Iter:    300,  Train Loss:   1.5,  Train Acc: 39.06%,  Val Loss:   1.5,  Val Acc: 38.60%,  Time: 0:04:40 *\n",
      "2020-05-23 22:15:42,664 - utils.utils - INFO - Iter:    400,  Train Loss:   1.4,  Train Acc: 44.53%,  Val Loss:   1.5,  Val Acc: 40.10%,  Time: 0:05:51 *\n",
      "2020-05-23 22:16:57,193 - utils.utils - INFO - Iter:    500,  Train Loss:   1.6,  Train Acc: 28.91%,  Val Loss:   1.5,  Val Acc: 40.64%,  Time: 0:07:06 *\n"
     ]
    }
   ],
   "source": [
    "# eval \n",
    "# lgb_model_params = {\n",
    "#                'objective': 'multiclass',  # multiclass, binary \n",
    "#                'boosting': 'gbdt',\n",
    "#                'learning_rate': 0.15,\n",
    "#                'metric': ['multi_logloss'],  # 'binary_logloss', 'multi_logloss'\n",
    "#                'num_threads': 20,\n",
    "#                'random_state': 2019,\n",
    "#                'num_boost_round': 1000,\n",
    "#                'device': 'cpu',\n",
    "#                'num_class':20,  # 2, 20 ,10\n",
    "#                'num_leaves':32,  # [16,32,64,128]\n",
    "#                'subsample': 0.9,  # [0.7,0.8,0.9,1]\n",
    "#                'colsample_bytree': 0.9, # [0.2,0.3,0.4,0.5,0.6]\n",
    "#                'min_data_in_leaf': 40, # [20,40,60,80,100]\n",
    "#                'lambda_l1': 1.0,  # (0.2,3)\n",
    "#                'lambda_l2': 1.0,  # (0.2,3)\n",
    "# }\n",
    "\n",
    "# 0.3903862863136468, 0.39146038751369455\n",
    "# xgb_model_params = {\n",
    "#                'objective': 'multi:softmax',  # multiclass, binary \n",
    "#                'booster': 'gbtree',\n",
    "#                'eta': 0.15,\n",
    "#                'eval_metric': ['mlogloss'],  # 'binary_logloss', 'multi_logloss'\n",
    "#                'nthread': 15,\n",
    "#                'random_state': 2019,\n",
    "#                'tree_method':'auto',\n",
    "#                'n_estimators': 2,\n",
    "#                'device': 'cpu',\n",
    "#                'num_class':20,  # 2, 20 ,10\n",
    "#                'max_leaves':32,  # [16,32,64,128]\n",
    "#                'subsample': 0.9,  # [0.7,0.8,0.9,1]\n",
    "#                'colsample_bytree': 0.9, # [0.2,0.3,0.4,0.5,0.6]\n",
    "#                'min_data_in_leaf': 40, # [20,40,60,80,100]\n",
    "#                'reg_lambda': 1.0,  # (0.2,3)\n",
    "#                'reg_alpha': 1.0,  # (0.2,3)\n",
    "# }\n",
    "lstm_model_params ={\n",
    "    'model_name':'lstm', \n",
    "    'num_classes':20, \n",
    "    'sparse_feat':'creative_id', \n",
    "    'embed':'embedding_creative_id_window_150_dim_300_sg_hs_w2v.npy',\n",
    "    'dropout':0.2,\n",
    "    'required_improvement':1000,\n",
    "    'num_epochs':3,\n",
    "    'batch_size':128,\n",
    "    'learning_rate':1e-3,\n",
    "    'hidden_size':128,\n",
    "    'use_pad':True,\n",
    "    'max_seq_len':110,\n",
    "    'seed':1234,\n",
    "    'init_method':'xavier' , # 'kaiming'\n",
    "    'num_layers' : 1,\n",
    "    'bidirectional':False\n",
    "}\n",
    "\n",
    "# textcnn_model_params={\n",
    "#     'model_name':'textcnn', \n",
    "#     'num_classes':20, \n",
    "#     'sparse_feat':'creative_id', \n",
    "#     'embed':'embedding_creative_id_300.npy',\n",
    "#     'dropout':0.2,\n",
    "#     'required_improvement':1000,\n",
    "#     'num_epochs':20,\n",
    "#     'batch_size':128,\n",
    "#     'learning_rate':1e-3,\n",
    "#     'filter_size':[3,5,7],\n",
    "#     'num_filters':3,\n",
    "#     'use_pad':True,\n",
    "#     'pad_size':64,\n",
    "# }\n",
    "\n",
    "# transformer_model_params = {\n",
    "#     'model_name':'transformer',\n",
    "#     'num_classes':20,\n",
    "#     'sparse_feat':'creative_id', \n",
    "#     'embed':'embedding_creative_id_300.npy',\n",
    "#     'dropout':0.2,\n",
    "#     'required_improvement':1000,\n",
    "#     'num_epochs':5,\n",
    "#     'batch_size':128,\n",
    "#     'learning_rate':1e-3,\n",
    "#     'dim_model':300,\n",
    "#     'hidden':1024,\n",
    "#     'last_hidden':512,\n",
    "#     'num_head':5,\n",
    "#     'init_method':'kaiming',\n",
    "#     'num_encoder':2,\n",
    "#     'use_pad':True,\n",
    "#     'seed':1,\n",
    "#     'pad_size':64,\n",
    "# }\n",
    "bilstm_attention_model_params = {\n",
    "    'model_name':'bilstm_attention',\n",
    "    'num_classes':10,\n",
    "    'sparse_feat':'creative_id', \n",
    "    'embed':'embedding_creative_id_window_150_dim_300_sg_hs_w2v.npy',\n",
    "    'vocab_path':'creative_id_window_150_dim_300_sg_hs_w2v_vocab.pkl',\n",
    "    'dropout':0.3,\n",
    "    'required_improvement':1000,\n",
    "    'num_epochs':20,\n",
    "    'batch_size':128,\n",
    "    'learning_rate':1e-3,\n",
    "    'hidden_size':256,\n",
    "    'use_pad':True,\n",
    "    'max_seq_len':110,\n",
    "    'seed':1234,\n",
    "    'init_method':'xavier' , # 'kaiming','xavier'\n",
    "    'num_layers' : 1,\n",
    "#     'attention_size':256,\n",
    "    'bidirectional':True\n",
    "}\n",
    "\n",
    "lstm_attention_model_params ={\n",
    "    'model_name':'lstm_attention',\n",
    "    'num_classes':20,\n",
    "    'sparse_feat':'creative_id', \n",
    "    'embed':'embedding_creative_id_300.npy',\n",
    "    'dropout':0.2,\n",
    "    'required_improvement':1000,\n",
    "    'num_epochs':3,\n",
    "    'batch_size':1,\n",
    "    'learning_rate':1e-3,\n",
    "    'hidden_size':256,\n",
    "    'use_pad':True,\n",
    "    'seed':1234,\n",
    "    'init_method':'kaiming' , # 'kaiming', 'xavier'\n",
    "    'num_layers' : 1,\n",
    "    'max_seq_len':90,\n",
    "    'bidirectional':False\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'fe_filename':'neural_train_fe_df.feather', \n",
    "    'is_eval':True, \n",
    "    'model_type': 'neural',\n",
    "    'model_name': 'bilstm_attention',\n",
    "    'model_params': bilstm_attention_model_params,\n",
    "    'use_log': False,\n",
    "    'use_std': False,\n",
    "    'use_cv': True,  \n",
    "    'n_splits':2,\n",
    "}\n",
    "train_wrapper(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-23 09:26:41,157 - mlpipeline.train - INFO - train开始\n",
      "2020-05-23 09:26:41,159 - mlpipeline.train - INFO - using_fe_df: neural_train_fe_df.feather, use_label: y, is_eval: False, model_type: neural, model_name: bilstm_attention, use_log: False, use_std: False, use_cv: False, n_splits: 2\n",
      "2020-05-23 09:26:42,109 - mlpipeline.train - INFO - _train_pipeline_neural开始\n",
      "2020-05-23 09:26:49,944 - mlpipeline.train - INFO - 模型参数: {'model_name': 'bilstm_attention', 'num_classes': 20, 'sparse_feat': 'creative_id', 'embed': 'embedding_creative_id_300.npy', 'dropout': 0.3, 'required_improvement': 1000, 'num_epochs': 4, 'batch_size': 256, 'learning_rate': 0.001, 'hidden_size': 256, 'use_pad': True, 'max_seq_len': 110, 'seed': 1234, 'init_method': 'xavier', 'num_layers': 1, 'bidirectional': True}\n",
      "2020-05-23 09:26:50,000 - utils.utils - INFO - build_dataset开始\n",
      "2020-05-23 09:26:52,080 - utils.utils - INFO - ../data/creative_id_vocab.pkl has been loaded\n",
      "2020-05-23 09:27:56,043 - utils.utils - INFO - build_dataset已完成，共用时0:01:06\n",
      "2020-05-23 09:27:56,044 - mlpipeline.train - INFO - Loading data...\n",
      "2020-05-23 09:27:56,045 - utils.utils - INFO - build_iterater开始\n",
      "2020-05-23 09:27:56,046 - utils.utils - INFO - build_iterater已完成，共用时0:00:00\n",
      "2020-05-23 09:27:56,047 - mlpipeline.train - INFO - Time usage:0:00:00\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "2020-05-23 09:28:01,253 - utils.utils - INFO - init_network开始\n",
      "2020-05-23 09:28:01,255 - utils.utils - INFO - init_network已完成，共用时0:00:00\n",
      "2020-05-23 09:28:01,256 - utils.utils - INFO - neural_train开始\n",
      "2020-05-23 09:28:01,259 - utils.utils - INFO - Epoch [1/4]\n",
      "2020-05-23 09:28:01,340 - utils.utils - INFO - Iter:      0,  Train Loss:   3.0,  Train Acc:  2.34%,Time: 0:00:00\n",
      "2020-05-23 09:28:06,891 - utils.utils - INFO - Iter:    100,  Train Loss:   1.8,  Train Acc: 33.20%,Time: 0:00:06\n",
      "2020-05-23 09:28:12,400 - utils.utils - INFO - Iter:    200,  Train Loss:   1.7,  Train Acc: 35.55%,Time: 0:00:11\n",
      "2020-05-23 09:28:17,876 - utils.utils - INFO - Iter:    300,  Train Loss:   1.8,  Train Acc: 33.59%,Time: 0:00:17\n",
      "2020-05-23 09:28:23,334 - utils.utils - INFO - Iter:    400,  Train Loss:   1.6,  Train Acc: 31.64%,Time: 0:00:22\n",
      "2020-05-23 09:28:28,799 - utils.utils - INFO - Iter:    500,  Train Loss:   1.7,  Train Acc: 38.67%,Time: 0:00:28\n",
      "2020-05-23 09:28:34,277 - utils.utils - INFO - Iter:    600,  Train Loss:   1.7,  Train Acc: 35.55%,Time: 0:00:33\n",
      "2020-05-23 09:28:39,781 - utils.utils - INFO - Iter:    700,  Train Loss:   1.6,  Train Acc: 41.80%,Time: 0:00:39\n",
      "2020-05-23 09:28:45,264 - utils.utils - INFO - Iter:    800,  Train Loss:   1.6,  Train Acc: 41.80%,Time: 0:00:44\n",
      "2020-05-23 09:28:50,726 - utils.utils - INFO - Iter:    900,  Train Loss:   1.7,  Train Acc: 41.02%,Time: 0:00:49\n",
      "2020-05-23 09:28:56,245 - utils.utils - INFO - Iter:   1000,  Train Loss:   1.6,  Train Acc: 36.33%,Time: 0:00:55\n",
      "2020-05-23 09:29:01,714 - utils.utils - INFO - Iter:   1100,  Train Loss:   1.6,  Train Acc: 35.94%,Time: 0:01:00\n",
      "2020-05-23 09:29:07,492 - utils.utils - INFO - Iter:   1200,  Train Loss:   1.6,  Train Acc: 46.09%,Time: 0:01:06\n",
      "2020-05-23 09:29:13,416 - utils.utils - INFO - Iter:   1300,  Train Loss:   1.7,  Train Acc: 39.06%,Time: 0:01:12\n",
      "2020-05-23 09:29:19,276 - utils.utils - INFO - Iter:   1400,  Train Loss:   1.5,  Train Acc: 38.28%,Time: 0:01:18\n",
      "2020-05-23 09:29:25,205 - utils.utils - INFO - Iter:   1500,  Train Loss:   1.6,  Train Acc: 39.45%,Time: 0:01:24\n",
      "2020-05-23 09:29:31,058 - utils.utils - INFO - Iter:   1600,  Train Loss:   1.6,  Train Acc: 40.62%,Time: 0:01:30\n",
      "2020-05-23 09:29:36,757 - utils.utils - INFO - Iter:   1700,  Train Loss:   1.5,  Train Acc: 46.09%,Time: 0:01:36\n",
      "2020-05-23 09:29:42,219 - utils.utils - INFO - Iter:   1800,  Train Loss:   1.6,  Train Acc: 38.67%,Time: 0:01:41\n",
      "2020-05-23 09:29:47,678 - utils.utils - INFO - Iter:   1900,  Train Loss:   1.5,  Train Acc: 42.58%,Time: 0:01:46\n",
      "2020-05-23 09:29:53,061 - utils.utils - INFO - Iter:   2000,  Train Loss:   1.7,  Train Acc: 34.77%,Time: 0:01:52\n",
      "2020-05-23 09:29:58,555 - utils.utils - INFO - Iter:   2100,  Train Loss:   1.6,  Train Acc: 42.97%,Time: 0:01:57\n",
      "2020-05-23 09:30:04,034 - utils.utils - INFO - Iter:   2200,  Train Loss:   1.7,  Train Acc: 37.89%,Time: 0:02:03\n",
      "2020-05-23 09:30:09,494 - utils.utils - INFO - Iter:   2300,  Train Loss:   1.6,  Train Acc: 43.75%,Time: 0:02:08\n",
      "2020-05-23 09:30:14,949 - utils.utils - INFO - Iter:   2400,  Train Loss:   1.5,  Train Acc: 40.23%,Time: 0:02:14\n",
      "2020-05-23 09:30:20,381 - utils.utils - INFO - Iter:   2500,  Train Loss:   1.6,  Train Acc: 40.62%,Time: 0:02:19\n",
      "2020-05-23 09:30:25,850 - utils.utils - INFO - Iter:   2600,  Train Loss:   1.5,  Train Acc: 45.70%,Time: 0:02:25\n",
      "2020-05-23 09:30:31,321 - utils.utils - INFO - Iter:   2700,  Train Loss:   1.5,  Train Acc: 44.92%,Time: 0:02:30\n",
      "2020-05-23 09:30:36,839 - utils.utils - INFO - Iter:   2800,  Train Loss:   1.5,  Train Acc: 39.84%,Time: 0:02:36\n",
      "2020-05-23 09:30:42,305 - utils.utils - INFO - Iter:   2900,  Train Loss:   1.5,  Train Acc: 37.11%,Time: 0:02:41\n",
      "2020-05-23 09:30:47,744 - utils.utils - INFO - Iter:   3000,  Train Loss:   1.6,  Train Acc: 39.84%,Time: 0:02:46\n",
      "2020-05-23 09:30:53,184 - utils.utils - INFO - Iter:   3100,  Train Loss:   1.6,  Train Acc: 40.62%,Time: 0:02:52\n",
      "2020-05-23 09:30:58,688 - utils.utils - INFO - Iter:   3200,  Train Loss:   1.5,  Train Acc: 39.84%,Time: 0:02:57\n",
      "2020-05-23 09:31:04,108 - utils.utils - INFO - Iter:   3300,  Train Loss:   1.4,  Train Acc: 44.14%,Time: 0:03:03\n",
      "2020-05-23 09:31:09,573 - utils.utils - INFO - Iter:   3400,  Train Loss:   1.6,  Train Acc: 42.97%,Time: 0:03:08\n",
      "2020-05-23 09:31:15,088 - utils.utils - INFO - Iter:   3500,  Train Loss:   1.6,  Train Acc: 40.62%,Time: 0:03:14\n",
      "2020-05-23 09:31:15,887 - utils.utils - INFO - Epoch [2/4]\n",
      "2020-05-23 09:31:20,585 - utils.utils - INFO - Iter:   3600,  Train Loss:   1.5,  Train Acc: 39.06%,Time: 0:03:19\n",
      "2020-05-23 09:31:26,090 - utils.utils - INFO - Iter:   3700,  Train Loss:   1.7,  Train Acc: 37.11%,Time: 0:03:25\n",
      "2020-05-23 09:31:31,615 - utils.utils - INFO - Iter:   3800,  Train Loss:   1.4,  Train Acc: 49.22%,Time: 0:03:30\n",
      "2020-05-23 09:31:37,157 - utils.utils - INFO - Iter:   3900,  Train Loss:   1.6,  Train Acc: 34.38%,Time: 0:03:36\n",
      "2020-05-23 09:31:42,561 - utils.utils - INFO - Iter:   4000,  Train Loss:   1.5,  Train Acc: 42.97%,Time: 0:03:41\n",
      "2020-05-23 09:31:48,076 - utils.utils - INFO - Iter:   4100,  Train Loss:   1.5,  Train Acc: 42.19%,Time: 0:03:47\n",
      "2020-05-23 09:31:53,540 - utils.utils - INFO - Iter:   4200,  Train Loss:   1.4,  Train Acc: 41.41%,Time: 0:03:52\n",
      "2020-05-23 09:31:59,059 - utils.utils - INFO - Iter:   4300,  Train Loss:   1.5,  Train Acc: 39.84%,Time: 0:03:58\n",
      "2020-05-23 09:32:04,455 - utils.utils - INFO - Iter:   4400,  Train Loss:   1.4,  Train Acc: 40.23%,Time: 0:04:03\n",
      "2020-05-23 09:32:09,916 - utils.utils - INFO - Iter:   4500,  Train Loss:   1.6,  Train Acc: 42.19%,Time: 0:04:09\n",
      "2020-05-23 09:32:15,363 - utils.utils - INFO - Iter:   4600,  Train Loss:   1.5,  Train Acc: 42.58%,Time: 0:04:14\n",
      "2020-05-23 09:32:20,820 - utils.utils - INFO - Iter:   4700,  Train Loss:   1.4,  Train Acc: 40.62%,Time: 0:04:20\n",
      "2020-05-23 09:32:26,271 - utils.utils - INFO - Iter:   4800,  Train Loss:   1.5,  Train Acc: 48.44%,Time: 0:04:25\n",
      "2020-05-23 09:32:31,728 - utils.utils - INFO - Iter:   4900,  Train Loss:   1.4,  Train Acc: 37.89%,Time: 0:04:30\n",
      "2020-05-23 09:32:37,194 - utils.utils - INFO - Iter:   5000,  Train Loss:   1.6,  Train Acc: 35.16%,Time: 0:04:36\n",
      "2020-05-23 09:32:42,629 - utils.utils - INFO - Iter:   5100,  Train Loss:   1.6,  Train Acc: 41.41%,Time: 0:04:41\n",
      "2020-05-23 09:32:48,054 - utils.utils - INFO - Iter:   5200,  Train Loss:   1.7,  Train Acc: 37.50%,Time: 0:04:47\n",
      "2020-05-23 09:32:53,564 - utils.utils - INFO - Iter:   5300,  Train Loss:   1.4,  Train Acc: 44.14%,Time: 0:04:52\n",
      "2020-05-23 09:32:59,006 - utils.utils - INFO - Iter:   5400,  Train Loss:   1.5,  Train Acc: 43.36%,Time: 0:04:58\n",
      "2020-05-23 09:33:04,414 - utils.utils - INFO - Iter:   5500,  Train Loss:   1.5,  Train Acc: 38.28%,Time: 0:05:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-23 09:33:09,836 - utils.utils - INFO - Iter:   5600,  Train Loss:   1.4,  Train Acc: 41.02%,Time: 0:05:09\n",
      "2020-05-23 09:33:15,291 - utils.utils - INFO - Iter:   5700,  Train Loss:   1.6,  Train Acc: 42.19%,Time: 0:05:14\n",
      "2020-05-23 09:33:20,755 - utils.utils - INFO - Iter:   5800,  Train Loss:   1.4,  Train Acc: 45.70%,Time: 0:05:19\n",
      "2020-05-23 09:33:26,178 - utils.utils - INFO - Iter:   5900,  Train Loss:   1.6,  Train Acc: 39.45%,Time: 0:05:25\n",
      "2020-05-23 09:33:31,638 - utils.utils - INFO - Iter:   6000,  Train Loss:   1.5,  Train Acc: 42.19%,Time: 0:05:30\n",
      "2020-05-23 09:33:37,108 - utils.utils - INFO - Iter:   6100,  Train Loss:   1.5,  Train Acc: 42.19%,Time: 0:05:36\n",
      "2020-05-23 09:33:42,566 - utils.utils - INFO - Iter:   6200,  Train Loss:   1.4,  Train Acc: 41.02%,Time: 0:05:41\n",
      "2020-05-23 09:33:48,130 - utils.utils - INFO - Iter:   6300,  Train Loss:   1.5,  Train Acc: 44.14%,Time: 0:05:47\n",
      "2020-05-23 09:33:53,954 - utils.utils - INFO - Iter:   6400,  Train Loss:   1.5,  Train Acc: 36.72%,Time: 0:05:53\n",
      "2020-05-23 09:33:59,788 - utils.utils - INFO - Iter:   6500,  Train Loss:   1.5,  Train Acc: 42.97%,Time: 0:05:59\n",
      "2020-05-23 09:34:05,609 - utils.utils - INFO - Iter:   6600,  Train Loss:   1.5,  Train Acc: 42.97%,Time: 0:06:04\n",
      "2020-05-23 09:34:11,371 - utils.utils - INFO - Iter:   6700,  Train Loss:   1.6,  Train Acc: 39.84%,Time: 0:06:10\n",
      "2020-05-23 09:34:17,148 - utils.utils - INFO - Iter:   6800,  Train Loss:   1.5,  Train Acc: 46.09%,Time: 0:06:16\n",
      "2020-05-23 09:34:22,945 - utils.utils - INFO - Iter:   6900,  Train Loss:   1.5,  Train Acc: 38.67%,Time: 0:06:22\n",
      "2020-05-23 09:34:28,774 - utils.utils - INFO - Iter:   7000,  Train Loss:   1.4,  Train Acc: 49.22%,Time: 0:06:28\n",
      "2020-05-23 09:34:30,547 - utils.utils - INFO - Epoch [3/4]\n",
      "2020-05-23 09:34:34,543 - utils.utils - INFO - Iter:   7100,  Train Loss:   1.5,  Train Acc: 44.53%,Time: 0:06:33\n",
      "2020-05-23 09:34:40,451 - utils.utils - INFO - Iter:   7200,  Train Loss:   1.7,  Train Acc: 37.11%,Time: 0:06:39\n",
      "2020-05-23 09:34:46,336 - utils.utils - INFO - Iter:   7300,  Train Loss:   1.4,  Train Acc: 47.66%,Time: 0:06:45\n",
      "2020-05-23 09:34:52,134 - utils.utils - INFO - Iter:   7400,  Train Loss:   1.4,  Train Acc: 49.22%,Time: 0:06:51\n",
      "2020-05-23 09:34:57,911 - utils.utils - INFO - Iter:   7500,  Train Loss:   1.5,  Train Acc: 44.53%,Time: 0:06:57\n",
      "2020-05-23 09:35:03,693 - utils.utils - INFO - Iter:   7600,  Train Loss:   1.6,  Train Acc: 41.02%,Time: 0:07:02\n",
      "2020-05-23 09:35:09,212 - utils.utils - INFO - Iter:   7700,  Train Loss:   1.5,  Train Acc: 42.58%,Time: 0:07:08\n",
      "2020-05-23 09:35:14,665 - utils.utils - INFO - Iter:   7800,  Train Loss:   1.5,  Train Acc: 44.14%,Time: 0:07:13\n",
      "2020-05-23 09:35:20,097 - utils.utils - INFO - Iter:   7900,  Train Loss:   1.4,  Train Acc: 44.53%,Time: 0:07:19\n",
      "2020-05-23 09:35:25,589 - utils.utils - INFO - Iter:   8000,  Train Loss:   1.5,  Train Acc: 44.92%,Time: 0:07:24\n",
      "2020-05-23 09:35:31,021 - utils.utils - INFO - Iter:   8100,  Train Loss:   1.5,  Train Acc: 44.14%,Time: 0:07:30\n",
      "2020-05-23 09:35:36,477 - utils.utils - INFO - Iter:   8200,  Train Loss:   1.5,  Train Acc: 38.28%,Time: 0:07:35\n",
      "2020-05-23 09:35:41,905 - utils.utils - INFO - Iter:   8300,  Train Loss:   1.6,  Train Acc: 41.02%,Time: 0:07:41\n",
      "2020-05-23 09:35:47,614 - utils.utils - INFO - Iter:   8400,  Train Loss:   1.5,  Train Acc: 44.92%,Time: 0:07:46\n",
      "2020-05-23 09:35:53,116 - utils.utils - INFO - Iter:   8500,  Train Loss:   1.6,  Train Acc: 35.16%,Time: 0:07:52\n",
      "2020-05-23 09:35:58,584 - utils.utils - INFO - Iter:   8600,  Train Loss:   1.5,  Train Acc: 41.41%,Time: 0:07:57\n",
      "2020-05-23 09:36:04,015 - utils.utils - INFO - Iter:   8700,  Train Loss:   1.5,  Train Acc: 41.41%,Time: 0:08:03\n",
      "2020-05-23 09:36:09,522 - utils.utils - INFO - Iter:   8800,  Train Loss:   1.5,  Train Acc: 46.88%,Time: 0:08:08\n",
      "2020-05-23 09:36:15,002 - utils.utils - INFO - Iter:   8900,  Train Loss:   1.5,  Train Acc: 43.75%,Time: 0:08:14\n",
      "2020-05-23 09:36:20,468 - utils.utils - INFO - Iter:   9000,  Train Loss:   1.4,  Train Acc: 43.75%,Time: 0:08:19\n",
      "2020-05-23 09:36:25,962 - utils.utils - INFO - Iter:   9100,  Train Loss:   1.4,  Train Acc: 47.66%,Time: 0:08:25\n",
      "2020-05-23 09:36:31,546 - utils.utils - INFO - Iter:   9200,  Train Loss:   1.4,  Train Acc: 46.48%,Time: 0:08:30\n",
      "2020-05-23 09:36:37,414 - utils.utils - INFO - Iter:   9300,  Train Loss:   1.5,  Train Acc: 48.05%,Time: 0:08:36\n",
      "2020-05-23 09:36:43,213 - utils.utils - INFO - Iter:   9400,  Train Loss:   1.5,  Train Acc: 43.36%,Time: 0:08:42\n",
      "2020-05-23 09:36:49,013 - utils.utils - INFO - Iter:   9500,  Train Loss:   1.4,  Train Acc: 44.53%,Time: 0:08:48\n",
      "2020-05-23 09:36:54,869 - utils.utils - INFO - Iter:   9600,  Train Loss:   1.3,  Train Acc: 43.36%,Time: 0:08:54\n",
      "2020-05-23 09:37:00,750 - utils.utils - INFO - Iter:   9700,  Train Loss:   1.4,  Train Acc: 44.53%,Time: 0:08:59\n",
      "2020-05-23 09:37:06,639 - utils.utils - INFO - Iter:   9800,  Train Loss:   1.5,  Train Acc: 40.62%,Time: 0:09:05\n",
      "2020-05-23 09:37:12,534 - utils.utils - INFO - Iter:   9900,  Train Loss:   1.6,  Train Acc: 40.62%,Time: 0:09:11\n",
      "2020-05-23 09:37:18,260 - utils.utils - INFO - Iter:  10000,  Train Loss:   1.6,  Train Acc: 39.84%,Time: 0:09:17\n",
      "2020-05-23 09:37:23,763 - utils.utils - INFO - Iter:  10100,  Train Loss:   1.6,  Train Acc: 42.19%,Time: 0:09:23\n",
      "2020-05-23 09:37:29,219 - utils.utils - INFO - Iter:  10200,  Train Loss:   1.4,  Train Acc: 44.53%,Time: 0:09:28\n",
      "2020-05-23 09:37:35,061 - utils.utils - INFO - Iter:  10300,  Train Loss:   1.5,  Train Acc: 42.97%,Time: 0:09:34\n",
      "2020-05-23 09:37:40,928 - utils.utils - INFO - Iter:  10400,  Train Loss:   1.4,  Train Acc: 44.14%,Time: 0:09:40\n",
      "2020-05-23 09:37:46,701 - utils.utils - INFO - Iter:  10500,  Train Loss:   1.5,  Train Acc: 48.83%,Time: 0:09:45\n",
      "2020-05-23 09:37:49,431 - utils.utils - INFO - Epoch [4/4]\n",
      "2020-05-23 09:37:52,517 - utils.utils - INFO - Iter:  10600,  Train Loss:   1.5,  Train Acc: 43.36%,Time: 0:09:51\n",
      "2020-05-23 09:37:58,104 - utils.utils - INFO - Iter:  10700,  Train Loss:   1.4,  Train Acc: 45.70%,Time: 0:09:57\n",
      "2020-05-23 09:38:03,569 - utils.utils - INFO - Iter:  10800,  Train Loss:   1.4,  Train Acc: 46.48%,Time: 0:10:02\n",
      "2020-05-23 09:38:09,027 - utils.utils - INFO - Iter:  10900,  Train Loss:   1.5,  Train Acc: 44.14%,Time: 0:10:08\n",
      "2020-05-23 09:38:14,531 - utils.utils - INFO - Iter:  11000,  Train Loss:   1.5,  Train Acc: 41.02%,Time: 0:10:13\n",
      "2020-05-23 09:38:20,002 - utils.utils - INFO - Iter:  11100,  Train Loss:   1.4,  Train Acc: 47.27%,Time: 0:10:19\n",
      "2020-05-23 09:38:25,443 - utils.utils - INFO - Iter:  11200,  Train Loss:   1.5,  Train Acc: 45.31%,Time: 0:10:24\n",
      "2020-05-23 09:38:30,920 - utils.utils - INFO - Iter:  11300,  Train Loss:   1.4,  Train Acc: 39.45%,Time: 0:10:30\n",
      "2020-05-23 09:38:36,349 - utils.utils - INFO - Iter:  11400,  Train Loss:   1.5,  Train Acc: 43.75%,Time: 0:10:35\n",
      "2020-05-23 09:38:41,839 - utils.utils - INFO - Iter:  11500,  Train Loss:   1.4,  Train Acc: 44.92%,Time: 0:10:41\n",
      "2020-05-23 09:38:47,264 - utils.utils - INFO - Iter:  11600,  Train Loss:   1.5,  Train Acc: 34.77%,Time: 0:10:46\n",
      "2020-05-23 09:38:52,722 - utils.utils - INFO - Iter:  11700,  Train Loss:   1.5,  Train Acc: 40.62%,Time: 0:10:51\n",
      "2020-05-23 09:38:58,160 - utils.utils - INFO - Iter:  11800,  Train Loss:   1.5,  Train Acc: 44.53%,Time: 0:10:57\n",
      "2020-05-23 09:39:03,622 - utils.utils - INFO - Iter:  11900,  Train Loss:   1.5,  Train Acc: 41.41%,Time: 0:11:02\n",
      "2020-05-23 09:39:09,067 - utils.utils - INFO - Iter:  12000,  Train Loss:   1.4,  Train Acc: 46.48%,Time: 0:11:08\n",
      "2020-05-23 09:39:14,466 - utils.utils - INFO - Iter:  12100,  Train Loss:   1.5,  Train Acc: 43.36%,Time: 0:11:13\n",
      "2020-05-23 09:39:19,895 - utils.utils - INFO - Iter:  12200,  Train Loss:   1.5,  Train Acc: 42.19%,Time: 0:11:19\n",
      "2020-05-23 09:39:25,378 - utils.utils - INFO - Iter:  12300,  Train Loss:   1.5,  Train Acc: 44.14%,Time: 0:11:24\n",
      "2020-05-23 09:39:30,812 - utils.utils - INFO - Iter:  12400,  Train Loss:   1.3,  Train Acc: 53.52%,Time: 0:11:30\n",
      "2020-05-23 09:39:36,273 - utils.utils - INFO - Iter:  12500,  Train Loss:   1.5,  Train Acc: 45.70%,Time: 0:11:35\n",
      "2020-05-23 09:39:41,702 - utils.utils - INFO - Iter:  12600,  Train Loss:   1.5,  Train Acc: 40.62%,Time: 0:11:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-23 09:39:47,203 - utils.utils - INFO - Iter:  12700,  Train Loss:   1.5,  Train Acc: 39.84%,Time: 0:11:46\n",
      "2020-05-23 09:39:52,719 - utils.utils - INFO - Iter:  12800,  Train Loss:   1.5,  Train Acc: 44.14%,Time: 0:11:51\n",
      "2020-05-23 09:39:58,179 - utils.utils - INFO - Iter:  12900,  Train Loss:   1.3,  Train Acc: 46.88%,Time: 0:11:57\n",
      "2020-05-23 09:40:03,638 - utils.utils - INFO - Iter:  13000,  Train Loss:   1.5,  Train Acc: 44.14%,Time: 0:12:02\n",
      "2020-05-23 09:40:09,146 - utils.utils - INFO - Iter:  13100,  Train Loss:   1.4,  Train Acc: 45.70%,Time: 0:12:08\n",
      "2020-05-23 09:40:14,610 - utils.utils - INFO - Iter:  13200,  Train Loss:   1.4,  Train Acc: 46.88%,Time: 0:12:13\n",
      "2020-05-23 09:40:20,073 - utils.utils - INFO - Iter:  13300,  Train Loss:   1.4,  Train Acc: 44.53%,Time: 0:12:19\n",
      "2020-05-23 09:40:25,497 - utils.utils - INFO - Iter:  13400,  Train Loss:   1.5,  Train Acc: 48.83%,Time: 0:12:24\n",
      "2020-05-23 09:40:30,951 - utils.utils - INFO - Iter:  13500,  Train Loss:   1.5,  Train Acc: 42.19%,Time: 0:12:30\n",
      "2020-05-23 09:40:36,704 - utils.utils - INFO - Iter:  13600,  Train Loss:   1.5,  Train Acc: 45.31%,Time: 0:12:35\n",
      "2020-05-23 09:40:42,442 - utils.utils - INFO - Iter:  13700,  Train Loss:   1.4,  Train Acc: 44.14%,Time: 0:12:41\n",
      "2020-05-23 09:40:48,143 - utils.utils - INFO - Iter:  13800,  Train Loss:   1.4,  Train Acc: 48.83%,Time: 0:12:47\n",
      "2020-05-23 09:40:53,905 - utils.utils - INFO - Iter:  13900,  Train Loss:   1.3,  Train Acc: 46.88%,Time: 0:12:53\n",
      "2020-05-23 09:40:59,693 - utils.utils - INFO - Iter:  14000,  Train Loss:   1.5,  Train Acc: 42.58%,Time: 0:12:58\n",
      "2020-05-23 09:41:33,595 - utils.utils - INFO - neural_train已完成，共用时0:13:32\n",
      "2020-05-23 09:41:37,846 - mlpipeline.train - INFO - _train_pipeline_neural已完成，共用时0:14:56\n",
      "2020-05-23 09:41:37,864 - mlpipeline.train - INFO - bilstm_attention模型训练完成!模型保存至:../trained_models/bilstm_attention.model.2020-05-23T09:26:42.107428\n",
      "2020-05-23 09:41:37,938 - mlpipeline.train - INFO - train已完成，共用时0:14:57\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "lstm_model_params ={\n",
    "    'model_name':'lstm', \n",
    "    'num_classes':20, \n",
    "    'sparse_feat':'creative_id', \n",
    "    'embed':'embedding_creative_id_300.npy',\n",
    "    'dropout':0.5,\n",
    "    'required_improvement':1000,\n",
    "    'num_epochs':5,\n",
    "    'batch_size':128,\n",
    "    'learning_rate':1e-3,\n",
    "    'hidden_size':128,\n",
    "    'use_pad':True,\n",
    "    'pad_size':64,\n",
    "    'seed':1234,\n",
    "    'init_method':'kaiming' , # 'kaiming'\n",
    "    'num_layers' : 1,\n",
    "    'bidirectional':True\n",
    "}\n",
    "\n",
    "bilstm_attention_model_params = {\n",
    "    'model_name':'bilstm_attention',\n",
    "    'num_classes':20,\n",
    "    'sparse_feat':'creative_id', \n",
    "    'embed':'embedding_creative_id_300.npy',\n",
    "    'dropout':0.3,\n",
    "    'required_improvement':1000,\n",
    "    'num_epochs':4,\n",
    "    'batch_size':256,\n",
    "    'learning_rate':1e-3,\n",
    "    'hidden_size':256,\n",
    "    'use_pad':True,\n",
    "    'max_seq_len':110,\n",
    "    'seed':1234,\n",
    "    'init_method':'xavier' , # 'kaiming','xavier'\n",
    "    'num_layers' : 1,\n",
    "#     'attention_size':256,\n",
    "    'bidirectional':True\n",
    "}\n",
    "\n",
    "\n",
    "lstm_attention_model_params ={\n",
    "    'model_name':'lstm_attention',\n",
    "    'num_classes':20,\n",
    "    'sparse_feat':'creative_id', \n",
    "    'embed':'embedding_creative_id_300.npy',\n",
    "    'dropout':0.2,\n",
    "    'required_improvement':1000,\n",
    "    'num_epochs':3,\n",
    "    'batch_size':128,\n",
    "    'learning_rate':1e-3,\n",
    "    'hidden_size':256,\n",
    "    'use_pad':True,\n",
    "    'pad_size':90,\n",
    "    'seed':1234,\n",
    "    'init_method':'kaiming' , # 'kaiming'\n",
    "    'num_layers' : 1,\n",
    "    'max_seq_len':90,\n",
    "    'bidirectional':False\n",
    "}\n",
    "params = {\n",
    "    'fe_filename':'neural_train_fe_df.feather', \n",
    "    'is_eval':False, \n",
    "    'model_type': 'neural',\n",
    "    'model_name': 'bilstm_attention',\n",
    "    'model_params': bilstm_attention_model_params,\n",
    "    'use_log': False,\n",
    "    'use_std': False,\n",
    "    'use_cv': False,  \n",
    "}\n",
    "model = train_wrapper(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-23 09:43:41,710 - mlpipeline.predict - INFO - predict开始\n",
      "2020-05-23 09:43:41,712 - mlpipeline.predict - INFO - test_fe_filename: neural_test_fe_df.feather, use_log: False, use_std: False, model_type: neural, model_name: bilstm_attention\n",
      "2020-05-23 09:43:42,692 - mlpipeline.predict - INFO - inference_pipeline_neural开始\n",
      "2020-05-23 09:43:49,264 - utils.utils - INFO - build_dataset开始\n",
      "2020-05-23 09:43:51,056 - utils.utils - INFO - ../data/creative_id_vocab.pkl has been loaded\n",
      "2020-05-23 09:45:01,610 - utils.utils - INFO - build_dataset已完成，共用时0:01:12\n",
      "2020-05-23 09:45:01,611 - mlpipeline.predict - INFO - Loading data...\n",
      "2020-05-23 09:45:01,612 - utils.utils - INFO - build_iterater开始\n",
      "2020-05-23 09:45:01,613 - utils.utils - INFO - build_iterater已完成，共用时0:00:00\n",
      "2020-05-23 09:45:01,614 - mlpipeline.predict - INFO - Time usage:0:00:00\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:376: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "2020-05-23 09:46:06,737 - mlpipeline.predict - INFO - inference_pipeline_neural已完成，共用时0:02:24\n",
      "2020-05-23 09:46:06,755 - mlpipeline.predict - INFO - _generate_submission_file开始\n",
      "2020-05-23 09:46:14,480 - mlpipeline.predict - INFO - submission file has been stored in ../submission/submission_y_2020-05-23T09:46:08.001373.csv\n",
      "2020-05-23 09:46:14,483 - mlpipeline.predict - INFO - _generate_submission_file已完成，共用时0:00:08\n",
      "2020-05-23 09:46:18,049 - mlpipeline.predict - INFO - predict已完成，共用时0:02:36\n"
     ]
    }
   ],
   "source": [
    "# predict \n",
    "lstm_attention_model_params ={\n",
    "    'model_name':'lstm_attention',\n",
    "    'num_classes':20,\n",
    "    'sparse_feat':'creative_id', \n",
    "    'embed':'embedding_creative_id_300.npy',\n",
    "    'dropout':0.2,\n",
    "    'required_improvement':1000,\n",
    "    'num_epochs':20,\n",
    "    'batch_size':128,\n",
    "    'learning_rate':1e-3,\n",
    "    'hidden_size':256,\n",
    "    'use_pad':True,\n",
    "    'pad_size':90,\n",
    "    'seed':1234,\n",
    "    'init_method':'kaiming' , # 'kaiming'\n",
    "    'num_layers' : 1,\n",
    "    'max_seq_len':90,\n",
    "    'bidirectional':False\n",
    "}\n",
    "\n",
    "\n",
    "bilstm_attention_model_params = {\n",
    "    'model_name':'bilstm_attention',\n",
    "    'num_classes':20,\n",
    "    'sparse_feat':'creative_id', \n",
    "    'embed':'embedding_creative_id_300.npy',\n",
    "    'dropout':0.3,\n",
    "    'required_improvement':1000,\n",
    "    'num_epochs':4,\n",
    "    'batch_size':256,\n",
    "    'learning_rate':1e-3,\n",
    "    'hidden_size':256,\n",
    "    'use_pad':True,\n",
    "    'max_seq_len':110,\n",
    "    'seed':1234,\n",
    "    'init_method':'xavier' , # 'kaiming','xavier'\n",
    "    'num_layers' : 1,\n",
    "#     'attention_size':256,\n",
    "    'bidirectional':True\n",
    "}\n",
    "\n",
    "params = {\n",
    "          'test_fe_filename':'neural_test_fe_df.feather',\n",
    "          'use_log':False,\n",
    "          'use_std': False,\n",
    "          'model_type': 'neural',\n",
    "          'model_name':'bilstm_attention',\n",
    "          'model_params': bilstm_attention_model_params\n",
    "            }\n",
    "\n",
    "submission_df = predict(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>predicted_gender</th>\n",
       "      <th>predicted_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3000001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3000002</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3000003</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3000004</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3000005</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  predicted_gender  predicted_age\n",
       "0  3000001                 1              3\n",
       "1  3000002                 2              7\n",
       "2  3000003                 2              2\n",
       "3  3000004                 1              3\n",
       "4  3000005                 1              4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine age and gender \n",
    "submission_age_df = pd.read_csv(os.path.join(conf.SUBMISSION_DIR,'submission_age_2020-05-16T14:43:48.536108.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_y_df = pd.read_csv(os.path.join(conf.SUBMISSION_DIR,'submission_y_2020-05-16T11:27:59.072741.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_age_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_y_df['predicted_age'] = submission_age_df['predicted_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_save_path = os.path.join(conf.SUBMISSION_DIR,'submission_y_%s.csv'%(datetime.now().isoformat()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_y_df.to_csv(os.path.join(conf.SUBMISSION_DIR, submission_save_path),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

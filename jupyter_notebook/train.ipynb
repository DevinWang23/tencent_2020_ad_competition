{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mlpipeline\n",
    "Feature engineering, train, evaluate model and generate submission \n",
    "- <a href='#1'>1. feature_engineering</a> \n",
    "- <a href='#2'>2. train</a> \n",
    "- <a href='#3'>3. predict</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "from time import time\n",
    "from datetime import timedelta, datetime\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from dinglingling import wx_reminder\n",
    "\n",
    "sys.path.append('../')\n",
    "import conf\n",
    "from mlpipeline import (\n",
    "    feature_engineering_pandas,\n",
    "    train,\n",
    "    predict,\n",
    ")\n",
    "from utils import (\n",
    "    check_columns,\n",
    "    check_nan_value,\n",
    "    load_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.width',100)\n",
    "sns.set(rc={'figure.figsize':(20,10)})\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "warnings.simplefilter('ignore', np.RankWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "SCKEY = 'SCU92138T03d57ff9d4b08ced24c2cceb440cd3bd5e843242680de'  # used for reminding when feature engineering or model training completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def __dummy():\n",
    "    pass\n",
    "\n",
    "@wx_reminder(SCKEY=SCKEY, remind_started=True)  \n",
    "def feature_engineering_wrapper(params):\n",
    "        \"\"\"\n",
    "        wrapper for feature engineering func \n",
    "        for reminding when it completes\n",
    "        \"\"\"\n",
    "        train_fe_df, test_fe_df = feature_engineering_pandas(**params)\n",
    "        \n",
    "        return train_fe_df, test_fe_df    \n",
    "    \n",
    "@wx_reminder(SCKEY=SCKEY, remind_started=True)  \n",
    "def train_wrapper(params):\n",
    "        if params['is_eval']:\n",
    "            _,_ = train(**params)\n",
    "        else: \n",
    "            model, scaler = train(**params) \n",
    "            return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3G\t../data/creative_id_w2v.bin\r\n",
      "28M\t../data/label_round_one_df.feather\r\n",
      "1.2G\t../data/raw_test_df.feather\r\n",
      "1.2G\t../data/raw_train_round_one_df.feather\r\n",
      "13G\t../data/tencent_2019_ad_competition_data\r\n",
      "732M\t../data/test\r\n",
      "217M\t../data/test.zip\r\n",
      "771M\t../data/test_fe_df.feather\r\n",
      "715M\t../data/train_fe_df.feather\r\n",
      "637M\t../data/train_preliminary\r\n",
      "197M\t../data/train_preliminary.zip\r\n"
     ]
    }
   ],
   "source": [
    "! du -sh ../data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='1'> 1.feature_engineering</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-15 12:46:20,809 - mlpipeline.feature_engineering.feature_engineering - INFO - feature_engineering_pandas开始\n",
      "2020-05-15 12:46:20,812 - mlpipeline.feature_engineering.feature_engineering - INFO - _load_preprocessed_data开始\n",
      "2020-05-15 12:46:21,884 - mlpipeline.feature_engineering.feature_engineering - INFO - _load_preprocessed_data已完成，共用时0:00:01\n",
      "2020-05-15 12:46:29,308 - mlpipeline.feature_engineering.feature_engineering - INFO - _generate_emb_for_sparse_feat开始\n",
      "../mlpipeline/feature_engineering/feature_engineering.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sparse_feat_seq_df[sparse_feat] = sparse_feat_seq_df[sparse_feat].astype(str)\n",
      "2020-05-15 12:52:32,259 - mlpipeline.feature_engineering.feature_engineering - INFO - sparse_feat: creative_id, method: w2v, emd_dim: 100, window: 5, min_count: 5, workers: 4, sg: 0, hs: 0, smaple: 6e-05, negative: 0, alpha: 0.03, min_alpha: 0.0007\n",
      "2020-05-15 12:52:32,292 - mlpipeline.feature_engineering.feature_engineering - INFO - ../data/creative_id_w2v.bin exists, loading...\n",
      "2020-05-15 12:53:18,399 - mlpipeline.feature_engineering.feature_engineering - INFO - ../data/creative_id_w2v.bin has been loaded\n",
      "2020-05-15 12:59:11,711 - mlpipeline.feature_engineering.feature_engineering - INFO - _generate_emb_for_sparse_feat已完成，共用时0:12:42\n",
      "2020-05-15 12:59:13,302 - mlpipeline.feature_engineering.feature_engineering - INFO - _get_label开始\n",
      "2020-05-15 12:59:14,132 - mlpipeline.feature_engineering.feature_engineering - INFO - _get_label已完成，共用时0:00:01\n",
      "2020-05-15 12:59:18,837 - mlpipeline.feature_engineering.feature_engineering - INFO - train_fe_df with shape (900000, 104) has been stored in ../data/train_fe_df.feather\n",
      "2020-05-15 12:59:24,035 - mlpipeline.feature_engineering.feature_engineering - INFO - test_fe_df with shape (1000000, 101) has been stored in ../data/test_fe_df.feather\n",
      "2020-05-15 12:59:24,583 - mlpipeline.feature_engineering.feature_engineering - INFO - feature_engineering_pandas已完成，共用时0:13:04\n"
     ]
    }
   ],
   "source": [
    "# feature engineering\n",
    "params = {\n",
    "    'train_preprocessed_data_filename':'raw_train_round_one_df.feather', \n",
    "    'test_preprocessed_data_filename':'raw_test_df.feather', \n",
    "    'train_fe_save_filename': 'train_fe_df.feather',\n",
    "    'test_fe_save_filename': 'test_fe_df.feather',\n",
    "    'emb_method':'w2v',\n",
    "    'max_df':0.9,  # param for tf_idf\n",
    "    'min_df':3,  # param for tf_idf\n",
    "    'emb_dim':128,\n",
    "    'window':5,\n",
    "    'min_count':1,\n",
    "    'sample':6e-5,\n",
    "    'negative':0,\n",
    "    'hs':1,\n",
    "    'alpha':0.03,\n",
    "    'min_alpha':0.0007,\n",
    "    'workers':20,\n",
    "    'sg':1,\n",
    "    'num_processes': 40,\n",
    "    'is_train':True,\n",
    "}\n",
    "\n",
    "train_fe_df, test_fe_df = feature_engineering_wrapper(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_fe_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f88479bca6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_fe_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_fe_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_fe_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_fe_df.shape, test_fe_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>w2v_1</th>\n",
       "      <th>w2v_2</th>\n",
       "      <th>w2v_3</th>\n",
       "      <th>w2v_4</th>\n",
       "      <th>w2v_5</th>\n",
       "      <th>w2v_6</th>\n",
       "      <th>w2v_7</th>\n",
       "      <th>w2v_8</th>\n",
       "      <th>w2v_9</th>\n",
       "      <th>w2v_10</th>\n",
       "      <th>w2v_11</th>\n",
       "      <th>w2v_12</th>\n",
       "      <th>w2v_13</th>\n",
       "      <th>w2v_14</th>\n",
       "      <th>w2v_15</th>\n",
       "      <th>w2v_16</th>\n",
       "      <th>w2v_17</th>\n",
       "      <th>w2v_18</th>\n",
       "      <th>w2v_19</th>\n",
       "      <th>w2v_20</th>\n",
       "      <th>w2v_21</th>\n",
       "      <th>w2v_22</th>\n",
       "      <th>w2v_23</th>\n",
       "      <th>w2v_24</th>\n",
       "      <th>w2v_25</th>\n",
       "      <th>w2v_26</th>\n",
       "      <th>w2v_27</th>\n",
       "      <th>w2v_28</th>\n",
       "      <th>w2v_29</th>\n",
       "      <th>w2v_30</th>\n",
       "      <th>w2v_31</th>\n",
       "      <th>w2v_32</th>\n",
       "      <th>w2v_33</th>\n",
       "      <th>w2v_34</th>\n",
       "      <th>w2v_35</th>\n",
       "      <th>w2v_36</th>\n",
       "      <th>w2v_37</th>\n",
       "      <th>w2v_38</th>\n",
       "      <th>w2v_39</th>\n",
       "      <th>w2v_40</th>\n",
       "      <th>w2v_41</th>\n",
       "      <th>w2v_42</th>\n",
       "      <th>w2v_43</th>\n",
       "      <th>w2v_44</th>\n",
       "      <th>w2v_45</th>\n",
       "      <th>w2v_46</th>\n",
       "      <th>w2v_47</th>\n",
       "      <th>w2v_48</th>\n",
       "      <th>w2v_49</th>\n",
       "      <th>w2v_50</th>\n",
       "      <th>w2v_51</th>\n",
       "      <th>w2v_52</th>\n",
       "      <th>w2v_53</th>\n",
       "      <th>w2v_54</th>\n",
       "      <th>w2v_55</th>\n",
       "      <th>w2v_56</th>\n",
       "      <th>w2v_57</th>\n",
       "      <th>w2v_58</th>\n",
       "      <th>w2v_59</th>\n",
       "      <th>w2v_60</th>\n",
       "      <th>w2v_61</th>\n",
       "      <th>w2v_62</th>\n",
       "      <th>w2v_63</th>\n",
       "      <th>w2v_64</th>\n",
       "      <th>w2v_65</th>\n",
       "      <th>w2v_66</th>\n",
       "      <th>w2v_67</th>\n",
       "      <th>w2v_68</th>\n",
       "      <th>w2v_69</th>\n",
       "      <th>w2v_70</th>\n",
       "      <th>w2v_71</th>\n",
       "      <th>w2v_72</th>\n",
       "      <th>w2v_73</th>\n",
       "      <th>w2v_74</th>\n",
       "      <th>w2v_75</th>\n",
       "      <th>w2v_76</th>\n",
       "      <th>w2v_77</th>\n",
       "      <th>w2v_78</th>\n",
       "      <th>w2v_79</th>\n",
       "      <th>w2v_80</th>\n",
       "      <th>w2v_81</th>\n",
       "      <th>w2v_82</th>\n",
       "      <th>w2v_83</th>\n",
       "      <th>w2v_84</th>\n",
       "      <th>w2v_85</th>\n",
       "      <th>w2v_86</th>\n",
       "      <th>w2v_87</th>\n",
       "      <th>w2v_88</th>\n",
       "      <th>w2v_89</th>\n",
       "      <th>w2v_90</th>\n",
       "      <th>w2v_91</th>\n",
       "      <th>w2v_92</th>\n",
       "      <th>w2v_93</th>\n",
       "      <th>w2v_94</th>\n",
       "      <th>w2v_95</th>\n",
       "      <th>w2v_96</th>\n",
       "      <th>w2v_97</th>\n",
       "      <th>w2v_98</th>\n",
       "      <th>w2v_99</th>\n",
       "      <th>w2v_100</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.317886</td>\n",
       "      <td>-0.526415</td>\n",
       "      <td>-0.055048</td>\n",
       "      <td>-0.528955</td>\n",
       "      <td>-0.096361</td>\n",
       "      <td>0.162796</td>\n",
       "      <td>0.048159</td>\n",
       "      <td>0.119717</td>\n",
       "      <td>-0.040841</td>\n",
       "      <td>-0.436568</td>\n",
       "      <td>-0.032624</td>\n",
       "      <td>0.112490</td>\n",
       "      <td>-0.325787</td>\n",
       "      <td>0.138070</td>\n",
       "      <td>-0.241391</td>\n",
       "      <td>-0.253797</td>\n",
       "      <td>0.129697</td>\n",
       "      <td>0.532830</td>\n",
       "      <td>-0.220061</td>\n",
       "      <td>-0.047494</td>\n",
       "      <td>0.281527</td>\n",
       "      <td>-0.292721</td>\n",
       "      <td>-0.180316</td>\n",
       "      <td>-0.217422</td>\n",
       "      <td>-0.299926</td>\n",
       "      <td>-0.409515</td>\n",
       "      <td>0.536922</td>\n",
       "      <td>-0.150833</td>\n",
       "      <td>0.608378</td>\n",
       "      <td>0.519931</td>\n",
       "      <td>0.327593</td>\n",
       "      <td>-0.047129</td>\n",
       "      <td>-0.153875</td>\n",
       "      <td>-0.308788</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>-0.284649</td>\n",
       "      <td>0.363233</td>\n",
       "      <td>-0.126311</td>\n",
       "      <td>-0.099959</td>\n",
       "      <td>-0.399930</td>\n",
       "      <td>-0.008959</td>\n",
       "      <td>-0.557281</td>\n",
       "      <td>0.208298</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.229310</td>\n",
       "      <td>-0.095422</td>\n",
       "      <td>0.478953</td>\n",
       "      <td>0.017111</td>\n",
       "      <td>-0.488578</td>\n",
       "      <td>0.167485</td>\n",
       "      <td>0.055130</td>\n",
       "      <td>-0.015642</td>\n",
       "      <td>0.154260</td>\n",
       "      <td>0.166007</td>\n",
       "      <td>-0.038812</td>\n",
       "      <td>-0.298765</td>\n",
       "      <td>-0.494071</td>\n",
       "      <td>-0.282491</td>\n",
       "      <td>0.016670</td>\n",
       "      <td>-0.373491</td>\n",
       "      <td>-0.480105</td>\n",
       "      <td>0.116221</td>\n",
       "      <td>-0.369969</td>\n",
       "      <td>0.233782</td>\n",
       "      <td>-0.427903</td>\n",
       "      <td>-0.053939</td>\n",
       "      <td>0.015587</td>\n",
       "      <td>0.521378</td>\n",
       "      <td>-0.069436</td>\n",
       "      <td>-0.167746</td>\n",
       "      <td>-0.121342</td>\n",
       "      <td>0.047510</td>\n",
       "      <td>-0.453079</td>\n",
       "      <td>-0.044746</td>\n",
       "      <td>-0.312390</td>\n",
       "      <td>0.092713</td>\n",
       "      <td>-0.270386</td>\n",
       "      <td>0.118622</td>\n",
       "      <td>0.128435</td>\n",
       "      <td>0.047801</td>\n",
       "      <td>0.021427</td>\n",
       "      <td>-0.179088</td>\n",
       "      <td>-0.134783</td>\n",
       "      <td>0.039494</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-0.009532</td>\n",
       "      <td>0.177008</td>\n",
       "      <td>0.685854</td>\n",
       "      <td>-0.450034</td>\n",
       "      <td>0.525002</td>\n",
       "      <td>0.347635</td>\n",
       "      <td>0.345707</td>\n",
       "      <td>0.180106</td>\n",
       "      <td>-0.140729</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>0.388303</td>\n",
       "      <td>-0.458613</td>\n",
       "      <td>-0.068500</td>\n",
       "      <td>-0.613453</td>\n",
       "      <td>0.725146</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.325203</td>\n",
       "      <td>-0.367411</td>\n",
       "      <td>-0.279791</td>\n",
       "      <td>-0.093693</td>\n",
       "      <td>-0.122351</td>\n",
       "      <td>0.297370</td>\n",
       "      <td>-0.206585</td>\n",
       "      <td>0.076459</td>\n",
       "      <td>0.294193</td>\n",
       "      <td>-0.571907</td>\n",
       "      <td>-0.055274</td>\n",
       "      <td>0.130691</td>\n",
       "      <td>-0.330579</td>\n",
       "      <td>0.090058</td>\n",
       "      <td>-0.629760</td>\n",
       "      <td>-0.099451</td>\n",
       "      <td>0.191704</td>\n",
       "      <td>0.427102</td>\n",
       "      <td>-0.219477</td>\n",
       "      <td>-0.062363</td>\n",
       "      <td>-0.117611</td>\n",
       "      <td>-0.434651</td>\n",
       "      <td>0.044292</td>\n",
       "      <td>-0.188073</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>-0.450696</td>\n",
       "      <td>0.693025</td>\n",
       "      <td>0.267765</td>\n",
       "      <td>0.488802</td>\n",
       "      <td>0.422140</td>\n",
       "      <td>0.118047</td>\n",
       "      <td>-0.051940</td>\n",
       "      <td>-0.085290</td>\n",
       "      <td>-0.212455</td>\n",
       "      <td>0.080108</td>\n",
       "      <td>-0.335424</td>\n",
       "      <td>0.391596</td>\n",
       "      <td>0.204405</td>\n",
       "      <td>-0.231755</td>\n",
       "      <td>-0.379702</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>-0.422194</td>\n",
       "      <td>0.579996</td>\n",
       "      <td>-0.274050</td>\n",
       "      <td>-0.108324</td>\n",
       "      <td>0.196841</td>\n",
       "      <td>0.737161</td>\n",
       "      <td>0.418932</td>\n",
       "      <td>-0.240259</td>\n",
       "      <td>0.285742</td>\n",
       "      <td>-0.203938</td>\n",
       "      <td>0.192743</td>\n",
       "      <td>0.287305</td>\n",
       "      <td>-0.290213</td>\n",
       "      <td>-0.273336</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>-0.399577</td>\n",
       "      <td>-0.422319</td>\n",
       "      <td>-0.018834</td>\n",
       "      <td>-0.419183</td>\n",
       "      <td>-0.187218</td>\n",
       "      <td>0.080601</td>\n",
       "      <td>-0.404032</td>\n",
       "      <td>-0.018926</td>\n",
       "      <td>-0.800364</td>\n",
       "      <td>0.318652</td>\n",
       "      <td>-0.158947</td>\n",
       "      <td>0.312360</td>\n",
       "      <td>-0.095327</td>\n",
       "      <td>-0.231974</td>\n",
       "      <td>-0.330422</td>\n",
       "      <td>0.121357</td>\n",
       "      <td>-0.249703</td>\n",
       "      <td>0.121524</td>\n",
       "      <td>-0.148256</td>\n",
       "      <td>0.157988</td>\n",
       "      <td>-0.415097</td>\n",
       "      <td>0.073084</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>-0.329378</td>\n",
       "      <td>0.122206</td>\n",
       "      <td>-0.130705</td>\n",
       "      <td>-0.119785</td>\n",
       "      <td>0.072925</td>\n",
       "      <td>-0.366768</td>\n",
       "      <td>-0.099851</td>\n",
       "      <td>0.126987</td>\n",
       "      <td>0.645205</td>\n",
       "      <td>-0.032412</td>\n",
       "      <td>0.242342</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>-0.006131</td>\n",
       "      <td>-0.254851</td>\n",
       "      <td>0.070728</td>\n",
       "      <td>-0.232397</td>\n",
       "      <td>-0.214553</td>\n",
       "      <td>-0.202904</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>-0.275755</td>\n",
       "      <td>0.398690</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.198565</td>\n",
       "      <td>-0.586632</td>\n",
       "      <td>-0.262671</td>\n",
       "      <td>0.027136</td>\n",
       "      <td>0.100215</td>\n",
       "      <td>0.322511</td>\n",
       "      <td>-0.073465</td>\n",
       "      <td>0.222631</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>-0.586876</td>\n",
       "      <td>-0.160328</td>\n",
       "      <td>0.126489</td>\n",
       "      <td>-0.086501</td>\n",
       "      <td>-0.182446</td>\n",
       "      <td>-0.321912</td>\n",
       "      <td>0.072478</td>\n",
       "      <td>0.058277</td>\n",
       "      <td>0.273296</td>\n",
       "      <td>0.040683</td>\n",
       "      <td>-0.374089</td>\n",
       "      <td>-0.047131</td>\n",
       "      <td>0.089502</td>\n",
       "      <td>-0.090019</td>\n",
       "      <td>-0.217124</td>\n",
       "      <td>0.050214</td>\n",
       "      <td>-0.337977</td>\n",
       "      <td>0.150414</td>\n",
       "      <td>0.182220</td>\n",
       "      <td>0.169077</td>\n",
       "      <td>0.158828</td>\n",
       "      <td>0.159270</td>\n",
       "      <td>0.144751</td>\n",
       "      <td>-0.066186</td>\n",
       "      <td>-0.040247</td>\n",
       "      <td>-0.032578</td>\n",
       "      <td>-0.190561</td>\n",
       "      <td>0.008836</td>\n",
       "      <td>0.151353</td>\n",
       "      <td>0.133934</td>\n",
       "      <td>-0.368623</td>\n",
       "      <td>0.134417</td>\n",
       "      <td>-0.560351</td>\n",
       "      <td>0.443211</td>\n",
       "      <td>-0.094252</td>\n",
       "      <td>-0.098049</td>\n",
       "      <td>-0.137316</td>\n",
       "      <td>0.347831</td>\n",
       "      <td>0.367783</td>\n",
       "      <td>-0.088025</td>\n",
       "      <td>0.299211</td>\n",
       "      <td>0.222399</td>\n",
       "      <td>0.057067</td>\n",
       "      <td>-0.206091</td>\n",
       "      <td>0.319835</td>\n",
       "      <td>-0.401369</td>\n",
       "      <td>-0.390524</td>\n",
       "      <td>-0.407110</td>\n",
       "      <td>-0.227170</td>\n",
       "      <td>-0.037186</td>\n",
       "      <td>-0.336939</td>\n",
       "      <td>-0.046157</td>\n",
       "      <td>-0.171637</td>\n",
       "      <td>-0.315089</td>\n",
       "      <td>0.435404</td>\n",
       "      <td>-0.487106</td>\n",
       "      <td>-0.021420</td>\n",
       "      <td>0.050940</td>\n",
       "      <td>0.625776</td>\n",
       "      <td>-0.113916</td>\n",
       "      <td>-0.152975</td>\n",
       "      <td>-0.259297</td>\n",
       "      <td>0.502868</td>\n",
       "      <td>-0.481878</td>\n",
       "      <td>0.038209</td>\n",
       "      <td>-0.421211</td>\n",
       "      <td>-0.215066</td>\n",
       "      <td>-0.276196</td>\n",
       "      <td>0.333828</td>\n",
       "      <td>0.009564</td>\n",
       "      <td>0.081381</td>\n",
       "      <td>-0.134869</td>\n",
       "      <td>-0.039792</td>\n",
       "      <td>-0.095843</td>\n",
       "      <td>0.164616</td>\n",
       "      <td>-0.148636</td>\n",
       "      <td>0.063577</td>\n",
       "      <td>0.637608</td>\n",
       "      <td>0.581705</td>\n",
       "      <td>-0.248123</td>\n",
       "      <td>0.566989</td>\n",
       "      <td>0.145143</td>\n",
       "      <td>-0.153065</td>\n",
       "      <td>0.301695</td>\n",
       "      <td>-0.154103</td>\n",
       "      <td>-0.555241</td>\n",
       "      <td>-0.109893</td>\n",
       "      <td>-0.180794</td>\n",
       "      <td>-0.198200</td>\n",
       "      <td>-0.214605</td>\n",
       "      <td>0.468438</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.151257</td>\n",
       "      <td>-0.374335</td>\n",
       "      <td>-0.428679</td>\n",
       "      <td>-0.272160</td>\n",
       "      <td>0.215631</td>\n",
       "      <td>0.185658</td>\n",
       "      <td>-0.006619</td>\n",
       "      <td>0.316194</td>\n",
       "      <td>0.247443</td>\n",
       "      <td>-0.474306</td>\n",
       "      <td>-0.171284</td>\n",
       "      <td>-0.099072</td>\n",
       "      <td>-0.241625</td>\n",
       "      <td>-0.016837</td>\n",
       "      <td>-0.539686</td>\n",
       "      <td>0.092095</td>\n",
       "      <td>0.188419</td>\n",
       "      <td>0.393323</td>\n",
       "      <td>0.016091</td>\n",
       "      <td>-0.133066</td>\n",
       "      <td>-0.085917</td>\n",
       "      <td>-0.090432</td>\n",
       "      <td>-0.349310</td>\n",
       "      <td>-0.248880</td>\n",
       "      <td>-0.120003</td>\n",
       "      <td>-0.250547</td>\n",
       "      <td>0.428762</td>\n",
       "      <td>0.223570</td>\n",
       "      <td>0.324844</td>\n",
       "      <td>0.603414</td>\n",
       "      <td>0.053672</td>\n",
       "      <td>0.051973</td>\n",
       "      <td>-0.119606</td>\n",
       "      <td>-0.243262</td>\n",
       "      <td>-0.225287</td>\n",
       "      <td>-0.183186</td>\n",
       "      <td>-0.009053</td>\n",
       "      <td>0.117919</td>\n",
       "      <td>0.068445</td>\n",
       "      <td>-0.254893</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>-0.525274</td>\n",
       "      <td>0.460843</td>\n",
       "      <td>0.026366</td>\n",
       "      <td>-0.001258</td>\n",
       "      <td>0.202677</td>\n",
       "      <td>0.586647</td>\n",
       "      <td>0.369458</td>\n",
       "      <td>-0.313746</td>\n",
       "      <td>0.418417</td>\n",
       "      <td>-0.087917</td>\n",
       "      <td>0.253184</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>0.022453</td>\n",
       "      <td>-0.494399</td>\n",
       "      <td>-0.131953</td>\n",
       "      <td>-0.218401</td>\n",
       "      <td>-0.447379</td>\n",
       "      <td>0.219920</td>\n",
       "      <td>-0.246410</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.083894</td>\n",
       "      <td>-0.352079</td>\n",
       "      <td>-0.056614</td>\n",
       "      <td>-0.610225</td>\n",
       "      <td>0.147180</td>\n",
       "      <td>-0.193593</td>\n",
       "      <td>0.466122</td>\n",
       "      <td>-0.173300</td>\n",
       "      <td>-0.265374</td>\n",
       "      <td>-0.296355</td>\n",
       "      <td>0.281162</td>\n",
       "      <td>-0.299944</td>\n",
       "      <td>0.284063</td>\n",
       "      <td>-0.218795</td>\n",
       "      <td>-0.251554</td>\n",
       "      <td>-0.383884</td>\n",
       "      <td>0.073274</td>\n",
       "      <td>0.035057</td>\n",
       "      <td>-0.054880</td>\n",
       "      <td>0.183362</td>\n",
       "      <td>0.023791</td>\n",
       "      <td>-0.213668</td>\n",
       "      <td>0.311855</td>\n",
       "      <td>-0.361867</td>\n",
       "      <td>0.152611</td>\n",
       "      <td>0.246283</td>\n",
       "      <td>0.489768</td>\n",
       "      <td>-0.208535</td>\n",
       "      <td>0.408645</td>\n",
       "      <td>0.242020</td>\n",
       "      <td>-0.153373</td>\n",
       "      <td>-0.096918</td>\n",
       "      <td>-0.067699</td>\n",
       "      <td>-0.415370</td>\n",
       "      <td>-0.180304</td>\n",
       "      <td>-0.016965</td>\n",
       "      <td>0.106257</td>\n",
       "      <td>-0.434509</td>\n",
       "      <td>0.394375</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.028938</td>\n",
       "      <td>-0.318149</td>\n",
       "      <td>-0.180849</td>\n",
       "      <td>-0.472022</td>\n",
       "      <td>-0.194668</td>\n",
       "      <td>0.347011</td>\n",
       "      <td>-0.163237</td>\n",
       "      <td>0.042353</td>\n",
       "      <td>-0.068275</td>\n",
       "      <td>-0.385615</td>\n",
       "      <td>-0.146023</td>\n",
       "      <td>0.317552</td>\n",
       "      <td>-0.404162</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>-0.127442</td>\n",
       "      <td>-0.034000</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.325168</td>\n",
       "      <td>0.035940</td>\n",
       "      <td>-0.300432</td>\n",
       "      <td>0.179213</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>-0.198190</td>\n",
       "      <td>-0.156287</td>\n",
       "      <td>-0.267601</td>\n",
       "      <td>0.278072</td>\n",
       "      <td>0.106210</td>\n",
       "      <td>0.427624</td>\n",
       "      <td>0.429296</td>\n",
       "      <td>0.367329</td>\n",
       "      <td>-0.092904</td>\n",
       "      <td>-0.162959</td>\n",
       "      <td>-0.244452</td>\n",
       "      <td>-0.024666</td>\n",
       "      <td>0.098418</td>\n",
       "      <td>-0.005921</td>\n",
       "      <td>0.200096</td>\n",
       "      <td>-0.214554</td>\n",
       "      <td>-0.367227</td>\n",
       "      <td>-0.248917</td>\n",
       "      <td>-0.543416</td>\n",
       "      <td>0.311109</td>\n",
       "      <td>-0.026497</td>\n",
       "      <td>-0.213172</td>\n",
       "      <td>-0.128181</td>\n",
       "      <td>0.477916</td>\n",
       "      <td>-0.170481</td>\n",
       "      <td>-0.177959</td>\n",
       "      <td>0.065573</td>\n",
       "      <td>-0.001948</td>\n",
       "      <td>0.026924</td>\n",
       "      <td>0.113313</td>\n",
       "      <td>0.161093</td>\n",
       "      <td>-0.144608</td>\n",
       "      <td>-0.357767</td>\n",
       "      <td>-0.131327</td>\n",
       "      <td>-0.406192</td>\n",
       "      <td>-0.057586</td>\n",
       "      <td>-0.251472</td>\n",
       "      <td>-0.062627</td>\n",
       "      <td>0.056552</td>\n",
       "      <td>-0.310126</td>\n",
       "      <td>0.402394</td>\n",
       "      <td>-0.536030</td>\n",
       "      <td>-0.194220</td>\n",
       "      <td>-0.035663</td>\n",
       "      <td>0.403383</td>\n",
       "      <td>-0.057518</td>\n",
       "      <td>-0.031682</td>\n",
       "      <td>-0.139198</td>\n",
       "      <td>0.312265</td>\n",
       "      <td>-0.296792</td>\n",
       "      <td>-0.067986</td>\n",
       "      <td>-0.307334</td>\n",
       "      <td>-0.157313</td>\n",
       "      <td>-0.079045</td>\n",
       "      <td>0.036528</td>\n",
       "      <td>-0.111514</td>\n",
       "      <td>-0.276553</td>\n",
       "      <td>0.174941</td>\n",
       "      <td>-0.330068</td>\n",
       "      <td>0.026196</td>\n",
       "      <td>0.163671</td>\n",
       "      <td>-0.126165</td>\n",
       "      <td>-0.056569</td>\n",
       "      <td>0.279290</td>\n",
       "      <td>0.349940</td>\n",
       "      <td>-0.562776</td>\n",
       "      <td>0.290090</td>\n",
       "      <td>0.236660</td>\n",
       "      <td>-0.012676</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>0.016177</td>\n",
       "      <td>-0.135031</td>\n",
       "      <td>0.079853</td>\n",
       "      <td>-0.236369</td>\n",
       "      <td>-0.080729</td>\n",
       "      <td>-0.466101</td>\n",
       "      <td>0.203967</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899995</td>\n",
       "      <td>899996</td>\n",
       "      <td>0.243240</td>\n",
       "      <td>-0.641140</td>\n",
       "      <td>-0.221246</td>\n",
       "      <td>-0.165119</td>\n",
       "      <td>-0.219668</td>\n",
       "      <td>0.288953</td>\n",
       "      <td>0.206380</td>\n",
       "      <td>0.095649</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>-0.321669</td>\n",
       "      <td>-0.062357</td>\n",
       "      <td>0.376121</td>\n",
       "      <td>-0.482505</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>-0.181318</td>\n",
       "      <td>-0.027182</td>\n",
       "      <td>-0.199615</td>\n",
       "      <td>0.301060</td>\n",
       "      <td>0.075792</td>\n",
       "      <td>-0.086011</td>\n",
       "      <td>-0.057258</td>\n",
       "      <td>-0.043454</td>\n",
       "      <td>-0.128335</td>\n",
       "      <td>-0.439783</td>\n",
       "      <td>-0.041851</td>\n",
       "      <td>-0.098818</td>\n",
       "      <td>0.259661</td>\n",
       "      <td>0.302658</td>\n",
       "      <td>0.445060</td>\n",
       "      <td>0.401655</td>\n",
       "      <td>0.134029</td>\n",
       "      <td>0.212844</td>\n",
       "      <td>-0.153095</td>\n",
       "      <td>-0.261479</td>\n",
       "      <td>-0.324027</td>\n",
       "      <td>-0.061661</td>\n",
       "      <td>0.060992</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.216859</td>\n",
       "      <td>-0.249309</td>\n",
       "      <td>0.137882</td>\n",
       "      <td>-0.415835</td>\n",
       "      <td>0.189478</td>\n",
       "      <td>-0.120426</td>\n",
       "      <td>-0.207229</td>\n",
       "      <td>0.083842</td>\n",
       "      <td>0.450378</td>\n",
       "      <td>-0.037286</td>\n",
       "      <td>-0.507741</td>\n",
       "      <td>0.396173</td>\n",
       "      <td>-0.234831</td>\n",
       "      <td>-0.050506</td>\n",
       "      <td>-0.083878</td>\n",
       "      <td>0.329780</td>\n",
       "      <td>-0.137773</td>\n",
       "      <td>-0.437423</td>\n",
       "      <td>-0.163581</td>\n",
       "      <td>-0.302451</td>\n",
       "      <td>0.316948</td>\n",
       "      <td>-0.139210</td>\n",
       "      <td>-0.242297</td>\n",
       "      <td>-0.023869</td>\n",
       "      <td>-0.287063</td>\n",
       "      <td>0.144579</td>\n",
       "      <td>-0.587444</td>\n",
       "      <td>0.211204</td>\n",
       "      <td>-0.085344</td>\n",
       "      <td>0.542422</td>\n",
       "      <td>-0.173311</td>\n",
       "      <td>-0.332935</td>\n",
       "      <td>-0.437832</td>\n",
       "      <td>0.077496</td>\n",
       "      <td>-0.258561</td>\n",
       "      <td>-0.007533</td>\n",
       "      <td>-0.308809</td>\n",
       "      <td>-0.254471</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>0.219235</td>\n",
       "      <td>-0.010523</td>\n",
       "      <td>-0.273479</td>\n",
       "      <td>0.216129</td>\n",
       "      <td>0.047848</td>\n",
       "      <td>-0.328360</td>\n",
       "      <td>-0.131239</td>\n",
       "      <td>-0.226935</td>\n",
       "      <td>0.088835</td>\n",
       "      <td>0.304606</td>\n",
       "      <td>0.662196</td>\n",
       "      <td>-0.328890</td>\n",
       "      <td>0.376784</td>\n",
       "      <td>0.046671</td>\n",
       "      <td>0.114476</td>\n",
       "      <td>0.057655</td>\n",
       "      <td>-0.170715</td>\n",
       "      <td>-0.316756</td>\n",
       "      <td>-0.005456</td>\n",
       "      <td>-0.289580</td>\n",
       "      <td>-0.164895</td>\n",
       "      <td>-0.630627</td>\n",
       "      <td>0.577789</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899996</td>\n",
       "      <td>899997</td>\n",
       "      <td>0.515683</td>\n",
       "      <td>-0.657982</td>\n",
       "      <td>-0.141998</td>\n",
       "      <td>-0.022650</td>\n",
       "      <td>-0.484071</td>\n",
       "      <td>0.476730</td>\n",
       "      <td>-0.027828</td>\n",
       "      <td>0.392949</td>\n",
       "      <td>0.037568</td>\n",
       "      <td>-0.851055</td>\n",
       "      <td>0.041133</td>\n",
       "      <td>0.077036</td>\n",
       "      <td>-0.276592</td>\n",
       "      <td>0.328957</td>\n",
       "      <td>-0.328961</td>\n",
       "      <td>0.150223</td>\n",
       "      <td>0.169104</td>\n",
       "      <td>0.631069</td>\n",
       "      <td>-0.180701</td>\n",
       "      <td>-0.014098</td>\n",
       "      <td>-0.170441</td>\n",
       "      <td>-0.357181</td>\n",
       "      <td>-0.084747</td>\n",
       "      <td>-0.019092</td>\n",
       "      <td>-0.125442</td>\n",
       "      <td>-0.529127</td>\n",
       "      <td>0.183871</td>\n",
       "      <td>0.093437</td>\n",
       "      <td>0.429113</td>\n",
       "      <td>0.468045</td>\n",
       "      <td>0.426172</td>\n",
       "      <td>0.068719</td>\n",
       "      <td>-0.193521</td>\n",
       "      <td>-0.202167</td>\n",
       "      <td>0.135101</td>\n",
       "      <td>-0.009935</td>\n",
       "      <td>0.324440</td>\n",
       "      <td>0.064370</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>-0.611104</td>\n",
       "      <td>-0.028211</td>\n",
       "      <td>-0.585192</td>\n",
       "      <td>0.607013</td>\n",
       "      <td>-0.167726</td>\n",
       "      <td>-0.367620</td>\n",
       "      <td>0.280053</td>\n",
       "      <td>0.372864</td>\n",
       "      <td>0.390724</td>\n",
       "      <td>-0.572494</td>\n",
       "      <td>-0.085832</td>\n",
       "      <td>-0.021984</td>\n",
       "      <td>0.092908</td>\n",
       "      <td>0.086178</td>\n",
       "      <td>-0.350186</td>\n",
       "      <td>-0.464652</td>\n",
       "      <td>-0.155343</td>\n",
       "      <td>0.064242</td>\n",
       "      <td>0.015871</td>\n",
       "      <td>0.246716</td>\n",
       "      <td>-0.066027</td>\n",
       "      <td>0.141708</td>\n",
       "      <td>0.130953</td>\n",
       "      <td>-0.532892</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>-0.582114</td>\n",
       "      <td>0.255843</td>\n",
       "      <td>0.055905</td>\n",
       "      <td>0.565095</td>\n",
       "      <td>-0.059821</td>\n",
       "      <td>-0.004112</td>\n",
       "      <td>-0.015955</td>\n",
       "      <td>0.447682</td>\n",
       "      <td>-0.311238</td>\n",
       "      <td>0.313101</td>\n",
       "      <td>-0.060271</td>\n",
       "      <td>0.095708</td>\n",
       "      <td>-0.186104</td>\n",
       "      <td>0.246352</td>\n",
       "      <td>0.109228</td>\n",
       "      <td>-0.058353</td>\n",
       "      <td>-0.208268</td>\n",
       "      <td>0.065131</td>\n",
       "      <td>-0.608763</td>\n",
       "      <td>0.306291</td>\n",
       "      <td>-0.244444</td>\n",
       "      <td>0.174844</td>\n",
       "      <td>0.579212</td>\n",
       "      <td>0.374679</td>\n",
       "      <td>-0.423249</td>\n",
       "      <td>0.662227</td>\n",
       "      <td>0.093215</td>\n",
       "      <td>0.094639</td>\n",
       "      <td>-0.043676</td>\n",
       "      <td>-0.219571</td>\n",
       "      <td>-0.526521</td>\n",
       "      <td>0.017701</td>\n",
       "      <td>-0.065823</td>\n",
       "      <td>0.079533</td>\n",
       "      <td>-0.551853</td>\n",
       "      <td>0.741822</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899997</td>\n",
       "      <td>899998</td>\n",
       "      <td>0.164928</td>\n",
       "      <td>-0.611155</td>\n",
       "      <td>-0.510723</td>\n",
       "      <td>-0.230253</td>\n",
       "      <td>0.287806</td>\n",
       "      <td>-0.177192</td>\n",
       "      <td>0.253795</td>\n",
       "      <td>0.792483</td>\n",
       "      <td>0.342007</td>\n",
       "      <td>-1.009153</td>\n",
       "      <td>-0.057842</td>\n",
       "      <td>-0.050730</td>\n",
       "      <td>-0.510912</td>\n",
       "      <td>-0.050470</td>\n",
       "      <td>-0.646040</td>\n",
       "      <td>0.146405</td>\n",
       "      <td>0.271567</td>\n",
       "      <td>0.687615</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>-0.034516</td>\n",
       "      <td>0.118729</td>\n",
       "      <td>-0.031271</td>\n",
       "      <td>-0.084964</td>\n",
       "      <td>0.257087</td>\n",
       "      <td>-0.032235</td>\n",
       "      <td>-0.299957</td>\n",
       "      <td>0.222076</td>\n",
       "      <td>0.313168</td>\n",
       "      <td>0.297853</td>\n",
       "      <td>0.149366</td>\n",
       "      <td>0.831230</td>\n",
       "      <td>0.045845</td>\n",
       "      <td>-0.063910</td>\n",
       "      <td>-0.346297</td>\n",
       "      <td>-0.079265</td>\n",
       "      <td>-0.371726</td>\n",
       "      <td>-0.135079</td>\n",
       "      <td>-0.008642</td>\n",
       "      <td>0.332060</td>\n",
       "      <td>-0.509973</td>\n",
       "      <td>-0.018216</td>\n",
       "      <td>-0.410527</td>\n",
       "      <td>0.529129</td>\n",
       "      <td>-0.119675</td>\n",
       "      <td>-0.734006</td>\n",
       "      <td>0.012256</td>\n",
       "      <td>0.538561</td>\n",
       "      <td>0.331921</td>\n",
       "      <td>-0.513746</td>\n",
       "      <td>0.465859</td>\n",
       "      <td>0.263832</td>\n",
       "      <td>-0.041647</td>\n",
       "      <td>-0.058444</td>\n",
       "      <td>0.257928</td>\n",
       "      <td>-0.809038</td>\n",
       "      <td>-0.436497</td>\n",
       "      <td>-0.076378</td>\n",
       "      <td>-0.035019</td>\n",
       "      <td>0.426222</td>\n",
       "      <td>-0.380657</td>\n",
       "      <td>-0.186435</td>\n",
       "      <td>0.295572</td>\n",
       "      <td>-0.060447</td>\n",
       "      <td>0.605304</td>\n",
       "      <td>-0.643241</td>\n",
       "      <td>-0.348190</td>\n",
       "      <td>0.233054</td>\n",
       "      <td>0.931285</td>\n",
       "      <td>0.228999</td>\n",
       "      <td>-0.215847</td>\n",
       "      <td>-0.171887</td>\n",
       "      <td>0.580200</td>\n",
       "      <td>-0.366655</td>\n",
       "      <td>-0.020602</td>\n",
       "      <td>-0.157568</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>-0.173951</td>\n",
       "      <td>0.649639</td>\n",
       "      <td>0.293386</td>\n",
       "      <td>0.513225</td>\n",
       "      <td>-0.119581</td>\n",
       "      <td>-0.336632</td>\n",
       "      <td>-0.419274</td>\n",
       "      <td>0.582473</td>\n",
       "      <td>-0.250532</td>\n",
       "      <td>0.082413</td>\n",
       "      <td>0.406791</td>\n",
       "      <td>0.701027</td>\n",
       "      <td>-0.162323</td>\n",
       "      <td>0.307486</td>\n",
       "      <td>0.176571</td>\n",
       "      <td>0.356786</td>\n",
       "      <td>0.074302</td>\n",
       "      <td>-0.374451</td>\n",
       "      <td>-0.977068</td>\n",
       "      <td>0.247008</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>-0.199047</td>\n",
       "      <td>0.170333</td>\n",
       "      <td>0.358967</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899998</td>\n",
       "      <td>899999</td>\n",
       "      <td>0.138514</td>\n",
       "      <td>-0.893192</td>\n",
       "      <td>-0.337032</td>\n",
       "      <td>-0.053397</td>\n",
       "      <td>-0.093038</td>\n",
       "      <td>0.053808</td>\n",
       "      <td>-0.188455</td>\n",
       "      <td>0.255990</td>\n",
       "      <td>0.068474</td>\n",
       "      <td>-0.824961</td>\n",
       "      <td>-0.025947</td>\n",
       "      <td>-0.210074</td>\n",
       "      <td>-0.005053</td>\n",
       "      <td>0.324245</td>\n",
       "      <td>-0.379235</td>\n",
       "      <td>-0.375546</td>\n",
       "      <td>-0.131549</td>\n",
       "      <td>0.898178</td>\n",
       "      <td>0.029652</td>\n",
       "      <td>-0.220556</td>\n",
       "      <td>0.242753</td>\n",
       "      <td>-0.126573</td>\n",
       "      <td>-0.408990</td>\n",
       "      <td>-0.169825</td>\n",
       "      <td>-0.007385</td>\n",
       "      <td>-0.235524</td>\n",
       "      <td>0.369205</td>\n",
       "      <td>-0.018608</td>\n",
       "      <td>0.058210</td>\n",
       "      <td>0.917898</td>\n",
       "      <td>0.704568</td>\n",
       "      <td>0.211497</td>\n",
       "      <td>0.046812</td>\n",
       "      <td>-0.196949</td>\n",
       "      <td>-0.053000</td>\n",
       "      <td>-0.146590</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>-0.269394</td>\n",
       "      <td>-0.232381</td>\n",
       "      <td>-0.272015</td>\n",
       "      <td>0.317240</td>\n",
       "      <td>-0.649360</td>\n",
       "      <td>0.758055</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.065853</td>\n",
       "      <td>0.265808</td>\n",
       "      <td>0.526169</td>\n",
       "      <td>0.245021</td>\n",
       "      <td>-0.200808</td>\n",
       "      <td>-0.059394</td>\n",
       "      <td>0.078007</td>\n",
       "      <td>0.251703</td>\n",
       "      <td>-0.186546</td>\n",
       "      <td>-0.322430</td>\n",
       "      <td>-0.305949</td>\n",
       "      <td>-0.280958</td>\n",
       "      <td>-0.271302</td>\n",
       "      <td>-0.430372</td>\n",
       "      <td>0.057687</td>\n",
       "      <td>-0.056453</td>\n",
       "      <td>-0.289901</td>\n",
       "      <td>0.370606</td>\n",
       "      <td>-0.298557</td>\n",
       "      <td>-0.032962</td>\n",
       "      <td>-0.551898</td>\n",
       "      <td>-0.105171</td>\n",
       "      <td>0.020347</td>\n",
       "      <td>0.110112</td>\n",
       "      <td>0.112550</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>-0.090431</td>\n",
       "      <td>0.552158</td>\n",
       "      <td>-0.336314</td>\n",
       "      <td>0.304919</td>\n",
       "      <td>-0.255759</td>\n",
       "      <td>0.283298</td>\n",
       "      <td>-0.145429</td>\n",
       "      <td>0.142749</td>\n",
       "      <td>0.105507</td>\n",
       "      <td>-0.008180</td>\n",
       "      <td>0.190217</td>\n",
       "      <td>0.051531</td>\n",
       "      <td>-0.350450</td>\n",
       "      <td>0.141987</td>\n",
       "      <td>-0.128540</td>\n",
       "      <td>0.032890</td>\n",
       "      <td>0.264250</td>\n",
       "      <td>0.729868</td>\n",
       "      <td>-0.731731</td>\n",
       "      <td>0.451732</td>\n",
       "      <td>0.159336</td>\n",
       "      <td>-0.307120</td>\n",
       "      <td>0.290921</td>\n",
       "      <td>0.197965</td>\n",
       "      <td>-0.158376</td>\n",
       "      <td>0.134121</td>\n",
       "      <td>-0.660091</td>\n",
       "      <td>-0.177486</td>\n",
       "      <td>-0.673690</td>\n",
       "      <td>0.468199</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899999</td>\n",
       "      <td>900000</td>\n",
       "      <td>0.150622</td>\n",
       "      <td>-0.450330</td>\n",
       "      <td>-0.065305</td>\n",
       "      <td>-0.101811</td>\n",
       "      <td>-0.146905</td>\n",
       "      <td>-0.018670</td>\n",
       "      <td>0.067131</td>\n",
       "      <td>0.205908</td>\n",
       "      <td>-0.039224</td>\n",
       "      <td>-0.742899</td>\n",
       "      <td>-0.107249</td>\n",
       "      <td>-0.127342</td>\n",
       "      <td>-0.417569</td>\n",
       "      <td>-0.064787</td>\n",
       "      <td>-0.142580</td>\n",
       "      <td>-0.072136</td>\n",
       "      <td>0.243392</td>\n",
       "      <td>0.255417</td>\n",
       "      <td>-0.154353</td>\n",
       "      <td>-0.077497</td>\n",
       "      <td>0.226301</td>\n",
       "      <td>-0.203331</td>\n",
       "      <td>-0.137239</td>\n",
       "      <td>-0.079588</td>\n",
       "      <td>-0.001172</td>\n",
       "      <td>-0.301632</td>\n",
       "      <td>0.387532</td>\n",
       "      <td>0.169814</td>\n",
       "      <td>0.245029</td>\n",
       "      <td>0.258662</td>\n",
       "      <td>0.565157</td>\n",
       "      <td>-0.022342</td>\n",
       "      <td>-0.015410</td>\n",
       "      <td>-0.365229</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>-0.182885</td>\n",
       "      <td>0.019374</td>\n",
       "      <td>0.100919</td>\n",
       "      <td>0.116496</td>\n",
       "      <td>-0.270430</td>\n",
       "      <td>0.211610</td>\n",
       "      <td>-0.322227</td>\n",
       "      <td>0.375522</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>-0.438312</td>\n",
       "      <td>0.035654</td>\n",
       "      <td>0.550800</td>\n",
       "      <td>0.208837</td>\n",
       "      <td>-0.343663</td>\n",
       "      <td>0.213310</td>\n",
       "      <td>0.089621</td>\n",
       "      <td>0.021496</td>\n",
       "      <td>-0.065914</td>\n",
       "      <td>0.161894</td>\n",
       "      <td>-0.355840</td>\n",
       "      <td>-0.321583</td>\n",
       "      <td>-0.013297</td>\n",
       "      <td>-0.060369</td>\n",
       "      <td>0.058977</td>\n",
       "      <td>-0.108302</td>\n",
       "      <td>-0.008760</td>\n",
       "      <td>0.114980</td>\n",
       "      <td>-0.095258</td>\n",
       "      <td>0.295492</td>\n",
       "      <td>-0.653108</td>\n",
       "      <td>-0.194717</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.674352</td>\n",
       "      <td>0.127985</td>\n",
       "      <td>-0.164916</td>\n",
       "      <td>-0.111851</td>\n",
       "      <td>0.351558</td>\n",
       "      <td>-0.373818</td>\n",
       "      <td>0.081576</td>\n",
       "      <td>0.022877</td>\n",
       "      <td>-0.048952</td>\n",
       "      <td>-0.036146</td>\n",
       "      <td>0.419958</td>\n",
       "      <td>-0.077609</td>\n",
       "      <td>0.152953</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>-0.142513</td>\n",
       "      <td>-0.191623</td>\n",
       "      <td>0.273999</td>\n",
       "      <td>-0.037756</td>\n",
       "      <td>-0.040002</td>\n",
       "      <td>0.258319</td>\n",
       "      <td>0.427493</td>\n",
       "      <td>-0.402164</td>\n",
       "      <td>0.167696</td>\n",
       "      <td>0.068482</td>\n",
       "      <td>0.238024</td>\n",
       "      <td>-0.054250</td>\n",
       "      <td>-0.287568</td>\n",
       "      <td>-0.454337</td>\n",
       "      <td>0.195452</td>\n",
       "      <td>-0.266824</td>\n",
       "      <td>-0.094228</td>\n",
       "      <td>-0.257902</td>\n",
       "      <td>0.540924</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id     w2v_1     w2v_2     w2v_3     w2v_4     w2v_5     w2v_6     w2v_7     w2v_8  \\\n",
       "0             1  0.317886 -0.526415 -0.055048 -0.528955 -0.096361  0.162796  0.048159  0.119717   \n",
       "1             2  0.325203 -0.367411 -0.279791 -0.093693 -0.122351  0.297370 -0.206585  0.076459   \n",
       "2             3  0.198565 -0.586632 -0.262671  0.027136  0.100215  0.322511 -0.073465  0.222631   \n",
       "3             4  0.151257 -0.374335 -0.428679 -0.272160  0.215631  0.185658 -0.006619  0.316194   \n",
       "4             5 -0.028938 -0.318149 -0.180849 -0.472022 -0.194668  0.347011 -0.163237  0.042353   \n",
       "...         ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995   899996  0.243240 -0.641140 -0.221246 -0.165119 -0.219668  0.288953  0.206380  0.095649   \n",
       "899996   899997  0.515683 -0.657982 -0.141998 -0.022650 -0.484071  0.476730 -0.027828  0.392949   \n",
       "899997   899998  0.164928 -0.611155 -0.510723 -0.230253  0.287806 -0.177192  0.253795  0.792483   \n",
       "899998   899999  0.138514 -0.893192 -0.337032 -0.053397 -0.093038  0.053808 -0.188455  0.255990   \n",
       "899999   900000  0.150622 -0.450330 -0.065305 -0.101811 -0.146905 -0.018670  0.067131  0.205908   \n",
       "\n",
       "           w2v_9    w2v_10    w2v_11    w2v_12    w2v_13    w2v_14    w2v_15    w2v_16    w2v_17  \\\n",
       "0      -0.040841 -0.436568 -0.032624  0.112490 -0.325787  0.138070 -0.241391 -0.253797  0.129697   \n",
       "1       0.294193 -0.571907 -0.055274  0.130691 -0.330579  0.090058 -0.629760 -0.099451  0.191704   \n",
       "2       0.041526 -0.586876 -0.160328  0.126489 -0.086501 -0.182446 -0.321912  0.072478  0.058277   \n",
       "3       0.247443 -0.474306 -0.171284 -0.099072 -0.241625 -0.016837 -0.539686  0.092095  0.188419   \n",
       "4      -0.068275 -0.385615 -0.146023  0.317552 -0.404162  0.115845 -0.127442 -0.034000  0.009917   \n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995  0.015800 -0.321669 -0.062357  0.376121 -0.482505  0.007533 -0.181318 -0.027182 -0.199615   \n",
       "899996  0.037568 -0.851055  0.041133  0.077036 -0.276592  0.328957 -0.328961  0.150223  0.169104   \n",
       "899997  0.342007 -1.009153 -0.057842 -0.050730 -0.510912 -0.050470 -0.646040  0.146405  0.271567   \n",
       "899998  0.068474 -0.824961 -0.025947 -0.210074 -0.005053  0.324245 -0.379235 -0.375546 -0.131549   \n",
       "899999 -0.039224 -0.742899 -0.107249 -0.127342 -0.417569 -0.064787 -0.142580 -0.072136  0.243392   \n",
       "\n",
       "          w2v_18    w2v_19    w2v_20    w2v_21    w2v_22    w2v_23    w2v_24    w2v_25    w2v_26  \\\n",
       "0       0.532830 -0.220061 -0.047494  0.281527 -0.292721 -0.180316 -0.217422 -0.299926 -0.409515   \n",
       "1       0.427102 -0.219477 -0.062363 -0.117611 -0.434651  0.044292 -0.188073  0.070248 -0.450696   \n",
       "2       0.273296  0.040683 -0.374089 -0.047131  0.089502 -0.090019 -0.217124  0.050214 -0.337977   \n",
       "3       0.393323  0.016091 -0.133066 -0.085917 -0.090432 -0.349310 -0.248880 -0.120003 -0.250547   \n",
       "4       0.325168  0.035940 -0.300432  0.179213 -0.001800  0.025641 -0.198190 -0.156287 -0.267601   \n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995  0.301060  0.075792 -0.086011 -0.057258 -0.043454 -0.128335 -0.439783 -0.041851 -0.098818   \n",
       "899996  0.631069 -0.180701 -0.014098 -0.170441 -0.357181 -0.084747 -0.019092 -0.125442 -0.529127   \n",
       "899997  0.687615  0.043600 -0.034516  0.118729 -0.031271 -0.084964  0.257087 -0.032235 -0.299957   \n",
       "899998  0.898178  0.029652 -0.220556  0.242753 -0.126573 -0.408990 -0.169825 -0.007385 -0.235524   \n",
       "899999  0.255417 -0.154353 -0.077497  0.226301 -0.203331 -0.137239 -0.079588 -0.001172 -0.301632   \n",
       "\n",
       "          w2v_27    w2v_28    w2v_29    w2v_30    w2v_31    w2v_32    w2v_33    w2v_34    w2v_35  \\\n",
       "0       0.536922 -0.150833  0.608378  0.519931  0.327593 -0.047129 -0.153875 -0.308788 -0.009804   \n",
       "1       0.693025  0.267765  0.488802  0.422140  0.118047 -0.051940 -0.085290 -0.212455  0.080108   \n",
       "2       0.150414  0.182220  0.169077  0.158828  0.159270  0.144751 -0.066186 -0.040247 -0.032578   \n",
       "3       0.428762  0.223570  0.324844  0.603414  0.053672  0.051973 -0.119606 -0.243262 -0.225287   \n",
       "4       0.278072  0.106210  0.427624  0.429296  0.367329 -0.092904 -0.162959 -0.244452 -0.024666   \n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995  0.259661  0.302658  0.445060  0.401655  0.134029  0.212844 -0.153095 -0.261479 -0.324027   \n",
       "899996  0.183871  0.093437  0.429113  0.468045  0.426172  0.068719 -0.193521 -0.202167  0.135101   \n",
       "899997  0.222076  0.313168  0.297853  0.149366  0.831230  0.045845 -0.063910 -0.346297 -0.079265   \n",
       "899998  0.369205 -0.018608  0.058210  0.917898  0.704568  0.211497  0.046812 -0.196949 -0.053000   \n",
       "899999  0.387532  0.169814  0.245029  0.258662  0.565157 -0.022342 -0.015410 -0.365229 -0.020203   \n",
       "\n",
       "          w2v_36    w2v_37    w2v_38    w2v_39    w2v_40    w2v_41    w2v_42    w2v_43    w2v_44  \\\n",
       "0      -0.284649  0.363233 -0.126311 -0.099959 -0.399930 -0.008959 -0.557281  0.208298  0.002441   \n",
       "1      -0.335424  0.391596  0.204405 -0.231755 -0.379702  0.040530 -0.422194  0.579996 -0.274050   \n",
       "2      -0.190561  0.008836  0.151353  0.133934 -0.368623  0.134417 -0.560351  0.443211 -0.094252   \n",
       "3      -0.183186 -0.009053  0.117919  0.068445 -0.254893  0.050980 -0.525274  0.460843  0.026366   \n",
       "4       0.098418 -0.005921  0.200096 -0.214554 -0.367227 -0.248917 -0.543416  0.311109 -0.026497   \n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995 -0.061661  0.060992  0.016667  0.216859 -0.249309  0.137882 -0.415835  0.189478 -0.120426   \n",
       "899996 -0.009935  0.324440  0.064370  0.008565 -0.611104 -0.028211 -0.585192  0.607013 -0.167726   \n",
       "899997 -0.371726 -0.135079 -0.008642  0.332060 -0.509973 -0.018216 -0.410527  0.529129 -0.119675   \n",
       "899998 -0.146590  0.061100 -0.269394 -0.232381 -0.272015  0.317240 -0.649360  0.758055 -0.023546   \n",
       "899999 -0.182885  0.019374  0.100919  0.116496 -0.270430  0.211610 -0.322227  0.375522  0.000777   \n",
       "\n",
       "          w2v_45    w2v_46    w2v_47    w2v_48    w2v_49    w2v_50    w2v_51    w2v_52    w2v_53  \\\n",
       "0       0.229310 -0.095422  0.478953  0.017111 -0.488578  0.167485  0.055130 -0.015642  0.154260   \n",
       "1      -0.108324  0.196841  0.737161  0.418932 -0.240259  0.285742 -0.203938  0.192743  0.287305   \n",
       "2      -0.098049 -0.137316  0.347831  0.367783 -0.088025  0.299211  0.222399  0.057067 -0.206091   \n",
       "3      -0.001258  0.202677  0.586647  0.369458 -0.313746  0.418417 -0.087917  0.253184  0.170249   \n",
       "4      -0.213172 -0.128181  0.477916 -0.170481 -0.177959  0.065573 -0.001948  0.026924  0.113313   \n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995 -0.207229  0.083842  0.450378 -0.037286 -0.507741  0.396173 -0.234831 -0.050506 -0.083878   \n",
       "899996 -0.367620  0.280053  0.372864  0.390724 -0.572494 -0.085832 -0.021984  0.092908  0.086178   \n",
       "899997 -0.734006  0.012256  0.538561  0.331921 -0.513746  0.465859  0.263832 -0.041647 -0.058444   \n",
       "899998 -0.065853  0.265808  0.526169  0.245021 -0.200808 -0.059394  0.078007  0.251703 -0.186546   \n",
       "899999 -0.438312  0.035654  0.550800  0.208837 -0.343663  0.213310  0.089621  0.021496 -0.065914   \n",
       "\n",
       "          w2v_54    w2v_55    w2v_56    w2v_57    w2v_58    w2v_59    w2v_60    w2v_61    w2v_62  \\\n",
       "0       0.166007 -0.038812 -0.298765 -0.494071 -0.282491  0.016670 -0.373491 -0.480105  0.116221   \n",
       "1      -0.290213 -0.273336  0.011194 -0.399577 -0.422319 -0.018834 -0.419183 -0.187218  0.080601   \n",
       "2       0.319835 -0.401369 -0.390524 -0.407110 -0.227170 -0.037186 -0.336939 -0.046157 -0.171637   \n",
       "3       0.022453 -0.494399 -0.131953 -0.218401 -0.447379  0.219920 -0.246410  0.006450  0.083894   \n",
       "4       0.161093 -0.144608 -0.357767 -0.131327 -0.406192 -0.057586 -0.251472 -0.062627  0.056552   \n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995  0.329780 -0.137773 -0.437423 -0.163581 -0.302451  0.316948 -0.139210 -0.242297 -0.023869   \n",
       "899996 -0.350186 -0.464652 -0.155343  0.064242  0.015871  0.246716 -0.066027  0.141708  0.130953   \n",
       "899997  0.257928 -0.809038 -0.436497 -0.076378 -0.035019  0.426222 -0.380657 -0.186435  0.295572   \n",
       "899998 -0.322430 -0.305949 -0.280958 -0.271302 -0.430372  0.057687 -0.056453 -0.289901  0.370606   \n",
       "899999  0.161894 -0.355840 -0.321583 -0.013297 -0.060369  0.058977 -0.108302 -0.008760  0.114980   \n",
       "\n",
       "          w2v_63    w2v_64    w2v_65    w2v_66    w2v_67    w2v_68    w2v_69    w2v_70    w2v_71  \\\n",
       "0      -0.369969  0.233782 -0.427903 -0.053939  0.015587  0.521378 -0.069436 -0.167746 -0.121342   \n",
       "1      -0.404032 -0.018926 -0.800364  0.318652 -0.158947  0.312360 -0.095327 -0.231974 -0.330422   \n",
       "2      -0.315089  0.435404 -0.487106 -0.021420  0.050940  0.625776 -0.113916 -0.152975 -0.259297   \n",
       "3      -0.352079 -0.056614 -0.610225  0.147180 -0.193593  0.466122 -0.173300 -0.265374 -0.296355   \n",
       "4      -0.310126  0.402394 -0.536030 -0.194220 -0.035663  0.403383 -0.057518 -0.031682 -0.139198   \n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995 -0.287063  0.144579 -0.587444  0.211204 -0.085344  0.542422 -0.173311 -0.332935 -0.437832   \n",
       "899996 -0.532892  0.039589 -0.582114  0.255843  0.055905  0.565095 -0.059821 -0.004112 -0.015955   \n",
       "899997 -0.060447  0.605304 -0.643241 -0.348190  0.233054  0.931285  0.228999 -0.215847 -0.171887   \n",
       "899998 -0.298557 -0.032962 -0.551898 -0.105171  0.020347  0.110112  0.112550  0.000826 -0.090431   \n",
       "899999 -0.095258  0.295492 -0.653108 -0.194717  0.003230  0.674352  0.127985 -0.164916 -0.111851   \n",
       "\n",
       "          w2v_72    w2v_73    w2v_74    w2v_75    w2v_76    w2v_77    w2v_78    w2v_79    w2v_80  \\\n",
       "0       0.047510 -0.453079 -0.044746 -0.312390  0.092713 -0.270386  0.118622  0.128435  0.047801   \n",
       "1       0.121357 -0.249703  0.121524 -0.148256  0.157988 -0.415097  0.073084  0.003465 -0.329378   \n",
       "2       0.502868 -0.481878  0.038209 -0.421211 -0.215066 -0.276196  0.333828  0.009564  0.081381   \n",
       "3       0.281162 -0.299944  0.284063 -0.218795 -0.251554 -0.383884  0.073274  0.035057 -0.054880   \n",
       "4       0.312265 -0.296792 -0.067986 -0.307334 -0.157313 -0.079045  0.036528 -0.111514 -0.276553   \n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995  0.077496 -0.258561 -0.007533 -0.308809 -0.254471 -0.026521  0.219235 -0.010523 -0.273479   \n",
       "899996  0.447682 -0.311238  0.313101 -0.060271  0.095708 -0.186104  0.246352  0.109228 -0.058353   \n",
       "899997  0.580200 -0.366655 -0.020602 -0.157568  0.010036 -0.173951  0.649639  0.293386  0.513225   \n",
       "899998  0.552158 -0.336314  0.304919 -0.255759  0.283298 -0.145429  0.142749  0.105507 -0.008180   \n",
       "899999  0.351558 -0.373818  0.081576  0.022877 -0.048952 -0.036146  0.419958 -0.077609  0.152953   \n",
       "\n",
       "          w2v_81    w2v_82    w2v_83    w2v_84    w2v_85    w2v_86    w2v_87    w2v_88    w2v_89  \\\n",
       "0       0.021427 -0.179088 -0.134783  0.039494 -0.272888 -0.009532  0.177008  0.685854 -0.450034   \n",
       "1       0.122206 -0.130705 -0.119785  0.072925 -0.366768 -0.099851  0.126987  0.645205 -0.032412   \n",
       "2      -0.134869 -0.039792 -0.095843  0.164616 -0.148636  0.063577  0.637608  0.581705 -0.248123   \n",
       "3       0.183362  0.023791 -0.213668  0.311855 -0.361867  0.152611  0.246283  0.489768 -0.208535   \n",
       "4       0.174941 -0.330068  0.026196  0.163671 -0.126165 -0.056569  0.279290  0.349940 -0.562776   \n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995  0.216129  0.047848 -0.328360 -0.131239 -0.226935  0.088835  0.304606  0.662196 -0.328890   \n",
       "899996 -0.208268  0.065131 -0.608763  0.306291 -0.244444  0.174844  0.579212  0.374679 -0.423249   \n",
       "899997 -0.119581 -0.336632 -0.419274  0.582473 -0.250532  0.082413  0.406791  0.701027 -0.162323   \n",
       "899998  0.190217  0.051531 -0.350450  0.141987 -0.128540  0.032890  0.264250  0.729868 -0.731731   \n",
       "899999  0.006795 -0.142513 -0.191623  0.273999 -0.037756 -0.040002  0.258319  0.427493 -0.402164   \n",
       "\n",
       "          w2v_90    w2v_91    w2v_92    w2v_93    w2v_94    w2v_95    w2v_96    w2v_97    w2v_98  \\\n",
       "0       0.525002  0.347635  0.345707  0.180106 -0.140729 -0.212545  0.388303 -0.458613 -0.068500   \n",
       "1       0.242342  0.211180 -0.006131 -0.254851  0.070728 -0.232397 -0.214553 -0.202904  0.094728   \n",
       "2       0.566989  0.145143 -0.153065  0.301695 -0.154103 -0.555241 -0.109893 -0.180794 -0.198200   \n",
       "3       0.408645  0.242020 -0.153373 -0.096918 -0.067699 -0.415370 -0.180304 -0.016965  0.106257   \n",
       "4       0.290090  0.236660 -0.012676  0.011908  0.016177 -0.135031  0.079853 -0.236369 -0.080729   \n",
       "...          ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "899995  0.376784  0.046671  0.114476  0.057655 -0.170715 -0.316756 -0.005456 -0.289580 -0.164895   \n",
       "899996  0.662227  0.093215  0.094639 -0.043676 -0.219571 -0.526521  0.017701 -0.065823  0.079533   \n",
       "899997  0.307486  0.176571  0.356786  0.074302 -0.374451 -0.977068  0.247008 -0.047853 -0.199047   \n",
       "899998  0.451732  0.159336 -0.307120  0.290921  0.197965 -0.158376  0.134121 -0.660091 -0.177486   \n",
       "899999  0.167696  0.068482  0.238024 -0.054250 -0.287568 -0.454337  0.195452 -0.266824 -0.094228   \n",
       "\n",
       "          w2v_99   w2v_100  age  gender   y  \n",
       "0      -0.613453  0.725146    4       1   3  \n",
       "1      -0.275755  0.398690   10       1   9  \n",
       "2      -0.214605  0.468438    7       2  16  \n",
       "3      -0.434509  0.394375    5       1   4  \n",
       "4      -0.466101  0.203967    4       1   3  \n",
       "...          ...       ...  ...     ...  ..  \n",
       "899995 -0.630627  0.577789    5       1   4  \n",
       "899996 -0.551853  0.741822    3       2  12  \n",
       "899997  0.170333  0.358967    4       2  13  \n",
       "899998 -0.673690  0.468199    3       1   2  \n",
       "899999 -0.257902  0.540924    3       2  12  \n",
       "\n",
       "[900000 rows x 104 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:36:20,286 - mlpipeline.train - INFO - using_fe_df: train_fe_df.feather, use_label: y, is_eval: True, model_type: ensemble, model_name: xgb, use_log: False, use_std: True, use_cv: True, n_splits: 2\n",
      "2020-05-15 16:36:20,451 - mlpipeline.train - INFO - _train_pipeline_ensemble_and_linear开始\n",
      "2020-05-15 16:36:20,456 - mlpipeline.train - INFO - 连续性特征数量: 100\n",
      "2020-05-15 16:36:20,457 - mlpipeline.train - INFO - 离散性特征数量: 0\n",
      "2020-05-15 16:36:21,895 - mlpipeline.train - INFO - eval参数: {'objective': 'multi:softmax', 'booster': 'gbtree', 'eta': 0.15, 'eval_metric': ['mlogloss'], 'nthread': 15, 'random_state': 2019, 'tree_method': 'auto', 'n_estimators': 1000, 'device': 'cpu', 'num_class': 20, 'max_leaves': 32, 'subsample': 0.9, 'colsample_bytree': 0.9, 'min_data_in_leaf': 40, 'lambda': 1.0, 'alpha': 1.0}\n",
      "2020-05-15 16:36:21,896 - mlpipeline.train - INFO - _get_cv_folds开始\n",
      "2020-05-15 16:36:21,898 - mlpipeline.train - INFO - _get_cv_folds已完成，共用时0:00:00\n",
      "2020-05-15 16:36:23,070 - utils.utils - INFO - standard_scale开始\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "2020-05-15 16:36:48,865 - utils.utils - INFO - standard_scale已完成，共用时0:00:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.83766\tvalidation_1-mlogloss:2.84314\n",
      "Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-mlogloss:2.72185\tvalidation_1-mlogloss:2.73148\n",
      "[2]\tvalidation_0-mlogloss:2.62847\tvalidation_1-mlogloss:2.64185\n",
      "[3]\tvalidation_0-mlogloss:2.55161\tvalidation_1-mlogloss:2.56833\n",
      "[4]\tvalidation_0-mlogloss:2.48569\tvalidation_1-mlogloss:2.50554\n",
      "[5]\tvalidation_0-mlogloss:2.42916\tvalidation_1-mlogloss:2.45206\n",
      "[6]\tvalidation_0-mlogloss:2.37942\tvalidation_1-mlogloss:2.40511\n",
      "[7]\tvalidation_0-mlogloss:2.33567\tvalidation_1-mlogloss:2.36424\n",
      "[8]\tvalidation_0-mlogloss:2.29666\tvalidation_1-mlogloss:2.32785\n",
      "[9]\tvalidation_0-mlogloss:2.26168\tvalidation_1-mlogloss:2.29547\n",
      "[10]\tvalidation_0-mlogloss:2.22982\tvalidation_1-mlogloss:2.26611\n",
      "[11]\tvalidation_0-mlogloss:2.20113\tvalidation_1-mlogloss:2.23992\n",
      "[12]\tvalidation_0-mlogloss:2.17512\tvalidation_1-mlogloss:2.21633\n",
      "[13]\tvalidation_0-mlogloss:2.15101\tvalidation_1-mlogloss:2.19458\n",
      "[14]\tvalidation_0-mlogloss:2.12912\tvalidation_1-mlogloss:2.17488\n",
      "[15]\tvalidation_0-mlogloss:2.10870\tvalidation_1-mlogloss:2.15682\n",
      "[16]\tvalidation_0-mlogloss:2.08999\tvalidation_1-mlogloss:2.14040\n",
      "[17]\tvalidation_0-mlogloss:2.07245\tvalidation_1-mlogloss:2.12502\n",
      "[18]\tvalidation_0-mlogloss:2.05605\tvalidation_1-mlogloss:2.11090\n",
      "[19]\tvalidation_0-mlogloss:2.04075\tvalidation_1-mlogloss:2.09789\n",
      "[20]\tvalidation_0-mlogloss:2.02626\tvalidation_1-mlogloss:2.08560\n",
      "[21]\tvalidation_0-mlogloss:2.01276\tvalidation_1-mlogloss:2.07435\n",
      "[22]\tvalidation_0-mlogloss:2.00002\tvalidation_1-mlogloss:2.06382\n",
      "[23]\tvalidation_0-mlogloss:1.98798\tvalidation_1-mlogloss:2.05399\n",
      "[24]\tvalidation_0-mlogloss:1.97634\tvalidation_1-mlogloss:2.04455\n",
      "[25]\tvalidation_0-mlogloss:1.96536\tvalidation_1-mlogloss:2.03575\n",
      "[26]\tvalidation_0-mlogloss:1.95500\tvalidation_1-mlogloss:2.02747\n",
      "[27]\tvalidation_0-mlogloss:1.94516\tvalidation_1-mlogloss:2.01981\n",
      "[28]\tvalidation_0-mlogloss:1.93578\tvalidation_1-mlogloss:2.01256\n",
      "[29]\tvalidation_0-mlogloss:1.92677\tvalidation_1-mlogloss:2.00570\n",
      "[30]\tvalidation_0-mlogloss:1.91823\tvalidation_1-mlogloss:1.99936\n",
      "[31]\tvalidation_0-mlogloss:1.90996\tvalidation_1-mlogloss:1.99320\n",
      "[32]\tvalidation_0-mlogloss:1.90213\tvalidation_1-mlogloss:1.98742\n",
      "[33]\tvalidation_0-mlogloss:1.89440\tvalidation_1-mlogloss:1.98186\n",
      "[34]\tvalidation_0-mlogloss:1.88702\tvalidation_1-mlogloss:1.97654\n",
      "[35]\tvalidation_0-mlogloss:1.87984\tvalidation_1-mlogloss:1.97149\n",
      "[36]\tvalidation_0-mlogloss:1.87297\tvalidation_1-mlogloss:1.96674\n",
      "[37]\tvalidation_0-mlogloss:1.86634\tvalidation_1-mlogloss:1.96219\n",
      "[38]\tvalidation_0-mlogloss:1.85994\tvalidation_1-mlogloss:1.95787\n",
      "[39]\tvalidation_0-mlogloss:1.85375\tvalidation_1-mlogloss:1.95376\n",
      "[40]\tvalidation_0-mlogloss:1.84766\tvalidation_1-mlogloss:1.94983\n",
      "[41]\tvalidation_0-mlogloss:1.84182\tvalidation_1-mlogloss:1.94606\n",
      "[42]\tvalidation_0-mlogloss:1.83607\tvalidation_1-mlogloss:1.94233\n",
      "[43]\tvalidation_0-mlogloss:1.83041\tvalidation_1-mlogloss:1.93876\n",
      "[44]\tvalidation_0-mlogloss:1.82496\tvalidation_1-mlogloss:1.93534\n",
      "[45]\tvalidation_0-mlogloss:1.81956\tvalidation_1-mlogloss:1.93197\n",
      "[46]\tvalidation_0-mlogloss:1.81445\tvalidation_1-mlogloss:1.92882\n",
      "[47]\tvalidation_0-mlogloss:1.80937\tvalidation_1-mlogloss:1.92582\n",
      "[48]\tvalidation_0-mlogloss:1.80444\tvalidation_1-mlogloss:1.92293\n",
      "[49]\tvalidation_0-mlogloss:1.79967\tvalidation_1-mlogloss:1.92016\n",
      "[50]\tvalidation_0-mlogloss:1.79494\tvalidation_1-mlogloss:1.91742\n",
      "[51]\tvalidation_0-mlogloss:1.79030\tvalidation_1-mlogloss:1.91472\n",
      "[52]\tvalidation_0-mlogloss:1.78579\tvalidation_1-mlogloss:1.91224\n",
      "[53]\tvalidation_0-mlogloss:1.78137\tvalidation_1-mlogloss:1.90980\n",
      "[54]\tvalidation_0-mlogloss:1.77697\tvalidation_1-mlogloss:1.90742\n",
      "[55]\tvalidation_0-mlogloss:1.77275\tvalidation_1-mlogloss:1.90517\n",
      "[56]\tvalidation_0-mlogloss:1.76843\tvalidation_1-mlogloss:1.90286\n",
      "[57]\tvalidation_0-mlogloss:1.76440\tvalidation_1-mlogloss:1.90079\n",
      "[58]\tvalidation_0-mlogloss:1.76030\tvalidation_1-mlogloss:1.89864\n",
      "[59]\tvalidation_0-mlogloss:1.75630\tvalidation_1-mlogloss:1.89664\n",
      "[60]\tvalidation_0-mlogloss:1.75234\tvalidation_1-mlogloss:1.89461\n",
      "[61]\tvalidation_0-mlogloss:1.74851\tvalidation_1-mlogloss:1.89266\n",
      "[62]\tvalidation_0-mlogloss:1.74460\tvalidation_1-mlogloss:1.89075\n",
      "[63]\tvalidation_0-mlogloss:1.74082\tvalidation_1-mlogloss:1.88891\n",
      "[64]\tvalidation_0-mlogloss:1.73710\tvalidation_1-mlogloss:1.88713\n",
      "[65]\tvalidation_0-mlogloss:1.73343\tvalidation_1-mlogloss:1.88539\n",
      "[66]\tvalidation_0-mlogloss:1.72980\tvalidation_1-mlogloss:1.88369\n",
      "[67]\tvalidation_0-mlogloss:1.72622\tvalidation_1-mlogloss:1.88202\n",
      "[68]\tvalidation_0-mlogloss:1.72275\tvalidation_1-mlogloss:1.88044\n",
      "[69]\tvalidation_0-mlogloss:1.71924\tvalidation_1-mlogloss:1.87886\n",
      "[70]\tvalidation_0-mlogloss:1.71583\tvalidation_1-mlogloss:1.87733\n",
      "[71]\tvalidation_0-mlogloss:1.71241\tvalidation_1-mlogloss:1.87581\n",
      "[72]\tvalidation_0-mlogloss:1.70915\tvalidation_1-mlogloss:1.87434\n",
      "[73]\tvalidation_0-mlogloss:1.70583\tvalidation_1-mlogloss:1.87292\n",
      "[74]\tvalidation_0-mlogloss:1.70257\tvalidation_1-mlogloss:1.87153\n",
      "[75]\tvalidation_0-mlogloss:1.69934\tvalidation_1-mlogloss:1.87018\n",
      "[76]\tvalidation_0-mlogloss:1.69610\tvalidation_1-mlogloss:1.86885\n",
      "[77]\tvalidation_0-mlogloss:1.69287\tvalidation_1-mlogloss:1.86752\n",
      "[78]\tvalidation_0-mlogloss:1.68974\tvalidation_1-mlogloss:1.86627\n",
      "[79]\tvalidation_0-mlogloss:1.68661\tvalidation_1-mlogloss:1.86498\n",
      "[80]\tvalidation_0-mlogloss:1.68359\tvalidation_1-mlogloss:1.86381\n",
      "[81]\tvalidation_0-mlogloss:1.68061\tvalidation_1-mlogloss:1.86265\n",
      "[82]\tvalidation_0-mlogloss:1.67749\tvalidation_1-mlogloss:1.86137\n",
      "[83]\tvalidation_0-mlogloss:1.67453\tvalidation_1-mlogloss:1.86022\n",
      "[84]\tvalidation_0-mlogloss:1.67164\tvalidation_1-mlogloss:1.85912\n",
      "[85]\tvalidation_0-mlogloss:1.66867\tvalidation_1-mlogloss:1.85800\n",
      "[86]\tvalidation_0-mlogloss:1.66575\tvalidation_1-mlogloss:1.85687\n",
      "[87]\tvalidation_0-mlogloss:1.66280\tvalidation_1-mlogloss:1.85578\n",
      "[88]\tvalidation_0-mlogloss:1.65995\tvalidation_1-mlogloss:1.85472\n",
      "[89]\tvalidation_0-mlogloss:1.65718\tvalidation_1-mlogloss:1.85373\n",
      "[90]\tvalidation_0-mlogloss:1.65441\tvalidation_1-mlogloss:1.85277\n",
      "[91]\tvalidation_0-mlogloss:1.65171\tvalidation_1-mlogloss:1.85178\n",
      "[92]\tvalidation_0-mlogloss:1.64885\tvalidation_1-mlogloss:1.85078\n",
      "[93]\tvalidation_0-mlogloss:1.64604\tvalidation_1-mlogloss:1.84980\n",
      "[94]\tvalidation_0-mlogloss:1.64332\tvalidation_1-mlogloss:1.84887\n",
      "[95]\tvalidation_0-mlogloss:1.64063\tvalidation_1-mlogloss:1.84796\n",
      "[96]\tvalidation_0-mlogloss:1.63811\tvalidation_1-mlogloss:1.84708\n",
      "[97]\tvalidation_0-mlogloss:1.63538\tvalidation_1-mlogloss:1.84610\n",
      "[98]\tvalidation_0-mlogloss:1.63273\tvalidation_1-mlogloss:1.84517\n",
      "[99]\tvalidation_0-mlogloss:1.63010\tvalidation_1-mlogloss:1.84434\n",
      "[100]\tvalidation_0-mlogloss:1.62755\tvalidation_1-mlogloss:1.84353\n",
      "[101]\tvalidation_0-mlogloss:1.62506\tvalidation_1-mlogloss:1.84276\n",
      "[102]\tvalidation_0-mlogloss:1.62256\tvalidation_1-mlogloss:1.84194\n",
      "[103]\tvalidation_0-mlogloss:1.62003\tvalidation_1-mlogloss:1.84117\n",
      "[104]\tvalidation_0-mlogloss:1.61755\tvalidation_1-mlogloss:1.84038\n",
      "[105]\tvalidation_0-mlogloss:1.61502\tvalidation_1-mlogloss:1.83956\n",
      "[106]\tvalidation_0-mlogloss:1.61258\tvalidation_1-mlogloss:1.83882\n",
      "[107]\tvalidation_0-mlogloss:1.61016\tvalidation_1-mlogloss:1.83807\n",
      "[108]\tvalidation_0-mlogloss:1.60778\tvalidation_1-mlogloss:1.83739\n",
      "[109]\tvalidation_0-mlogloss:1.60531\tvalidation_1-mlogloss:1.83664\n",
      "[110]\tvalidation_0-mlogloss:1.60289\tvalidation_1-mlogloss:1.83593\n",
      "[111]\tvalidation_0-mlogloss:1.60051\tvalidation_1-mlogloss:1.83521\n",
      "[112]\tvalidation_0-mlogloss:1.59822\tvalidation_1-mlogloss:1.83456\n",
      "[113]\tvalidation_0-mlogloss:1.59593\tvalidation_1-mlogloss:1.83387\n",
      "[114]\tvalidation_0-mlogloss:1.59371\tvalidation_1-mlogloss:1.83324\n",
      "[115]\tvalidation_0-mlogloss:1.59142\tvalidation_1-mlogloss:1.83262\n",
      "[116]\tvalidation_0-mlogloss:1.58920\tvalidation_1-mlogloss:1.83201\n",
      "[117]\tvalidation_0-mlogloss:1.58689\tvalidation_1-mlogloss:1.83133\n",
      "[118]\tvalidation_0-mlogloss:1.58469\tvalidation_1-mlogloss:1.83069\n",
      "[119]\tvalidation_0-mlogloss:1.58251\tvalidation_1-mlogloss:1.83011\n",
      "[120]\tvalidation_0-mlogloss:1.58026\tvalidation_1-mlogloss:1.82950\n",
      "[121]\tvalidation_0-mlogloss:1.57806\tvalidation_1-mlogloss:1.82891\n",
      "[122]\tvalidation_0-mlogloss:1.57588\tvalidation_1-mlogloss:1.82838\n",
      "[123]\tvalidation_0-mlogloss:1.57373\tvalidation_1-mlogloss:1.82781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]\tvalidation_0-mlogloss:1.57153\tvalidation_1-mlogloss:1.82722\n",
      "[125]\tvalidation_0-mlogloss:1.56925\tvalidation_1-mlogloss:1.82665\n",
      "[126]\tvalidation_0-mlogloss:1.56703\tvalidation_1-mlogloss:1.82604\n",
      "[127]\tvalidation_0-mlogloss:1.56489\tvalidation_1-mlogloss:1.82545\n",
      "[128]\tvalidation_0-mlogloss:1.56277\tvalidation_1-mlogloss:1.82491\n",
      "[129]\tvalidation_0-mlogloss:1.56063\tvalidation_1-mlogloss:1.82436\n",
      "[130]\tvalidation_0-mlogloss:1.55856\tvalidation_1-mlogloss:1.82385\n",
      "[131]\tvalidation_0-mlogloss:1.55650\tvalidation_1-mlogloss:1.82343\n",
      "[132]\tvalidation_0-mlogloss:1.55444\tvalidation_1-mlogloss:1.82292\n",
      "[133]\tvalidation_0-mlogloss:1.55245\tvalidation_1-mlogloss:1.82238\n",
      "[134]\tvalidation_0-mlogloss:1.55032\tvalidation_1-mlogloss:1.82181\n",
      "[135]\tvalidation_0-mlogloss:1.54823\tvalidation_1-mlogloss:1.82130\n",
      "[136]\tvalidation_0-mlogloss:1.54629\tvalidation_1-mlogloss:1.82083\n",
      "[137]\tvalidation_0-mlogloss:1.54435\tvalidation_1-mlogloss:1.82038\n",
      "[138]\tvalidation_0-mlogloss:1.54243\tvalidation_1-mlogloss:1.81989\n",
      "[139]\tvalidation_0-mlogloss:1.54040\tvalidation_1-mlogloss:1.81943\n",
      "[140]\tvalidation_0-mlogloss:1.53830\tvalidation_1-mlogloss:1.81897\n",
      "[141]\tvalidation_0-mlogloss:1.53627\tvalidation_1-mlogloss:1.81856\n",
      "[142]\tvalidation_0-mlogloss:1.53434\tvalidation_1-mlogloss:1.81813\n",
      "[143]\tvalidation_0-mlogloss:1.53246\tvalidation_1-mlogloss:1.81772\n",
      "[144]\tvalidation_0-mlogloss:1.53052\tvalidation_1-mlogloss:1.81731\n",
      "[145]\tvalidation_0-mlogloss:1.52854\tvalidation_1-mlogloss:1.81686\n",
      "[146]\tvalidation_0-mlogloss:1.52661\tvalidation_1-mlogloss:1.81648\n",
      "[147]\tvalidation_0-mlogloss:1.52472\tvalidation_1-mlogloss:1.81609\n",
      "[148]\tvalidation_0-mlogloss:1.52290\tvalidation_1-mlogloss:1.81567\n",
      "[149]\tvalidation_0-mlogloss:1.52103\tvalidation_1-mlogloss:1.81527\n",
      "[150]\tvalidation_0-mlogloss:1.51904\tvalidation_1-mlogloss:1.81486\n",
      "[151]\tvalidation_0-mlogloss:1.51712\tvalidation_1-mlogloss:1.81447\n",
      "[152]\tvalidation_0-mlogloss:1.51520\tvalidation_1-mlogloss:1.81406\n",
      "[153]\tvalidation_0-mlogloss:1.51332\tvalidation_1-mlogloss:1.81365\n",
      "[154]\tvalidation_0-mlogloss:1.51140\tvalidation_1-mlogloss:1.81323\n",
      "[155]\tvalidation_0-mlogloss:1.50959\tvalidation_1-mlogloss:1.81283\n",
      "[156]\tvalidation_0-mlogloss:1.50775\tvalidation_1-mlogloss:1.81245\n",
      "[157]\tvalidation_0-mlogloss:1.50585\tvalidation_1-mlogloss:1.81205\n",
      "[158]\tvalidation_0-mlogloss:1.50412\tvalidation_1-mlogloss:1.81172\n",
      "[159]\tvalidation_0-mlogloss:1.50223\tvalidation_1-mlogloss:1.81132\n",
      "[160]\tvalidation_0-mlogloss:1.50050\tvalidation_1-mlogloss:1.81099\n",
      "[161]\tvalidation_0-mlogloss:1.49871\tvalidation_1-mlogloss:1.81061\n",
      "[162]\tvalidation_0-mlogloss:1.49690\tvalidation_1-mlogloss:1.81023\n",
      "[163]\tvalidation_0-mlogloss:1.49517\tvalidation_1-mlogloss:1.80988\n",
      "[164]\tvalidation_0-mlogloss:1.49351\tvalidation_1-mlogloss:1.80954\n",
      "[165]\tvalidation_0-mlogloss:1.49170\tvalidation_1-mlogloss:1.80917\n",
      "[166]\tvalidation_0-mlogloss:1.48997\tvalidation_1-mlogloss:1.80887\n",
      "[167]\tvalidation_0-mlogloss:1.48825\tvalidation_1-mlogloss:1.80854\n",
      "[168]\tvalidation_0-mlogloss:1.48644\tvalidation_1-mlogloss:1.80818\n",
      "[169]\tvalidation_0-mlogloss:1.48474\tvalidation_1-mlogloss:1.80787\n",
      "[170]\tvalidation_0-mlogloss:1.48302\tvalidation_1-mlogloss:1.80758\n",
      "[171]\tvalidation_0-mlogloss:1.48126\tvalidation_1-mlogloss:1.80727\n",
      "[172]\tvalidation_0-mlogloss:1.47956\tvalidation_1-mlogloss:1.80696\n",
      "[173]\tvalidation_0-mlogloss:1.47778\tvalidation_1-mlogloss:1.80661\n",
      "[174]\tvalidation_0-mlogloss:1.47612\tvalidation_1-mlogloss:1.80633\n",
      "[175]\tvalidation_0-mlogloss:1.47439\tvalidation_1-mlogloss:1.80605\n",
      "[176]\tvalidation_0-mlogloss:1.47272\tvalidation_1-mlogloss:1.80575\n",
      "[177]\tvalidation_0-mlogloss:1.47109\tvalidation_1-mlogloss:1.80547\n",
      "[178]\tvalidation_0-mlogloss:1.46943\tvalidation_1-mlogloss:1.80518\n",
      "[179]\tvalidation_0-mlogloss:1.46761\tvalidation_1-mlogloss:1.80487\n",
      "[180]\tvalidation_0-mlogloss:1.46583\tvalidation_1-mlogloss:1.80457\n",
      "[181]\tvalidation_0-mlogloss:1.46417\tvalidation_1-mlogloss:1.80427\n",
      "[182]\tvalidation_0-mlogloss:1.46246\tvalidation_1-mlogloss:1.80399\n",
      "[183]\tvalidation_0-mlogloss:1.46083\tvalidation_1-mlogloss:1.80372\n",
      "[184]\tvalidation_0-mlogloss:1.45922\tvalidation_1-mlogloss:1.80345\n",
      "[185]\tvalidation_0-mlogloss:1.45750\tvalidation_1-mlogloss:1.80317\n",
      "[186]\tvalidation_0-mlogloss:1.45587\tvalidation_1-mlogloss:1.80290\n",
      "[187]\tvalidation_0-mlogloss:1.45429\tvalidation_1-mlogloss:1.80266\n",
      "[188]\tvalidation_0-mlogloss:1.45272\tvalidation_1-mlogloss:1.80240\n",
      "[189]\tvalidation_0-mlogloss:1.45106\tvalidation_1-mlogloss:1.80217\n",
      "[190]\tvalidation_0-mlogloss:1.44949\tvalidation_1-mlogloss:1.80192\n",
      "[191]\tvalidation_0-mlogloss:1.44795\tvalidation_1-mlogloss:1.80167\n",
      "[192]\tvalidation_0-mlogloss:1.44636\tvalidation_1-mlogloss:1.80142\n",
      "[193]\tvalidation_0-mlogloss:1.44476\tvalidation_1-mlogloss:1.80118\n",
      "[194]\tvalidation_0-mlogloss:1.44317\tvalidation_1-mlogloss:1.80095\n",
      "[195]\tvalidation_0-mlogloss:1.44158\tvalidation_1-mlogloss:1.80073\n",
      "[196]\tvalidation_0-mlogloss:1.43999\tvalidation_1-mlogloss:1.80048\n",
      "[197]\tvalidation_0-mlogloss:1.43834\tvalidation_1-mlogloss:1.80025\n",
      "[198]\tvalidation_0-mlogloss:1.43687\tvalidation_1-mlogloss:1.80003\n",
      "[199]\tvalidation_0-mlogloss:1.43534\tvalidation_1-mlogloss:1.79981\n",
      "[200]\tvalidation_0-mlogloss:1.43382\tvalidation_1-mlogloss:1.79961\n",
      "[201]\tvalidation_0-mlogloss:1.43221\tvalidation_1-mlogloss:1.79938\n",
      "[202]\tvalidation_0-mlogloss:1.43063\tvalidation_1-mlogloss:1.79913\n",
      "[203]\tvalidation_0-mlogloss:1.42910\tvalidation_1-mlogloss:1.79889\n",
      "[204]\tvalidation_0-mlogloss:1.42747\tvalidation_1-mlogloss:1.79863\n",
      "[205]\tvalidation_0-mlogloss:1.42604\tvalidation_1-mlogloss:1.79842\n",
      "[206]\tvalidation_0-mlogloss:1.42447\tvalidation_1-mlogloss:1.79821\n",
      "[207]\tvalidation_0-mlogloss:1.42306\tvalidation_1-mlogloss:1.79801\n",
      "[208]\tvalidation_0-mlogloss:1.42162\tvalidation_1-mlogloss:1.79779\n",
      "[209]\tvalidation_0-mlogloss:1.42021\tvalidation_1-mlogloss:1.79763\n",
      "[210]\tvalidation_0-mlogloss:1.41871\tvalidation_1-mlogloss:1.79741\n",
      "[211]\tvalidation_0-mlogloss:1.41736\tvalidation_1-mlogloss:1.79723\n",
      "[212]\tvalidation_0-mlogloss:1.41584\tvalidation_1-mlogloss:1.79698\n",
      "[213]\tvalidation_0-mlogloss:1.41439\tvalidation_1-mlogloss:1.79679\n",
      "[214]\tvalidation_0-mlogloss:1.41300\tvalidation_1-mlogloss:1.79660\n",
      "[215]\tvalidation_0-mlogloss:1.41155\tvalidation_1-mlogloss:1.79641\n",
      "[216]\tvalidation_0-mlogloss:1.41017\tvalidation_1-mlogloss:1.79622\n",
      "[217]\tvalidation_0-mlogloss:1.40869\tvalidation_1-mlogloss:1.79601\n",
      "[218]\tvalidation_0-mlogloss:1.40718\tvalidation_1-mlogloss:1.79580\n",
      "[219]\tvalidation_0-mlogloss:1.40567\tvalidation_1-mlogloss:1.79557\n",
      "[220]\tvalidation_0-mlogloss:1.40419\tvalidation_1-mlogloss:1.79537\n",
      "[221]\tvalidation_0-mlogloss:1.40260\tvalidation_1-mlogloss:1.79516\n",
      "[222]\tvalidation_0-mlogloss:1.40116\tvalidation_1-mlogloss:1.79495\n",
      "[223]\tvalidation_0-mlogloss:1.39967\tvalidation_1-mlogloss:1.79475\n",
      "[224]\tvalidation_0-mlogloss:1.39820\tvalidation_1-mlogloss:1.79454\n",
      "[225]\tvalidation_0-mlogloss:1.39682\tvalidation_1-mlogloss:1.79435\n",
      "[226]\tvalidation_0-mlogloss:1.39533\tvalidation_1-mlogloss:1.79416\n",
      "[227]\tvalidation_0-mlogloss:1.39383\tvalidation_1-mlogloss:1.79396\n",
      "[228]\tvalidation_0-mlogloss:1.39234\tvalidation_1-mlogloss:1.79381\n",
      "[229]\tvalidation_0-mlogloss:1.39099\tvalidation_1-mlogloss:1.79365\n",
      "[230]\tvalidation_0-mlogloss:1.38959\tvalidation_1-mlogloss:1.79349\n",
      "[231]\tvalidation_0-mlogloss:1.38819\tvalidation_1-mlogloss:1.79333\n",
      "[232]\tvalidation_0-mlogloss:1.38674\tvalidation_1-mlogloss:1.79318\n",
      "[233]\tvalidation_0-mlogloss:1.38534\tvalidation_1-mlogloss:1.79301\n",
      "[234]\tvalidation_0-mlogloss:1.38397\tvalidation_1-mlogloss:1.79286\n",
      "[235]\tvalidation_0-mlogloss:1.38248\tvalidation_1-mlogloss:1.79269\n",
      "[236]\tvalidation_0-mlogloss:1.38107\tvalidation_1-mlogloss:1.79257\n",
      "[237]\tvalidation_0-mlogloss:1.37971\tvalidation_1-mlogloss:1.79243\n",
      "[238]\tvalidation_0-mlogloss:1.37831\tvalidation_1-mlogloss:1.79223\n",
      "[239]\tvalidation_0-mlogloss:1.37686\tvalidation_1-mlogloss:1.79207\n",
      "[240]\tvalidation_0-mlogloss:1.37555\tvalidation_1-mlogloss:1.79194\n",
      "[241]\tvalidation_0-mlogloss:1.37413\tvalidation_1-mlogloss:1.79178\n",
      "[242]\tvalidation_0-mlogloss:1.37272\tvalidation_1-mlogloss:1.79163\n",
      "[243]\tvalidation_0-mlogloss:1.37138\tvalidation_1-mlogloss:1.79149\n",
      "[244]\tvalidation_0-mlogloss:1.36998\tvalidation_1-mlogloss:1.79131\n",
      "[245]\tvalidation_0-mlogloss:1.36868\tvalidation_1-mlogloss:1.79120\n",
      "[246]\tvalidation_0-mlogloss:1.36733\tvalidation_1-mlogloss:1.79105\n",
      "[247]\tvalidation_0-mlogloss:1.36601\tvalidation_1-mlogloss:1.79090\n",
      "[248]\tvalidation_0-mlogloss:1.36459\tvalidation_1-mlogloss:1.79072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[249]\tvalidation_0-mlogloss:1.36316\tvalidation_1-mlogloss:1.79053\n",
      "[250]\tvalidation_0-mlogloss:1.36187\tvalidation_1-mlogloss:1.79039\n",
      "[251]\tvalidation_0-mlogloss:1.36045\tvalidation_1-mlogloss:1.79024\n",
      "[252]\tvalidation_0-mlogloss:1.35910\tvalidation_1-mlogloss:1.79011\n",
      "[253]\tvalidation_0-mlogloss:1.35776\tvalidation_1-mlogloss:1.78998\n",
      "[254]\tvalidation_0-mlogloss:1.35641\tvalidation_1-mlogloss:1.78985\n",
      "[255]\tvalidation_0-mlogloss:1.35508\tvalidation_1-mlogloss:1.78972\n",
      "[256]\tvalidation_0-mlogloss:1.35376\tvalidation_1-mlogloss:1.78962\n",
      "[257]\tvalidation_0-mlogloss:1.35242\tvalidation_1-mlogloss:1.78948\n",
      "[258]\tvalidation_0-mlogloss:1.35118\tvalidation_1-mlogloss:1.78933\n",
      "[259]\tvalidation_0-mlogloss:1.34981\tvalidation_1-mlogloss:1.78921\n",
      "[260]\tvalidation_0-mlogloss:1.34843\tvalidation_1-mlogloss:1.78908\n",
      "[261]\tvalidation_0-mlogloss:1.34717\tvalidation_1-mlogloss:1.78893\n",
      "[262]\tvalidation_0-mlogloss:1.34577\tvalidation_1-mlogloss:1.78881\n",
      "[263]\tvalidation_0-mlogloss:1.34450\tvalidation_1-mlogloss:1.78872\n",
      "[264]\tvalidation_0-mlogloss:1.34318\tvalidation_1-mlogloss:1.78858\n",
      "[265]\tvalidation_0-mlogloss:1.34180\tvalidation_1-mlogloss:1.78848\n",
      "[266]\tvalidation_0-mlogloss:1.34042\tvalidation_1-mlogloss:1.78835\n",
      "[267]\tvalidation_0-mlogloss:1.33910\tvalidation_1-mlogloss:1.78821\n",
      "[268]\tvalidation_0-mlogloss:1.33769\tvalidation_1-mlogloss:1.78811\n",
      "[269]\tvalidation_0-mlogloss:1.33641\tvalidation_1-mlogloss:1.78796\n",
      "[270]\tvalidation_0-mlogloss:1.33506\tvalidation_1-mlogloss:1.78786\n",
      "[271]\tvalidation_0-mlogloss:1.33381\tvalidation_1-mlogloss:1.78774\n",
      "[272]\tvalidation_0-mlogloss:1.33250\tvalidation_1-mlogloss:1.78761\n",
      "[273]\tvalidation_0-mlogloss:1.33120\tvalidation_1-mlogloss:1.78749\n",
      "[274]\tvalidation_0-mlogloss:1.32992\tvalidation_1-mlogloss:1.78738\n",
      "[275]\tvalidation_0-mlogloss:1.32867\tvalidation_1-mlogloss:1.78727\n",
      "[276]\tvalidation_0-mlogloss:1.32747\tvalidation_1-mlogloss:1.78715\n",
      "[277]\tvalidation_0-mlogloss:1.32620\tvalidation_1-mlogloss:1.78706\n",
      "[278]\tvalidation_0-mlogloss:1.32495\tvalidation_1-mlogloss:1.78697\n",
      "[279]\tvalidation_0-mlogloss:1.32372\tvalidation_1-mlogloss:1.78686\n",
      "[280]\tvalidation_0-mlogloss:1.32231\tvalidation_1-mlogloss:1.78669\n",
      "[281]\tvalidation_0-mlogloss:1.32103\tvalidation_1-mlogloss:1.78660\n",
      "[282]\tvalidation_0-mlogloss:1.31979\tvalidation_1-mlogloss:1.78645\n",
      "[283]\tvalidation_0-mlogloss:1.31859\tvalidation_1-mlogloss:1.78637\n",
      "[284]\tvalidation_0-mlogloss:1.31742\tvalidation_1-mlogloss:1.78628\n",
      "[285]\tvalidation_0-mlogloss:1.31612\tvalidation_1-mlogloss:1.78619\n",
      "[286]\tvalidation_0-mlogloss:1.31489\tvalidation_1-mlogloss:1.78609\n",
      "[287]\tvalidation_0-mlogloss:1.31366\tvalidation_1-mlogloss:1.78599\n",
      "[288]\tvalidation_0-mlogloss:1.31237\tvalidation_1-mlogloss:1.78591\n",
      "[289]\tvalidation_0-mlogloss:1.31124\tvalidation_1-mlogloss:1.78581\n",
      "[290]\tvalidation_0-mlogloss:1.31004\tvalidation_1-mlogloss:1.78571\n",
      "[291]\tvalidation_0-mlogloss:1.30886\tvalidation_1-mlogloss:1.78558\n",
      "[292]\tvalidation_0-mlogloss:1.30770\tvalidation_1-mlogloss:1.78548\n",
      "[293]\tvalidation_0-mlogloss:1.30646\tvalidation_1-mlogloss:1.78538\n",
      "[294]\tvalidation_0-mlogloss:1.30517\tvalidation_1-mlogloss:1.78526\n",
      "[295]\tvalidation_0-mlogloss:1.30406\tvalidation_1-mlogloss:1.78522\n",
      "[296]\tvalidation_0-mlogloss:1.30279\tvalidation_1-mlogloss:1.78513\n",
      "[297]\tvalidation_0-mlogloss:1.30157\tvalidation_1-mlogloss:1.78501\n",
      "[298]\tvalidation_0-mlogloss:1.30030\tvalidation_1-mlogloss:1.78494\n",
      "[299]\tvalidation_0-mlogloss:1.29903\tvalidation_1-mlogloss:1.78488\n",
      "[300]\tvalidation_0-mlogloss:1.29788\tvalidation_1-mlogloss:1.78480\n",
      "[301]\tvalidation_0-mlogloss:1.29660\tvalidation_1-mlogloss:1.78470\n",
      "[302]\tvalidation_0-mlogloss:1.29539\tvalidation_1-mlogloss:1.78461\n",
      "[303]\tvalidation_0-mlogloss:1.29434\tvalidation_1-mlogloss:1.78455\n",
      "[304]\tvalidation_0-mlogloss:1.29323\tvalidation_1-mlogloss:1.78446\n",
      "[305]\tvalidation_0-mlogloss:1.29209\tvalidation_1-mlogloss:1.78438\n",
      "[306]\tvalidation_0-mlogloss:1.29085\tvalidation_1-mlogloss:1.78432\n",
      "[307]\tvalidation_0-mlogloss:1.28968\tvalidation_1-mlogloss:1.78425\n",
      "[308]\tvalidation_0-mlogloss:1.28857\tvalidation_1-mlogloss:1.78419\n",
      "[309]\tvalidation_0-mlogloss:1.28738\tvalidation_1-mlogloss:1.78409\n",
      "[310]\tvalidation_0-mlogloss:1.28623\tvalidation_1-mlogloss:1.78405\n",
      "[311]\tvalidation_0-mlogloss:1.28499\tvalidation_1-mlogloss:1.78397\n",
      "[312]\tvalidation_0-mlogloss:1.28375\tvalidation_1-mlogloss:1.78390\n",
      "[313]\tvalidation_0-mlogloss:1.28257\tvalidation_1-mlogloss:1.78381\n",
      "[314]\tvalidation_0-mlogloss:1.28140\tvalidation_1-mlogloss:1.78374\n",
      "[315]\tvalidation_0-mlogloss:1.28020\tvalidation_1-mlogloss:1.78367\n",
      "[316]\tvalidation_0-mlogloss:1.27914\tvalidation_1-mlogloss:1.78358\n",
      "[317]\tvalidation_0-mlogloss:1.27810\tvalidation_1-mlogloss:1.78356\n",
      "[318]\tvalidation_0-mlogloss:1.27689\tvalidation_1-mlogloss:1.78345\n",
      "[319]\tvalidation_0-mlogloss:1.27569\tvalidation_1-mlogloss:1.78338\n",
      "[320]\tvalidation_0-mlogloss:1.27454\tvalidation_1-mlogloss:1.78332\n",
      "[321]\tvalidation_0-mlogloss:1.27346\tvalidation_1-mlogloss:1.78327\n",
      "[322]\tvalidation_0-mlogloss:1.27230\tvalidation_1-mlogloss:1.78323\n",
      "[323]\tvalidation_0-mlogloss:1.27117\tvalidation_1-mlogloss:1.78316\n",
      "[324]\tvalidation_0-mlogloss:1.27004\tvalidation_1-mlogloss:1.78306\n",
      "[325]\tvalidation_0-mlogloss:1.26884\tvalidation_1-mlogloss:1.78300\n",
      "[326]\tvalidation_0-mlogloss:1.26774\tvalidation_1-mlogloss:1.78293\n",
      "[327]\tvalidation_0-mlogloss:1.26661\tvalidation_1-mlogloss:1.78287\n",
      "[328]\tvalidation_0-mlogloss:1.26543\tvalidation_1-mlogloss:1.78278\n",
      "[329]\tvalidation_0-mlogloss:1.26431\tvalidation_1-mlogloss:1.78273\n",
      "[330]\tvalidation_0-mlogloss:1.26308\tvalidation_1-mlogloss:1.78265\n",
      "[331]\tvalidation_0-mlogloss:1.26198\tvalidation_1-mlogloss:1.78259\n",
      "[332]\tvalidation_0-mlogloss:1.26083\tvalidation_1-mlogloss:1.78250\n",
      "[333]\tvalidation_0-mlogloss:1.25975\tvalidation_1-mlogloss:1.78245\n",
      "[334]\tvalidation_0-mlogloss:1.25863\tvalidation_1-mlogloss:1.78239\n",
      "[335]\tvalidation_0-mlogloss:1.25747\tvalidation_1-mlogloss:1.78234\n",
      "[336]\tvalidation_0-mlogloss:1.25641\tvalidation_1-mlogloss:1.78229\n",
      "[337]\tvalidation_0-mlogloss:1.25530\tvalidation_1-mlogloss:1.78225\n"
     ]
    }
   ],
   "source": [
    "# eval \n",
    "# lgb_model_params = {\n",
    "#                'objective': 'multiclass',  # multiclass, binary \n",
    "#                'boosting': 'gbdt',\n",
    "#                'learning_rate': 0.15,\n",
    "#                'metric': ['multi_logloss'],  # 'binary_logloss', 'multi_logloss'\n",
    "#                'num_threads': 15,\n",
    "#                'random_state': 2019,\n",
    "#                'num_boost_round': 1000,\n",
    "#                'device': 'cpu',\n",
    "#                'num_class':20,  # 2, 20 ,10\n",
    "#                'num_leaves':32,  # [16,32,64,128]\n",
    "#                'subsample': 0.9,  # [0.7,0.8,0.9,1]\n",
    "#                'colsample_bytree': 0.9, # [0.2,0.3,0.4,0.5,0.6]\n",
    "#                'min_data_in_leaf': 40, # [20,40,60,80,100]\n",
    "#                'lambda_l1': 1.0,  # (0.2,3)\n",
    "#                'lambda_l2': 1.0,  # (0.2,3)\n",
    "# }\n",
    "\n",
    "xgb_model_params = {\n",
    "               'objective': 'multi:softmax',  # multiclass, binary \n",
    "               'booster': 'gbtree',\n",
    "               'eta': 0.15,\n",
    "               'eval_metric': ['mlogloss'],  # 'binary_logloss', 'multi_logloss'\n",
    "               'nthread': 15,\n",
    "               'random_state': 2019,\n",
    "               'tree_method':'auto',\n",
    "               'n_estimators': 1000,\n",
    "               'device': 'cpu',\n",
    "               'num_class':20,  # 2, 20 ,10\n",
    "               'max_leaves':32,  # [16,32,64,128]\n",
    "               'subsample': 0.9,  # [0.7,0.8,0.9,1]\n",
    "               'colsample_bytree': 0.9, # [0.2,0.3,0.4,0.5,0.6]\n",
    "               'min_data_in_leaf': 40, # [20,40,60,80,100]\n",
    "               'lambda': 1.0,  # (0.2,3)\n",
    "               'alpha': 1.0,  # (0.2,3)\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'fe_filename':'train_fe_df.feather', \n",
    "    'is_eval':True, \n",
    "    'model_type': 'ensemble',\n",
    "    'model_name': 'xgb',\n",
    "    'model_params': xgb_model_params,\n",
    "    'use_log': False,\n",
    "    'use_std': True,\n",
    "    'use_cv': True,  \n",
    "    'n_splits':2,  \n",
    "}\n",
    "train_wrapper(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-15 15:18:14,504 - mlpipeline.train - INFO - using_fe_df: train_fe_df.feather, use_label: y, is_eval: False, model_type: ensemble, model_name: lgb, use_log: False, use_std: True, use_cv: False, n_splits: 2\n",
      "2020-05-15 15:18:14,680 - mlpipeline.train - INFO - _train_pipeline_ensemble_and_linear开始\n",
      "2020-05-15 15:18:14,684 - mlpipeline.train - INFO - 连续性特征数量: 100\n",
      "2020-05-15 15:18:14,684 - mlpipeline.train - INFO - 离散性特征数量: 0\n",
      "2020-05-15 15:18:15,241 - mlpipeline.train - INFO - train参数:{'objective': 'multiclass', 'boosting': 'gbdt', 'learning_rate': 0.15, 'metric': ['multi_logloss'], 'num_threads': 20, 'random_state': 2019, 'num_boost_round': 620, 'device': 'cpu', 'num_class': 20, 'num_leaves': 32, 'subsample': 0.9, 'colsample_bytree': 0.9, 'min_data_in_leaf': 40, 'lambda_l1': 1.0, 'lambda_l2': 1.0}\n",
      "2020-05-15 15:18:15,242 - utils.utils - INFO - standard_scale开始\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "2020-05-15 15:18:42,114 - utils.utils - INFO - standard_scale已完成，共用时0:00:27\n",
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's multi_logloss: 2.57176\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's multi_logloss: 2.5083\n",
      "[3]\ttraining's multi_logloss: 2.4552\n",
      "[4]\ttraining's multi_logloss: 2.40947\n",
      "[5]\ttraining's multi_logloss: 2.36945\n",
      "[6]\ttraining's multi_logloss: 2.33421\n",
      "[7]\ttraining's multi_logloss: 2.3024\n",
      "[8]\ttraining's multi_logloss: 2.27397\n",
      "[9]\ttraining's multi_logloss: 2.248\n",
      "[10]\ttraining's multi_logloss: 2.22435\n",
      "[11]\ttraining's multi_logloss: 2.20279\n",
      "[12]\ttraining's multi_logloss: 2.18303\n",
      "[13]\ttraining's multi_logloss: 2.16459\n",
      "[14]\ttraining's multi_logloss: 2.14777\n",
      "[15]\ttraining's multi_logloss: 2.13202\n",
      "[16]\ttraining's multi_logloss: 2.11719\n",
      "[17]\ttraining's multi_logloss: 2.10366\n",
      "[18]\ttraining's multi_logloss: 2.09074\n",
      "[19]\ttraining's multi_logloss: 2.07859\n",
      "[20]\ttraining's multi_logloss: 2.06732\n",
      "[21]\ttraining's multi_logloss: 2.05667\n",
      "[22]\ttraining's multi_logloss: 2.04649\n",
      "[23]\ttraining's multi_logloss: 2.03683\n",
      "[24]\ttraining's multi_logloss: 2.02773\n",
      "[25]\ttraining's multi_logloss: 2.01902\n",
      "[26]\ttraining's multi_logloss: 2.0108\n",
      "[27]\ttraining's multi_logloss: 2.00304\n",
      "[28]\ttraining's multi_logloss: 1.99553\n",
      "[29]\ttraining's multi_logloss: 1.98845\n",
      "[30]\ttraining's multi_logloss: 1.98164\n",
      "[31]\ttraining's multi_logloss: 1.97511\n",
      "[32]\ttraining's multi_logloss: 1.96879\n",
      "[33]\ttraining's multi_logloss: 1.96275\n",
      "[34]\ttraining's multi_logloss: 1.95691\n",
      "[35]\ttraining's multi_logloss: 1.95135\n",
      "[36]\ttraining's multi_logloss: 1.94598\n",
      "[37]\ttraining's multi_logloss: 1.94075\n",
      "[38]\ttraining's multi_logloss: 1.93577\n",
      "[39]\ttraining's multi_logloss: 1.93091\n",
      "[40]\ttraining's multi_logloss: 1.92623\n",
      "[41]\ttraining's multi_logloss: 1.92171\n",
      "[42]\ttraining's multi_logloss: 1.91734\n",
      "[43]\ttraining's multi_logloss: 1.91305\n",
      "[44]\ttraining's multi_logloss: 1.90886\n",
      "[45]\ttraining's multi_logloss: 1.90484\n",
      "[46]\ttraining's multi_logloss: 1.90093\n",
      "[47]\ttraining's multi_logloss: 1.89716\n",
      "[48]\ttraining's multi_logloss: 1.89346\n",
      "[49]\ttraining's multi_logloss: 1.88985\n",
      "[50]\ttraining's multi_logloss: 1.88638\n",
      "[51]\ttraining's multi_logloss: 1.88297\n",
      "[52]\ttraining's multi_logloss: 1.87966\n",
      "[53]\ttraining's multi_logloss: 1.87641\n",
      "[54]\ttraining's multi_logloss: 1.87329\n",
      "[55]\ttraining's multi_logloss: 1.87023\n",
      "[56]\ttraining's multi_logloss: 1.86714\n",
      "[57]\ttraining's multi_logloss: 1.86421\n",
      "[58]\ttraining's multi_logloss: 1.86133\n",
      "[59]\ttraining's multi_logloss: 1.8585\n",
      "[60]\ttraining's multi_logloss: 1.85566\n",
      "[61]\ttraining's multi_logloss: 1.85291\n",
      "[62]\ttraining's multi_logloss: 1.85029\n",
      "[63]\ttraining's multi_logloss: 1.84766\n",
      "[64]\ttraining's multi_logloss: 1.84503\n",
      "[65]\ttraining's multi_logloss: 1.8425\n",
      "[66]\ttraining's multi_logloss: 1.84004\n",
      "[67]\ttraining's multi_logloss: 1.83759\n",
      "[68]\ttraining's multi_logloss: 1.83518\n",
      "[69]\ttraining's multi_logloss: 1.83283\n",
      "[70]\ttraining's multi_logloss: 1.83053\n",
      "[71]\ttraining's multi_logloss: 1.82824\n",
      "[72]\ttraining's multi_logloss: 1.82599\n",
      "[73]\ttraining's multi_logloss: 1.82376\n",
      "[74]\ttraining's multi_logloss: 1.82158\n",
      "[75]\ttraining's multi_logloss: 1.81942\n",
      "[76]\ttraining's multi_logloss: 1.81734\n",
      "[77]\ttraining's multi_logloss: 1.81528\n",
      "[78]\ttraining's multi_logloss: 1.81323\n",
      "[79]\ttraining's multi_logloss: 1.81125\n",
      "[80]\ttraining's multi_logloss: 1.80925\n",
      "[81]\ttraining's multi_logloss: 1.80726\n",
      "[82]\ttraining's multi_logloss: 1.8053\n",
      "[83]\ttraining's multi_logloss: 1.80336\n",
      "[84]\ttraining's multi_logloss: 1.80147\n",
      "[85]\ttraining's multi_logloss: 1.7996\n",
      "[86]\ttraining's multi_logloss: 1.79774\n",
      "[87]\ttraining's multi_logloss: 1.79589\n",
      "[88]\ttraining's multi_logloss: 1.79413\n",
      "[89]\ttraining's multi_logloss: 1.79236\n",
      "[90]\ttraining's multi_logloss: 1.79059\n",
      "[91]\ttraining's multi_logloss: 1.78883\n",
      "[92]\ttraining's multi_logloss: 1.78708\n",
      "[93]\ttraining's multi_logloss: 1.78538\n",
      "[94]\ttraining's multi_logloss: 1.78366\n",
      "[95]\ttraining's multi_logloss: 1.78199\n",
      "[96]\ttraining's multi_logloss: 1.78033\n",
      "[97]\ttraining's multi_logloss: 1.77871\n",
      "[98]\ttraining's multi_logloss: 1.7771\n",
      "[99]\ttraining's multi_logloss: 1.77548\n",
      "[100]\ttraining's multi_logloss: 1.77391\n",
      "[101]\ttraining's multi_logloss: 1.77234\n",
      "[102]\ttraining's multi_logloss: 1.77081\n",
      "[103]\ttraining's multi_logloss: 1.76928\n",
      "[104]\ttraining's multi_logloss: 1.76775\n",
      "[105]\ttraining's multi_logloss: 1.76627\n",
      "[106]\ttraining's multi_logloss: 1.76482\n",
      "[107]\ttraining's multi_logloss: 1.76332\n",
      "[108]\ttraining's multi_logloss: 1.76187\n",
      "[109]\ttraining's multi_logloss: 1.76043\n",
      "[110]\ttraining's multi_logloss: 1.75896\n",
      "[111]\ttraining's multi_logloss: 1.75754\n",
      "[112]\ttraining's multi_logloss: 1.75612\n",
      "[113]\ttraining's multi_logloss: 1.75468\n",
      "[114]\ttraining's multi_logloss: 1.75328\n",
      "[115]\ttraining's multi_logloss: 1.75188\n",
      "[116]\ttraining's multi_logloss: 1.75047\n",
      "[117]\ttraining's multi_logloss: 1.74914\n",
      "[118]\ttraining's multi_logloss: 1.7478\n",
      "[119]\ttraining's multi_logloss: 1.74646\n",
      "[120]\ttraining's multi_logloss: 1.74512\n",
      "[121]\ttraining's multi_logloss: 1.74381\n",
      "[122]\ttraining's multi_logloss: 1.7425\n",
      "[123]\ttraining's multi_logloss: 1.74117\n",
      "[124]\ttraining's multi_logloss: 1.73987\n",
      "[125]\ttraining's multi_logloss: 1.73858\n",
      "[126]\ttraining's multi_logloss: 1.73731\n",
      "[127]\ttraining's multi_logloss: 1.73605\n",
      "[128]\ttraining's multi_logloss: 1.73477\n",
      "[129]\ttraining's multi_logloss: 1.73352\n",
      "[130]\ttraining's multi_logloss: 1.73228\n",
      "[131]\ttraining's multi_logloss: 1.73102\n",
      "[132]\ttraining's multi_logloss: 1.72979\n",
      "[133]\ttraining's multi_logloss: 1.72858\n",
      "[134]\ttraining's multi_logloss: 1.72736\n",
      "[135]\ttraining's multi_logloss: 1.72616\n",
      "[136]\ttraining's multi_logloss: 1.72496\n",
      "[137]\ttraining's multi_logloss: 1.72377\n",
      "[138]\ttraining's multi_logloss: 1.72258\n",
      "[139]\ttraining's multi_logloss: 1.72142\n",
      "[140]\ttraining's multi_logloss: 1.72024\n",
      "[141]\ttraining's multi_logloss: 1.71909\n",
      "[142]\ttraining's multi_logloss: 1.71794\n",
      "[143]\ttraining's multi_logloss: 1.71679\n",
      "[144]\ttraining's multi_logloss: 1.71564\n",
      "[145]\ttraining's multi_logloss: 1.71451\n",
      "[146]\ttraining's multi_logloss: 1.71339\n",
      "[147]\ttraining's multi_logloss: 1.71228\n",
      "[148]\ttraining's multi_logloss: 1.71117\n",
      "[149]\ttraining's multi_logloss: 1.71004\n",
      "[150]\ttraining's multi_logloss: 1.70893\n",
      "[151]\ttraining's multi_logloss: 1.70783\n",
      "[152]\ttraining's multi_logloss: 1.70673\n",
      "[153]\ttraining's multi_logloss: 1.70563\n",
      "[154]\ttraining's multi_logloss: 1.70455\n",
      "[155]\ttraining's multi_logloss: 1.70349\n",
      "[156]\ttraining's multi_logloss: 1.70241\n",
      "[157]\ttraining's multi_logloss: 1.70135\n",
      "[158]\ttraining's multi_logloss: 1.70029\n",
      "[159]\ttraining's multi_logloss: 1.69924\n",
      "[160]\ttraining's multi_logloss: 1.6982\n",
      "[161]\ttraining's multi_logloss: 1.69715\n",
      "[162]\ttraining's multi_logloss: 1.6961\n",
      "[163]\ttraining's multi_logloss: 1.69507\n",
      "[164]\ttraining's multi_logloss: 1.69405\n",
      "[165]\ttraining's multi_logloss: 1.69302\n",
      "[166]\ttraining's multi_logloss: 1.692\n",
      "[167]\ttraining's multi_logloss: 1.69098\n",
      "[168]\ttraining's multi_logloss: 1.68998\n",
      "[169]\ttraining's multi_logloss: 1.68897\n",
      "[170]\ttraining's multi_logloss: 1.68797\n",
      "[171]\ttraining's multi_logloss: 1.68699\n",
      "[172]\ttraining's multi_logloss: 1.68599\n",
      "[173]\ttraining's multi_logloss: 1.68499\n",
      "[174]\ttraining's multi_logloss: 1.68398\n",
      "[175]\ttraining's multi_logloss: 1.683\n",
      "[176]\ttraining's multi_logloss: 1.68201\n",
      "[177]\ttraining's multi_logloss: 1.68104\n",
      "[178]\ttraining's multi_logloss: 1.68008\n",
      "[179]\ttraining's multi_logloss: 1.6791\n",
      "[180]\ttraining's multi_logloss: 1.67815\n",
      "[181]\ttraining's multi_logloss: 1.6772\n",
      "[182]\ttraining's multi_logloss: 1.67623\n",
      "[183]\ttraining's multi_logloss: 1.67528\n",
      "[184]\ttraining's multi_logloss: 1.67435\n",
      "[185]\ttraining's multi_logloss: 1.67342\n",
      "[186]\ttraining's multi_logloss: 1.67247\n",
      "[187]\ttraining's multi_logloss: 1.67152\n",
      "[188]\ttraining's multi_logloss: 1.67057\n",
      "[189]\ttraining's multi_logloss: 1.66964\n",
      "[190]\ttraining's multi_logloss: 1.66872\n",
      "[191]\ttraining's multi_logloss: 1.6678\n",
      "[192]\ttraining's multi_logloss: 1.66689\n",
      "[193]\ttraining's multi_logloss: 1.66598\n",
      "[194]\ttraining's multi_logloss: 1.66507\n",
      "[195]\ttraining's multi_logloss: 1.66415\n",
      "[196]\ttraining's multi_logloss: 1.66322\n",
      "[197]\ttraining's multi_logloss: 1.66231\n",
      "[198]\ttraining's multi_logloss: 1.66141\n",
      "[199]\ttraining's multi_logloss: 1.6605\n",
      "[200]\ttraining's multi_logloss: 1.65962\n",
      "[201]\ttraining's multi_logloss: 1.65874\n",
      "[202]\ttraining's multi_logloss: 1.65784\n",
      "[203]\ttraining's multi_logloss: 1.65696\n",
      "[204]\ttraining's multi_logloss: 1.65607\n",
      "[205]\ttraining's multi_logloss: 1.65519\n",
      "[206]\ttraining's multi_logloss: 1.65432\n",
      "[207]\ttraining's multi_logloss: 1.65346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208]\ttraining's multi_logloss: 1.65259\n",
      "[209]\ttraining's multi_logloss: 1.65173\n",
      "[210]\ttraining's multi_logloss: 1.65085\n",
      "[211]\ttraining's multi_logloss: 1.64999\n",
      "[212]\ttraining's multi_logloss: 1.64913\n",
      "[213]\ttraining's multi_logloss: 1.64828\n",
      "[214]\ttraining's multi_logloss: 1.64743\n",
      "[215]\ttraining's multi_logloss: 1.64658\n",
      "[216]\ttraining's multi_logloss: 1.64575\n",
      "[217]\ttraining's multi_logloss: 1.64491\n",
      "[218]\ttraining's multi_logloss: 1.6441\n",
      "[219]\ttraining's multi_logloss: 1.64326\n",
      "[220]\ttraining's multi_logloss: 1.64243\n",
      "[221]\ttraining's multi_logloss: 1.64161\n",
      "[222]\ttraining's multi_logloss: 1.64076\n",
      "[223]\ttraining's multi_logloss: 1.63994\n",
      "[224]\ttraining's multi_logloss: 1.63913\n",
      "[225]\ttraining's multi_logloss: 1.63831\n",
      "[226]\ttraining's multi_logloss: 1.63749\n",
      "[227]\ttraining's multi_logloss: 1.63667\n",
      "[228]\ttraining's multi_logloss: 1.63586\n",
      "[229]\ttraining's multi_logloss: 1.63504\n",
      "[230]\ttraining's multi_logloss: 1.63425\n",
      "[231]\ttraining's multi_logloss: 1.63346\n",
      "[232]\ttraining's multi_logloss: 1.63264\n",
      "[233]\ttraining's multi_logloss: 1.63184\n",
      "[234]\ttraining's multi_logloss: 1.63104\n",
      "[235]\ttraining's multi_logloss: 1.63024\n",
      "[236]\ttraining's multi_logloss: 1.62946\n",
      "[237]\ttraining's multi_logloss: 1.62867\n",
      "[238]\ttraining's multi_logloss: 1.62788\n",
      "[239]\ttraining's multi_logloss: 1.62711\n",
      "[240]\ttraining's multi_logloss: 1.62633\n",
      "[241]\ttraining's multi_logloss: 1.62557\n",
      "[242]\ttraining's multi_logloss: 1.62484\n",
      "[243]\ttraining's multi_logloss: 1.62403\n",
      "[244]\ttraining's multi_logloss: 1.62325\n",
      "[245]\ttraining's multi_logloss: 1.62249\n",
      "[246]\ttraining's multi_logloss: 1.62171\n",
      "[247]\ttraining's multi_logloss: 1.62095\n",
      "[248]\ttraining's multi_logloss: 1.62017\n",
      "[249]\ttraining's multi_logloss: 1.61939\n",
      "[250]\ttraining's multi_logloss: 1.61863\n",
      "[251]\ttraining's multi_logloss: 1.61784\n",
      "[252]\ttraining's multi_logloss: 1.61709\n",
      "[253]\ttraining's multi_logloss: 1.61633\n",
      "[254]\ttraining's multi_logloss: 1.61559\n",
      "[255]\ttraining's multi_logloss: 1.61483\n",
      "[256]\ttraining's multi_logloss: 1.61408\n",
      "[257]\ttraining's multi_logloss: 1.61334\n",
      "[258]\ttraining's multi_logloss: 1.61258\n",
      "[259]\ttraining's multi_logloss: 1.61183\n",
      "[260]\ttraining's multi_logloss: 1.61109\n",
      "[261]\ttraining's multi_logloss: 1.61034\n",
      "[262]\ttraining's multi_logloss: 1.6096\n",
      "[263]\ttraining's multi_logloss: 1.60887\n",
      "[264]\ttraining's multi_logloss: 1.60815\n",
      "[265]\ttraining's multi_logloss: 1.60743\n",
      "[266]\ttraining's multi_logloss: 1.6067\n",
      "[267]\ttraining's multi_logloss: 1.60598\n",
      "[268]\ttraining's multi_logloss: 1.60527\n",
      "[269]\ttraining's multi_logloss: 1.60456\n",
      "[270]\ttraining's multi_logloss: 1.60381\n",
      "[271]\ttraining's multi_logloss: 1.6031\n",
      "[272]\ttraining's multi_logloss: 1.60238\n",
      "[273]\ttraining's multi_logloss: 1.60169\n",
      "[274]\ttraining's multi_logloss: 1.60097\n",
      "[275]\ttraining's multi_logloss: 1.60027\n",
      "[276]\ttraining's multi_logloss: 1.59959\n",
      "[277]\ttraining's multi_logloss: 1.59887\n",
      "[278]\ttraining's multi_logloss: 1.59816\n",
      "[279]\ttraining's multi_logloss: 1.59746\n",
      "[280]\ttraining's multi_logloss: 1.59673\n",
      "[281]\ttraining's multi_logloss: 1.59602\n",
      "[282]\ttraining's multi_logloss: 1.59532\n",
      "[283]\ttraining's multi_logloss: 1.59461\n",
      "[284]\ttraining's multi_logloss: 1.5939\n",
      "[285]\ttraining's multi_logloss: 1.59322\n",
      "[286]\ttraining's multi_logloss: 1.59251\n",
      "[287]\ttraining's multi_logloss: 1.59182\n",
      "[288]\ttraining's multi_logloss: 1.59112\n",
      "[289]\ttraining's multi_logloss: 1.59042\n",
      "[290]\ttraining's multi_logloss: 1.58974\n",
      "[291]\ttraining's multi_logloss: 1.58904\n",
      "[292]\ttraining's multi_logloss: 1.58833\n",
      "[293]\ttraining's multi_logloss: 1.58765\n",
      "[294]\ttraining's multi_logloss: 1.58696\n",
      "[295]\ttraining's multi_logloss: 1.5863\n",
      "[296]\ttraining's multi_logloss: 1.58564\n",
      "[297]\ttraining's multi_logloss: 1.58494\n",
      "[298]\ttraining's multi_logloss: 1.58425\n",
      "[299]\ttraining's multi_logloss: 1.58357\n",
      "[300]\ttraining's multi_logloss: 1.58289\n",
      "[301]\ttraining's multi_logloss: 1.58219\n",
      "[302]\ttraining's multi_logloss: 1.58153\n",
      "[303]\ttraining's multi_logloss: 1.58087\n",
      "[304]\ttraining's multi_logloss: 1.58021\n",
      "[305]\ttraining's multi_logloss: 1.57955\n",
      "[306]\ttraining's multi_logloss: 1.57887\n",
      "[307]\ttraining's multi_logloss: 1.57821\n",
      "[308]\ttraining's multi_logloss: 1.57756\n",
      "[309]\ttraining's multi_logloss: 1.57689\n",
      "[310]\ttraining's multi_logloss: 1.57625\n",
      "[311]\ttraining's multi_logloss: 1.57557\n",
      "[312]\ttraining's multi_logloss: 1.57492\n",
      "[313]\ttraining's multi_logloss: 1.57425\n",
      "[314]\ttraining's multi_logloss: 1.5736\n",
      "[315]\ttraining's multi_logloss: 1.57291\n",
      "[316]\ttraining's multi_logloss: 1.57229\n",
      "[317]\ttraining's multi_logloss: 1.57164\n",
      "[318]\ttraining's multi_logloss: 1.57096\n",
      "[319]\ttraining's multi_logloss: 1.57033\n",
      "[320]\ttraining's multi_logloss: 1.56967\n",
      "[321]\ttraining's multi_logloss: 1.56904\n",
      "[322]\ttraining's multi_logloss: 1.56839\n",
      "[323]\ttraining's multi_logloss: 1.56777\n",
      "[324]\ttraining's multi_logloss: 1.56713\n",
      "[325]\ttraining's multi_logloss: 1.56648\n",
      "[326]\ttraining's multi_logloss: 1.56584\n",
      "[327]\ttraining's multi_logloss: 1.56522\n",
      "[328]\ttraining's multi_logloss: 1.56458\n",
      "[329]\ttraining's multi_logloss: 1.56395\n",
      "[330]\ttraining's multi_logloss: 1.56329\n",
      "[331]\ttraining's multi_logloss: 1.56266\n",
      "[332]\ttraining's multi_logloss: 1.56202\n",
      "[333]\ttraining's multi_logloss: 1.56136\n",
      "[334]\ttraining's multi_logloss: 1.56072\n",
      "[335]\ttraining's multi_logloss: 1.56007\n",
      "[336]\ttraining's multi_logloss: 1.55945\n",
      "[337]\ttraining's multi_logloss: 1.55883\n",
      "[338]\ttraining's multi_logloss: 1.55821\n",
      "[339]\ttraining's multi_logloss: 1.55758\n",
      "[340]\ttraining's multi_logloss: 1.55695\n",
      "[341]\ttraining's multi_logloss: 1.55633\n",
      "[342]\ttraining's multi_logloss: 1.55573\n",
      "[343]\ttraining's multi_logloss: 1.5551\n",
      "[344]\ttraining's multi_logloss: 1.55447\n",
      "[345]\ttraining's multi_logloss: 1.55385\n",
      "[346]\ttraining's multi_logloss: 1.55323\n",
      "[347]\ttraining's multi_logloss: 1.55262\n",
      "[348]\ttraining's multi_logloss: 1.55202\n",
      "[349]\ttraining's multi_logloss: 1.55142\n",
      "[350]\ttraining's multi_logloss: 1.55081\n",
      "[351]\ttraining's multi_logloss: 1.55019\n",
      "[352]\ttraining's multi_logloss: 1.54958\n",
      "[353]\ttraining's multi_logloss: 1.54897\n",
      "[354]\ttraining's multi_logloss: 1.54838\n",
      "[355]\ttraining's multi_logloss: 1.54776\n",
      "[356]\ttraining's multi_logloss: 1.54715\n",
      "[357]\ttraining's multi_logloss: 1.54652\n",
      "[358]\ttraining's multi_logloss: 1.54591\n",
      "[359]\ttraining's multi_logloss: 1.54531\n",
      "[360]\ttraining's multi_logloss: 1.54471\n",
      "[361]\ttraining's multi_logloss: 1.54413\n",
      "[362]\ttraining's multi_logloss: 1.54353\n",
      "[363]\ttraining's multi_logloss: 1.54295\n",
      "[364]\ttraining's multi_logloss: 1.54235\n",
      "[365]\ttraining's multi_logloss: 1.54173\n",
      "[366]\ttraining's multi_logloss: 1.54112\n",
      "[367]\ttraining's multi_logloss: 1.54054\n",
      "[368]\ttraining's multi_logloss: 1.53993\n",
      "[369]\ttraining's multi_logloss: 1.53933\n",
      "[370]\ttraining's multi_logloss: 1.53875\n",
      "[371]\ttraining's multi_logloss: 1.53817\n",
      "[372]\ttraining's multi_logloss: 1.53759\n",
      "[373]\ttraining's multi_logloss: 1.53699\n",
      "[374]\ttraining's multi_logloss: 1.5364\n",
      "[375]\ttraining's multi_logloss: 1.53582\n",
      "[376]\ttraining's multi_logloss: 1.53524\n",
      "[377]\ttraining's multi_logloss: 1.53466\n",
      "[378]\ttraining's multi_logloss: 1.53411\n",
      "[379]\ttraining's multi_logloss: 1.53351\n",
      "[380]\ttraining's multi_logloss: 1.53293\n",
      "[381]\ttraining's multi_logloss: 1.53234\n",
      "[382]\ttraining's multi_logloss: 1.53176\n",
      "[383]\ttraining's multi_logloss: 1.53118\n",
      "[384]\ttraining's multi_logloss: 1.5306\n",
      "[385]\ttraining's multi_logloss: 1.53004\n",
      "[386]\ttraining's multi_logloss: 1.52947\n",
      "[387]\ttraining's multi_logloss: 1.52891\n",
      "[388]\ttraining's multi_logloss: 1.52834\n",
      "[389]\ttraining's multi_logloss: 1.52776\n",
      "[390]\ttraining's multi_logloss: 1.52719\n",
      "[391]\ttraining's multi_logloss: 1.5266\n",
      "[392]\ttraining's multi_logloss: 1.52604\n",
      "[393]\ttraining's multi_logloss: 1.52546\n",
      "[394]\ttraining's multi_logloss: 1.52488\n",
      "[395]\ttraining's multi_logloss: 1.52431\n",
      "[396]\ttraining's multi_logloss: 1.52372\n",
      "[397]\ttraining's multi_logloss: 1.52315\n",
      "[398]\ttraining's multi_logloss: 1.52259\n",
      "[399]\ttraining's multi_logloss: 1.52203\n",
      "[400]\ttraining's multi_logloss: 1.52146\n",
      "[401]\ttraining's multi_logloss: 1.52091\n",
      "[402]\ttraining's multi_logloss: 1.52034\n",
      "[403]\ttraining's multi_logloss: 1.51976\n",
      "[404]\ttraining's multi_logloss: 1.5192\n",
      "[405]\ttraining's multi_logloss: 1.51861\n",
      "[406]\ttraining's multi_logloss: 1.51803\n",
      "[407]\ttraining's multi_logloss: 1.51747\n",
      "[408]\ttraining's multi_logloss: 1.51688\n",
      "[409]\ttraining's multi_logloss: 1.51632\n",
      "[410]\ttraining's multi_logloss: 1.51576\n",
      "[411]\ttraining's multi_logloss: 1.51522\n",
      "[412]\ttraining's multi_logloss: 1.51467\n",
      "[413]\ttraining's multi_logloss: 1.51413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[414]\ttraining's multi_logloss: 1.51358\n",
      "[415]\ttraining's multi_logloss: 1.51304\n",
      "[416]\ttraining's multi_logloss: 1.51248\n",
      "[417]\ttraining's multi_logloss: 1.51193\n",
      "[418]\ttraining's multi_logloss: 1.51139\n",
      "[419]\ttraining's multi_logloss: 1.51082\n",
      "[420]\ttraining's multi_logloss: 1.51027\n",
      "[421]\ttraining's multi_logloss: 1.5097\n",
      "[422]\ttraining's multi_logloss: 1.50914\n",
      "[423]\ttraining's multi_logloss: 1.50861\n",
      "[424]\ttraining's multi_logloss: 1.50811\n",
      "[425]\ttraining's multi_logloss: 1.50756\n",
      "[426]\ttraining's multi_logloss: 1.50697\n",
      "[427]\ttraining's multi_logloss: 1.50639\n",
      "[428]\ttraining's multi_logloss: 1.50584\n",
      "[429]\ttraining's multi_logloss: 1.50529\n",
      "[430]\ttraining's multi_logloss: 1.50474\n",
      "[431]\ttraining's multi_logloss: 1.50422\n",
      "[432]\ttraining's multi_logloss: 1.50368\n",
      "[433]\ttraining's multi_logloss: 1.50316\n",
      "[434]\ttraining's multi_logloss: 1.50261\n",
      "[435]\ttraining's multi_logloss: 1.50204\n",
      "[436]\ttraining's multi_logloss: 1.50147\n",
      "[437]\ttraining's multi_logloss: 1.50095\n",
      "[438]\ttraining's multi_logloss: 1.50043\n",
      "[439]\ttraining's multi_logloss: 1.49989\n",
      "[440]\ttraining's multi_logloss: 1.49934\n",
      "[441]\ttraining's multi_logloss: 1.49881\n",
      "[442]\ttraining's multi_logloss: 1.49825\n",
      "[443]\ttraining's multi_logloss: 1.4977\n",
      "[444]\ttraining's multi_logloss: 1.49716\n",
      "[445]\ttraining's multi_logloss: 1.49662\n",
      "[446]\ttraining's multi_logloss: 1.49608\n",
      "[447]\ttraining's multi_logloss: 1.49554\n",
      "[448]\ttraining's multi_logloss: 1.49502\n",
      "[449]\ttraining's multi_logloss: 1.4945\n",
      "[450]\ttraining's multi_logloss: 1.49395\n",
      "[451]\ttraining's multi_logloss: 1.49339\n",
      "[452]\ttraining's multi_logloss: 1.49287\n",
      "[453]\ttraining's multi_logloss: 1.49234\n",
      "[454]\ttraining's multi_logloss: 1.49179\n",
      "[455]\ttraining's multi_logloss: 1.49127\n",
      "[456]\ttraining's multi_logloss: 1.49071\n",
      "[457]\ttraining's multi_logloss: 1.49017\n",
      "[458]\ttraining's multi_logloss: 1.48964\n",
      "[459]\ttraining's multi_logloss: 1.4891\n",
      "[460]\ttraining's multi_logloss: 1.48857\n",
      "[461]\ttraining's multi_logloss: 1.48805\n",
      "[462]\ttraining's multi_logloss: 1.48756\n",
      "[463]\ttraining's multi_logloss: 1.48702\n",
      "[464]\ttraining's multi_logloss: 1.48648\n",
      "[465]\ttraining's multi_logloss: 1.48596\n",
      "[466]\ttraining's multi_logloss: 1.48543\n",
      "[467]\ttraining's multi_logloss: 1.48491\n",
      "[468]\ttraining's multi_logloss: 1.48439\n",
      "[469]\ttraining's multi_logloss: 1.48386\n",
      "[470]\ttraining's multi_logloss: 1.48335\n",
      "[471]\ttraining's multi_logloss: 1.48279\n",
      "[472]\ttraining's multi_logloss: 1.48226\n",
      "[473]\ttraining's multi_logloss: 1.48175\n",
      "[474]\ttraining's multi_logloss: 1.48125\n",
      "[475]\ttraining's multi_logloss: 1.48072\n",
      "[476]\ttraining's multi_logloss: 1.48021\n",
      "[477]\ttraining's multi_logloss: 1.47969\n",
      "[478]\ttraining's multi_logloss: 1.47918\n",
      "[479]\ttraining's multi_logloss: 1.47867\n",
      "[480]\ttraining's multi_logloss: 1.47814\n",
      "[481]\ttraining's multi_logloss: 1.4776\n",
      "[482]\ttraining's multi_logloss: 1.4771\n",
      "[483]\ttraining's multi_logloss: 1.47658\n",
      "[484]\ttraining's multi_logloss: 1.47607\n",
      "[485]\ttraining's multi_logloss: 1.47555\n",
      "[486]\ttraining's multi_logloss: 1.47505\n",
      "[487]\ttraining's multi_logloss: 1.47452\n",
      "[488]\ttraining's multi_logloss: 1.47399\n",
      "[489]\ttraining's multi_logloss: 1.47348\n",
      "[490]\ttraining's multi_logloss: 1.47298\n",
      "[491]\ttraining's multi_logloss: 1.47246\n",
      "[492]\ttraining's multi_logloss: 1.47195\n",
      "[493]\ttraining's multi_logloss: 1.47144\n",
      "[494]\ttraining's multi_logloss: 1.47096\n",
      "[495]\ttraining's multi_logloss: 1.47046\n",
      "[496]\ttraining's multi_logloss: 1.46994\n",
      "[497]\ttraining's multi_logloss: 1.46941\n",
      "[498]\ttraining's multi_logloss: 1.46891\n",
      "[499]\ttraining's multi_logloss: 1.46842\n",
      "[500]\ttraining's multi_logloss: 1.46792\n",
      "[501]\ttraining's multi_logloss: 1.46742\n",
      "[502]\ttraining's multi_logloss: 1.46693\n",
      "[503]\ttraining's multi_logloss: 1.46646\n",
      "[504]\ttraining's multi_logloss: 1.46598\n",
      "[505]\ttraining's multi_logloss: 1.46548\n",
      "[506]\ttraining's multi_logloss: 1.46499\n",
      "[507]\ttraining's multi_logloss: 1.46448\n",
      "[508]\ttraining's multi_logloss: 1.46397\n",
      "[509]\ttraining's multi_logloss: 1.46347\n",
      "[510]\ttraining's multi_logloss: 1.46297\n",
      "[511]\ttraining's multi_logloss: 1.46248\n",
      "[512]\ttraining's multi_logloss: 1.46196\n",
      "[513]\ttraining's multi_logloss: 1.46143\n",
      "[514]\ttraining's multi_logloss: 1.46095\n",
      "[515]\ttraining's multi_logloss: 1.46046\n",
      "[516]\ttraining's multi_logloss: 1.45997\n",
      "[517]\ttraining's multi_logloss: 1.45947\n",
      "[518]\ttraining's multi_logloss: 1.45897\n",
      "[519]\ttraining's multi_logloss: 1.45846\n",
      "[520]\ttraining's multi_logloss: 1.45798\n",
      "[521]\ttraining's multi_logloss: 1.45748\n",
      "[522]\ttraining's multi_logloss: 1.45699\n",
      "[523]\ttraining's multi_logloss: 1.45647\n",
      "[524]\ttraining's multi_logloss: 1.45599\n",
      "[525]\ttraining's multi_logloss: 1.45549\n",
      "[526]\ttraining's multi_logloss: 1.45502\n",
      "[527]\ttraining's multi_logloss: 1.45453\n",
      "[528]\ttraining's multi_logloss: 1.45404\n",
      "[529]\ttraining's multi_logloss: 1.45355\n",
      "[530]\ttraining's multi_logloss: 1.45307\n",
      "[531]\ttraining's multi_logloss: 1.45258\n",
      "[532]\ttraining's multi_logloss: 1.45208\n",
      "[533]\ttraining's multi_logloss: 1.4516\n",
      "[534]\ttraining's multi_logloss: 1.45112\n",
      "[535]\ttraining's multi_logloss: 1.45061\n",
      "[536]\ttraining's multi_logloss: 1.45013\n",
      "[537]\ttraining's multi_logloss: 1.44962\n",
      "[538]\ttraining's multi_logloss: 1.44915\n",
      "[539]\ttraining's multi_logloss: 1.44865\n",
      "[540]\ttraining's multi_logloss: 1.44816\n",
      "[541]\ttraining's multi_logloss: 1.44766\n",
      "[542]\ttraining's multi_logloss: 1.44718\n",
      "[543]\ttraining's multi_logloss: 1.44667\n",
      "[544]\ttraining's multi_logloss: 1.44618\n",
      "[545]\ttraining's multi_logloss: 1.44569\n",
      "[546]\ttraining's multi_logloss: 1.44516\n",
      "[547]\ttraining's multi_logloss: 1.44468\n",
      "[548]\ttraining's multi_logloss: 1.44419\n",
      "[549]\ttraining's multi_logloss: 1.4437\n",
      "[550]\ttraining's multi_logloss: 1.44321\n",
      "[551]\ttraining's multi_logloss: 1.44272\n",
      "[552]\ttraining's multi_logloss: 1.44221\n",
      "[553]\ttraining's multi_logloss: 1.44171\n",
      "[554]\ttraining's multi_logloss: 1.44123\n",
      "[555]\ttraining's multi_logloss: 1.44076\n",
      "[556]\ttraining's multi_logloss: 1.44029\n",
      "[557]\ttraining's multi_logloss: 1.4398\n",
      "[558]\ttraining's multi_logloss: 1.43933\n",
      "[559]\ttraining's multi_logloss: 1.43887\n",
      "[560]\ttraining's multi_logloss: 1.43841\n",
      "[561]\ttraining's multi_logloss: 1.43794\n",
      "[562]\ttraining's multi_logloss: 1.43747\n",
      "[563]\ttraining's multi_logloss: 1.43697\n",
      "[564]\ttraining's multi_logloss: 1.43647\n",
      "[565]\ttraining's multi_logloss: 1.436\n",
      "[566]\ttraining's multi_logloss: 1.43552\n",
      "[567]\ttraining's multi_logloss: 1.43505\n",
      "[568]\ttraining's multi_logloss: 1.4346\n",
      "[569]\ttraining's multi_logloss: 1.43414\n",
      "[570]\ttraining's multi_logloss: 1.43367\n",
      "[571]\ttraining's multi_logloss: 1.43319\n",
      "[572]\ttraining's multi_logloss: 1.43272\n",
      "[573]\ttraining's multi_logloss: 1.43226\n",
      "[574]\ttraining's multi_logloss: 1.43178\n",
      "[575]\ttraining's multi_logloss: 1.43132\n",
      "[576]\ttraining's multi_logloss: 1.43084\n",
      "[577]\ttraining's multi_logloss: 1.43035\n",
      "[578]\ttraining's multi_logloss: 1.42989\n",
      "[579]\ttraining's multi_logloss: 1.4294\n",
      "[580]\ttraining's multi_logloss: 1.42893\n",
      "[581]\ttraining's multi_logloss: 1.42849\n",
      "[582]\ttraining's multi_logloss: 1.42802\n",
      "[583]\ttraining's multi_logloss: 1.42754\n",
      "[584]\ttraining's multi_logloss: 1.42708\n",
      "[585]\ttraining's multi_logloss: 1.4266\n",
      "[586]\ttraining's multi_logloss: 1.42614\n",
      "[587]\ttraining's multi_logloss: 1.42569\n",
      "[588]\ttraining's multi_logloss: 1.42523\n",
      "[589]\ttraining's multi_logloss: 1.42478\n",
      "[590]\ttraining's multi_logloss: 1.42431\n",
      "[591]\ttraining's multi_logloss: 1.42383\n",
      "[592]\ttraining's multi_logloss: 1.42336\n",
      "[593]\ttraining's multi_logloss: 1.42289\n",
      "[594]\ttraining's multi_logloss: 1.42242\n",
      "[595]\ttraining's multi_logloss: 1.42196\n",
      "[596]\ttraining's multi_logloss: 1.42146\n",
      "[597]\ttraining's multi_logloss: 1.42098\n",
      "[598]\ttraining's multi_logloss: 1.42051\n",
      "[599]\ttraining's multi_logloss: 1.42005\n",
      "[600]\ttraining's multi_logloss: 1.41958\n",
      "[601]\ttraining's multi_logloss: 1.41911\n",
      "[602]\ttraining's multi_logloss: 1.41863\n",
      "[603]\ttraining's multi_logloss: 1.41817\n",
      "[604]\ttraining's multi_logloss: 1.4177\n",
      "[605]\ttraining's multi_logloss: 1.41723\n",
      "[606]\ttraining's multi_logloss: 1.41676\n",
      "[607]\ttraining's multi_logloss: 1.4163\n",
      "[608]\ttraining's multi_logloss: 1.41583\n",
      "[609]\ttraining's multi_logloss: 1.41533\n",
      "[610]\ttraining's multi_logloss: 1.41489\n",
      "[611]\ttraining's multi_logloss: 1.41443\n",
      "[612]\ttraining's multi_logloss: 1.41398\n",
      "[613]\ttraining's multi_logloss: 1.41353\n",
      "[614]\ttraining's multi_logloss: 1.41309\n",
      "[615]\ttraining's multi_logloss: 1.41263\n",
      "[616]\ttraining's multi_logloss: 1.41218\n",
      "[617]\ttraining's multi_logloss: 1.41172\n",
      "[618]\ttraining's multi_logloss: 1.41127\n",
      "[619]\ttraining's multi_logloss: 1.41084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[620]\ttraining's multi_logloss: 1.41037\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[620]\ttraining's multi_logloss: 1.41037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-15 15:31:20,213 - mlpipeline.train - INFO - _train_pipeline_ensemble_and_linear已完成，共用时0:13:06\n",
      "2020-05-15 15:31:20,214 - mlpipeline.train - INFO - lgb模型训练完成!模型保存至:../trained_models/lgb.model.2020-05-15T15:18:14.680759\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "model_params = {\n",
    "               'objective': 'multiclass',  # multiclass, binary \n",
    "               'boosting': 'gbdt',\n",
    "               'learning_rate': 0.15,\n",
    "               'metric': ['multi_logloss'],  # 'binary_logloss', 'multi_logloss'\n",
    "               'num_threads': 20,\n",
    "               'random_state': 2019,\n",
    "               'num_boost_round': 620,\n",
    "               'device': 'cpu',\n",
    "               'num_class':20,  # 2, 20 ,10\n",
    "               'num_leaves':32,  # [16,32,64,128]\n",
    "               'subsample': 0.9,  # [0.7,0.8,0.9,1]\n",
    "               'colsample_bytree': 0.9, # [0.2,0.3,0.4,0.5,0.6]\n",
    "               'min_data_in_leaf': 40, # [20,40,60,80,100]\n",
    "               'lambda_l1': 1.0,  # (0.2,3)\n",
    "               'lambda_l2': 1.0,  # (0.2,3)\n",
    "}\n",
    "\n",
    "\n",
    "params = {\n",
    "    'fe_filename':'train_fe_df.feather', \n",
    "    'is_eval':False, \n",
    "    'model_type': 'ensemble',\n",
    "    'model_name': 'xgb',\n",
    "    'model_params': model_params,\n",
    "    'use_log': False,\n",
    "    'use_std': True,\n",
    "}\n",
    "model, scaler = train_wrapper(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-15 15:35:07,734 - mlpipeline.predict - INFO - predict开始\n",
      "2020-05-15 15:35:07,735 - mlpipeline.predict - INFO - test_fe_filename: test_fe_df.feather, use_log: False, use_std: True, model_type: ensemble, model_name: lgb\n",
      "2020-05-15 15:35:07,847 - mlpipeline.predict - INFO - inference_pipeline_ensemble_and_linear开始\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:376: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "2020-05-15 15:37:22,909 - mlpipeline.predict - INFO - _generate_submission_file开始\n",
      "../mlpipeline/predict.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submission_df['predicted_gender'] = submission_df['pred'].apply(lambda x: label_map_dict[x][0])\n",
      "../mlpipeline/predict.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submission_df['predicted_age'] = submission_df['pred'].apply(lambda x: label_map_dict[x][1])\n",
      "2020-05-15 15:37:29,698 - mlpipeline.predict - INFO - submission file has been stored in ../submission/submission_y_2020-05-15T15:37:24.103202.csv\n",
      "2020-05-15 15:37:29,701 - mlpipeline.predict - INFO - _generate_submission_file已完成，共用时0:00:07\n",
      "2020-05-15 15:37:29,971 - mlpipeline.predict - INFO - inference_pipeline_ensemble_and_linear已完成，共用时0:02:22\n",
      "2020-05-15 15:37:29,973 - mlpipeline.predict - INFO - predict已完成，共用时0:02:22\n"
     ]
    }
   ],
   "source": [
    "# predict \n",
    "params = {\n",
    "          'test_fe_filename':'test_fe_df.feather',\n",
    "          'use_log':False,\n",
    "          'use_std': True,\n",
    "          'model_type': 'ensemble',\n",
    "          'model_name':'lgb',\n",
    "          'scaler': scaler  # scaler\n",
    "            }\n",
    "\n",
    "submission_df = predict(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>predicted_gender</th>\n",
       "      <th>predicted_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3000001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3000002</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3000003</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3000004</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3000005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  predicted_gender  predicted_age\n",
       "0  3000001                 1              3\n",
       "1  3000002                 2              7\n",
       "2  3000003                 2              2\n",
       "3  3000004                 1              2\n",
       "4  3000005                 1              3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

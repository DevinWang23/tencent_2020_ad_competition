{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Prediction\n",
    "model training and inference\n",
    "- <a href='#1'>1. model</a> \n",
    "- <a href='#2'>2. train and predict</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import (\n",
    "    to_categorical,\n",
    "    Sequence\n",
    ")\n",
    "from tensorflow.keras import (\n",
    "    Input,\n",
    "    Model\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    Bidirectional,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Dense,\n",
    "    concatenate,\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    TimeDistributed,\n",
    "    Dropout,\n",
    "    Lambda,\n",
    "    Conv1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    GlobalAveragePooling1D,\n",
    "    TimeDistributed,\n",
    "    Dropout,\n",
    "    Lambda,\n",
    "    Conv1D,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    TFBertPreTrainedModel\n",
    ")\n",
    "from transformers.modeling_tf_utils import (\n",
    "    keras_serializable,\n",
    "    shape_list,\n",
    "    get_initializer\n",
    ")\n",
    "from transformers.modeling_tf_bert import (\n",
    "    TFBertEncoder\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sys.path.append('../')\n",
    "from utils import (\n",
    "    LogManager\n",
    ")\n",
    "import conf\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global setting\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "LogManager.created_filename = os.path.join(conf.LOG_DIR, 'train.log')\n",
    "logger = LogManager.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "embed_size = 200\n",
    "max_len = 80  # 用户点击的中位数为22个广告。\n",
    "batch_size = 256\n",
    "epochs = 20\n",
    "age_class_num = 10\n",
    "gender_class_num = 2\n",
    "len_stats = 50\n",
    "agg_col = ['creative_id', 'ad_id', 'product_id', 'advertiser_id', 'industry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions \n",
    "def decay_schedule(epoch, lr):\n",
    "    # decay by 0.1 every 9 epochs; use `% 1` to decay after each epoch\n",
    "    if epoch == 9:\n",
    "        lr = lr * 0.1\n",
    "    return lr\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    @staticmethod\n",
    "    def on_epoch_end(epoch, logs=None):\n",
    "        logger.info('epoch: %s, val_loss: %s, val_age_out_accuracy: %s, val_gender_out_accuracy: %s, val_acc: %s' % (\n",
    "            epoch,\n",
    "            logs['val_loss'],\n",
    "            logs['val_age_out_accuracy'],\n",
    "            logs['val_gender_out_accuracy'],\n",
    "            float(logs['val_age_out_accuracy']) +\n",
    "            float(logs['val_gender_out_accuracy'])\n",
    "        )\n",
    "                    )\n",
    "        \n",
    "def search_weight(\n",
    "        valid_y,\n",
    "        raw_prob,\n",
    "        class_num,\n",
    "        step=0.001\n",
    "):\n",
    "    init_weight = [1.0] * class_num\n",
    "    weight = init_weight.copy()\n",
    "    f_best = accuracy_score(\n",
    "        y_true=valid_y,\n",
    "        y_pred=raw_prob.argmax(\n",
    "            axis=1))\n",
    "\n",
    "    flag_score = 0\n",
    "    round_num = 1\n",
    "    while flag_score != f_best:\n",
    "        logger.info(\"round: %s\" % round_num)\n",
    "        round_num += 1\n",
    "        flag_score = f_best\n",
    "        for c in range(class_num):\n",
    "            for n_w in range(0, 2000, 10):\n",
    "                num = n_w * step\n",
    "                new_weight = weight.copy()\n",
    "                new_weight[c] = num\n",
    "                prob_df = raw_prob.copy()\n",
    "                prob_df = prob_df * np.array(new_weight)\n",
    "                f = accuracy_score(y_true=valid_y, y_pred=prob_df.argmax(\n",
    "                    axis=1))\n",
    "                if f > f_best:\n",
    "                    weight = new_weight.copy()\n",
    "                    f_best = f\n",
    "    logger.info('flag_score: %s' % flag_score)\n",
    "    return weight, flag_score\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\"Generates data for Keras\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_train, labels_train):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.dataset_train = dataset_train\n",
    "        self.labels_train = labels_train\n",
    "        self.batch_size = batch_size\n",
    "        self.click_times_length = self.dataset_train['click_times_length']\n",
    "        self.click_times_prob = self.dataset_train['click_times_prob']\n",
    "        self.stats_feat = self.dataset_train['stats_feat']\n",
    "        self.total_size = self.click_times_length.shape[0]\n",
    "        self.indexes = np.arange(self.total_size)\n",
    "\n",
    "        self.enhanced_data = {}\n",
    "        self.epoch_count = 0\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        logger.info('reinforment data begin')\n",
    "        del self.enhanced_data\n",
    "        gc.collect()\n",
    "        self.enhanced_data = {}\n",
    "\n",
    "        keys = [key for key in self.dataset_train.keys() if\n",
    "                key != 'click_times_length' and key != 'user_id' and key != 'click_times_prob' and key != 'stats_feat']\n",
    "        for key in keys:\n",
    "            self.enhanced_data[key] = []\n",
    "        for index, value in enumerate(self.click_times_length):\n",
    "            if value[0] > max_len:\n",
    "                value[0] = max_len\n",
    "            big = np.random.randint(int(value[0] * (2 / 3)), value[0])\n",
    "            sampled_action = np.random.choice(value[0], big, replace=False, p=self.click_times_prob[index])\n",
    "            for key in keys:\n",
    "                row = self.dataset_train[key][index]\n",
    "                if key == 'attention_mask':\n",
    "                    self.enhanced_data[key].append(\n",
    "                        np.hstack([row[sampled_action], np.array([-10000] * (max_len - len(sampled_action)))]))\n",
    "                else:\n",
    "                    self.enhanced_data[key].append(\n",
    "                        np.hstack([row[sampled_action], np.array([0] * (max_len - len(sampled_action)))]))\n",
    "        for key in keys:\n",
    "            self.enhanced_data[key] = np.stack(self.enhanced_data[key])\n",
    "\n",
    "        self.enhanced_data['click_times_length'] = self.click_times_length\n",
    "        self.enhanced_data['click_times_prob'] = self.click_times_prob\n",
    "        self.enhanced_data['stats_feat'] = self.stats_feat\n",
    "        np.random.shuffle(self.indexes)\n",
    "        logger.info('reinforment data end')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return int(np.floor(self.total_size / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        high = (index + 1) * self.batch_size\n",
    "        if high > self.total_size:\n",
    "            high = self.total_size\n",
    "        batch_data = {}\n",
    "        selected = self.indexes[index * self.batch_size:high]\n",
    "        for key in self.enhanced_data.keys():\n",
    "            batch_data[key] = self.enhanced_data[key][selected]\n",
    "        batch_labels = {\n",
    "            'age_out': self.labels_train['age_out'][selected],\n",
    "            'gender_out': self.labels_train['gender_out'][selected]\n",
    "        }\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "class OnEpochEnd(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, callbacks):\n",
    "        self.callbacks = callbacks\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='1'> 1.model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertMainLayer(tf.keras.layers.Layer):\n",
    "    config_class = BertConfig\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_hidden_layers = config.num_hidden_layers\n",
    "        self.initializer_range = config.initializer_range\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.encoder = TFBertEncoder(config, name=\"encoder\")\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\" Prunes heads of the model.\n",
    "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "            See base class PreTrainedModel\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(\n",
    "            self,\n",
    "            inputs,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            output_attentions=None,\n",
    "            training=True,\n",
    "    ):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
    "            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n",
    "            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n",
    "            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n",
    "            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n",
    "            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n",
    "            assert len(inputs) <= 7, \"Too many inputs.\"\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = tf.fill(input_shape, 1)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_shape, 0)\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "\n",
    "        extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.num_hidden_layers\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            [inputs_embeds, extended_attention_mask, head_mask, None, None], training=training\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        outputs = sequence_output  # add hidden_states and attentions if they are here\n",
    "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "\n",
    "class TransformerBasedModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, embedding_group, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.config = config\n",
    "        vocab_len_list = []\n",
    "        for index, key in enumerate(embedding_group.keys()):\n",
    "            vocab_len_list += [len(embedding_group[key])]\n",
    "\n",
    "        self.config.vocab_size = vocab_len_list[0]\n",
    "        self.trans_layer_1 = TFBertMainLayer(self.config, name='transformer1')\n",
    "        self.config.vocab_size = vocab_len_list[1]\n",
    "        self.trans_layer_2 = TFBertMainLayer(self.config, name='transformer2')\n",
    "        self.config.vocab_size = vocab_len_list[2]\n",
    "        self.trans_layer_3 = TFBertMainLayer(self.config, name='transformer3')\n",
    "        self.config.vocab_size = vocab_len_list[3]\n",
    "        self.trans_layer_4 = TFBertMainLayer(self.config, name='transformer4')\n",
    "        self.config.vocab_size = vocab_len_list[4]\n",
    "        self.trans_layer_5 = TFBertMainLayer(self.config, name='transformer5')\n",
    "        self.trans_layer_list = [\n",
    "            self.trans_layer_1,\n",
    "            self.trans_layer_2,\n",
    "            self.trans_layer_3,\n",
    "            self.trans_layer_4,\n",
    "            self.trans_layer_5,\n",
    "        ]\n",
    "\n",
    "        self.bilstm_layer_1 = Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "        self.bilstm_layer_2 = Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "        self.bilstm_layer_3 = Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "        self.bilstm_layer_4 = Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "        self.bilstm_layer_5 = Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "        self.bilstm_layer_list = [\n",
    "            self.bilstm_layer_1,\n",
    "            self.bilstm_layer_2,\n",
    "            self.bilstm_layer_3,\n",
    "            self.bilstm_layer_4,\n",
    "            self.bilstm_layer_5,\n",
    "        ]\n",
    "        self.bilstm_layer_7 = Bidirectional(LSTM(640, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "        self.bilstm_layer_8 = Bidirectional(LSTM(320, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "\n",
    "        self.bilstm_max_pool_1 = GlobalMaxPooling1D()\n",
    "        self.bilstm_max_pool_2 = GlobalMaxPooling1D()\n",
    "        self.bilstm_max_pool_3 = GlobalMaxPooling1D()\n",
    "        self.bilstm_max_pool_4 = GlobalMaxPooling1D()\n",
    "        self.bilstm_max_pool_5 = GlobalMaxPooling1D()\n",
    "        self.bilstm_max_pool_6 = GlobalMaxPooling1D()\n",
    "\n",
    "        self.bilstm_max_pool_list = [\n",
    "            self.bilstm_max_pool_1,\n",
    "            self.bilstm_max_pool_2,\n",
    "            self.bilstm_max_pool_3,\n",
    "            self.bilstm_max_pool_4,\n",
    "            self.bilstm_max_pool_5,\n",
    "            self.bilstm_max_pool_6,\n",
    "        ]\n",
    "\n",
    "        self.conv_max_pool_1 = GlobalMaxPooling1D()\n",
    "        self.conv_max_pool_2 = GlobalMaxPooling1D()\n",
    "        self.conv_max_pool_3 = GlobalMaxPooling1D()\n",
    "        self.conv_max_pool_list = [\n",
    "            self.conv_max_pool_1,\n",
    "            self.conv_max_pool_2,\n",
    "            self.conv_max_pool_3\n",
    "        ]\n",
    "\n",
    "        self.conv_1d_1 = Conv1D(128, kernel_size=2, padding=\"valid\", kernel_initializer=\"he_uniform\")\n",
    "        self.conv_1d_2 = Conv1D(128, kernel_size=3, padding=\"valid\", kernel_initializer=\"he_uniform\")\n",
    "        self.conv_1d_3 = Conv1D(128, kernel_size=4, padding=\"valid\", kernel_initializer=\"he_uniform\")\n",
    "        self.conv_1d_list = [\n",
    "            self.conv_1d_1,\n",
    "            self.conv_1d_2,\n",
    "            self.conv_1d_3\n",
    "        ]\n",
    "\n",
    "        self.dropout_1 = Dropout(0.4)\n",
    "        self.dropout_2 = Dropout(0.4)\n",
    "\n",
    "        self.dense_1 = Dense(512, activation='relu')\n",
    "        self.dense_2 = Dense(128, activation='relu')\n",
    "\n",
    "        self.stats_dense1 = Dense(256, activation='relu')\n",
    "        self.stats_dropout_1 = Dropout(0.2)\n",
    "        self.stats_dropout_2 = Dropout(0.2)\n",
    "        self.stats_dense2 = Dense(128, activation='relu')\n",
    "\n",
    "    def call(self, embeds, attention_mask, stats_feat, **kwargs):\n",
    "        temp_out = []\n",
    "        origin_mask = attention_mask[:, :, tf.newaxis]\n",
    "        clip_mask = tf.clip_by_value(origin_mask, clip_value_min=0, clip_value_max=1)\n",
    "        max_mask = (1.0 - clip_mask) * -10000.0\n",
    "        embed_all = concatenate(embeds)\n",
    "        out = self.bilstm_layer_7(embed_all)\n",
    "        out = out * clip_mask\n",
    "        out = self.bilstm_layer_8(out)\n",
    "        for_max = out + max_mask\n",
    "\n",
    "        for i, conv1_d in enumerate(self.conv_1d_list):\n",
    "            con_x = conv1_d(out)\n",
    "            max_pool = self.conv_max_pool_list[i](con_x)\n",
    "            temp_out.append(max_pool)\n",
    "        max_pool = self.bilstm_max_pool_list[-1](for_max)\n",
    "        temp_out.append(max_pool)\n",
    "\n",
    "        for index, embed in enumerate(embeds):\n",
    "            trans_inputs = [None, attention_mask, None, None, None, embed]\n",
    "            trans_outputs = self.trans_layer_list[i](trans_inputs, training=True)\n",
    "            bilstm_outputs = self.bilstm_layer_list[i](trans_outputs)\n",
    "            bilstm_max_outputs = self.bilstm_max_pool_list[i](bilstm_outputs)\n",
    "            temp_out.append(bilstm_max_outputs)\n",
    "\n",
    "        stats_out = self.stats_dropout_1(stats_feat)\n",
    "        stats_out = self.stats_dense1(stats_out)\n",
    "        stats_out = self.stats_dropout_2(stats_out)\n",
    "        stats_out = self.stats_dense2(stats_out)\n",
    "        temp_out.append(stats_out)\n",
    "\n",
    "        x = concatenate(temp_out)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "\n",
    "def model_(\n",
    "        config,\n",
    "        embedding_group\n",
    "):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    logger.info('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    with strategy.scope():\n",
    "        embeds = []\n",
    "        all_inputs = []\n",
    "        for col in agg_col:\n",
    "            input_x = Input(shape=(max_len,), name=col)\n",
    "            embed_x = Embedding(\n",
    "                input_dim=embedding_group[col].shape[0],\n",
    "                output_dim=embedding_group[col].shape[1],\n",
    "                weights=[embedding_group[col]],\n",
    "                input_length=max_len,\n",
    "                trainable=False,\n",
    "                name=col + '_embeding',\n",
    "                mask_zero=False,\n",
    "            )(input_x)\n",
    "            embeds.append(embed_x)\n",
    "            all_inputs.append(input_x)\n",
    "\n",
    "        attention_mask = Input(shape=(max_len,), name='attention_mask')\n",
    "        click_times_prob = Input(shape=(max_len,), name='click_times_prob')\n",
    "        click_length_input = Input(shape=(1,), name='click_times_length')\n",
    "        stats_feat = Input(shape=(len_stats,), name='stats_feat')\n",
    "        all_inputs.append(attention_mask)\n",
    "        all_inputs.append(click_length_input)\n",
    "        all_inputs.append(stats_feat)\n",
    "        all_inputs.append(click_times_prob)\n",
    "        gc.collect()\n",
    "\n",
    "        x = TransformerBasedModel(config, embedding_group, name='transformer')(embeds, attention_mask,\n",
    "                                                                               stats_feat=stats_feat)\n",
    "        age_out = Dense(10, activation='softmax', name='age_out')(x)\n",
    "        gender_out = Dense(2, activation='softmax', name='gender_out')(x)\n",
    "        model = Model(inputs=all_inputs, outputs=[age_out, gender_out])\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-08, clipnorm=1.0)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss={\n",
    "                'age_out': 'categorical_crossentropy',\n",
    "                'gender_out': 'categorical_crossentropy'\n",
    "            },\n",
    "            metrics={\n",
    "                'age_out': 'accuracy',\n",
    "                'gender_out': 'accuracy'\n",
    "            },\n",
    "            loss_weights={\n",
    "                'age_out': 0.5,\n",
    "                'gender_out': 0.5\n",
    "            }\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2'> 2.train and predict</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        dataset_train_x,\n",
    "        dataset_test_x,\n",
    "        df_test_final,\n",
    "        label_age,\n",
    "        label_gender,\n",
    "        folds,\n",
    "        config,\n",
    "        embedding_group,\n",
    "):\n",
    "    try:\n",
    "        age_score = []\n",
    "        age_score_weight = []\n",
    "        age_sub = np.zeros((df_test_final.shape[0], 10))\n",
    "        age_sub_weight = np.zeros((df_test_final.shape[0], 10))\n",
    "        gender_score = []\n",
    "        gender_score_weight = []\n",
    "        gender_sub = np.zeros((df_test_final.shape[0], 2))\n",
    "        gender_sub_weight = np.zeros((df_test_final.shape[0], 2))\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, random_state=100, shuffle=True)\n",
    "        count = 0\n",
    "        for i, (train_index, test_index) in enumerate(folds):\n",
    "            logger.info(\"FOLD | %s\" % (count + 1))\n",
    "            logger.info(\"###\" * 35)\n",
    "            gc.collect()\n",
    "            filepath = \"./model/nn_v1_%s.ckpt\" % count\n",
    "            checkpoint = ModelCheckpoint(\n",
    "                filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True)\n",
    "            #         reduce_lr = ReduceLROnPlateau(\n",
    "            #             monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1, mode='min')\n",
    "            earlystopping = EarlyStopping(\n",
    "                monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='min')\n",
    "            lr_scheduler = LearningRateScheduler(decay_schedule)\n",
    "            logcallback = CustomCallback()\n",
    "            logger.info('model params: %s' % config)\n",
    "            model = model_(\n",
    "                config,\n",
    "                embedding_group\n",
    "            )\n",
    "            if count == 0:\n",
    "                stringlist = []\n",
    "                model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "                short_model_summary = \"\\n\".join(stringlist)\n",
    "                logger.info(short_model_summary)\n",
    "\n",
    "            fold_data_train_x = {}\n",
    "            fold_data_val_x = {}\n",
    "            for col in agg_col + ['attention_mask'] + ['click_times_length'] + ['stats_feat']:\n",
    "                fold_data_train_x[col] = dataset_train_x[col][train_index]\n",
    "                fold_data_val_x[col] = dataset_train_x[col][test_index]\n",
    "            fold_data_train_y = {\n",
    "                'age_out': label_age[train_index],\n",
    "                'gender_out': label_gender[train_index]\n",
    "            }\n",
    "            fold_data_val_y = {\n",
    "                'age_out': label_age[test_index],\n",
    "                'gender_out': label_gender[test_index]\n",
    "            }\n",
    "            data_generator = DataGenerator(fold_data_train_x, fold_data_train_y)\n",
    "            callbacks = [checkpoint, earlystopping, lr_scheduler, logcallback,\n",
    "                         OnEpochEnd([data_generator.on_epoch_end])]\n",
    "            hist = model.fit(\n",
    "                data_generator,\n",
    "                epochs=epochs,\n",
    "                validation_data=(fold_data_val_x, fold_data_val_y),\n",
    "                callbacks=callbacks,\n",
    "                verbose=1,\n",
    "                shuffle=True\n",
    "            )\n",
    "            model.load_weights(filepath)\n",
    "            age_val_prob, gender_val_prob = model.predict(fold_data_val_x, batch_size=512, verbose=1)\n",
    "            age_weight, age_flag_score = search_weight(\n",
    "                'age',\n",
    "                fold_data_val_y['age_out'].argmax(\n",
    "                    axis=1),\n",
    "                age_val_prob,\n",
    "                age_class_num\n",
    "            )\n",
    "            gender_weight, gender_flag_score = search_weight(\n",
    "                'gender',\n",
    "                fold_data_val_y['gender_out'].argmax(\n",
    "                    axis=1),\n",
    "                gender_val_prob,\n",
    "                gender_class_num\n",
    "            )\n",
    "            # oof_pred[test_index] = model_age.predict(dataset_test_x,batch_size=512,verbose=1)\n",
    "            age_tmp_sub, gender_tmp_sub = model.predict(dataset_test_x, batch_size=512, verbose=1)\n",
    "            cur_time = datetime.now().isoformat()\n",
    "            np.save('./age_prob/age_result_fold_%s_acc_%s_%s' % (\n",
    "                count + 1, np.max(hist.history['val_age_out_accuracy']), cur_time), age_tmp_sub / skf.n_splits)\n",
    "            np.save('./age_prob/age_result_weight_fold_%s_acc_%s_%s' % (count + 1, age_flag_score, cur_time),\n",
    "                    age_tmp_sub / skf.n_splits * np.asarray(age_weight))\n",
    "            np.save('./gender_prob/gender_result_fold_%s_acc_%s_%s' % (\n",
    "                count + 1, np.max(hist.history['val_gender_out_accuracy']), cur_time), gender_tmp_sub / skf.n_splits)\n",
    "            np.save('./gender_prob/gender_result_weight_fold_%s_acc_%s_%s' % (count + 1, gender_flag_score, cur_time),\n",
    "                    gender_tmp_sub / skf.n_splits * np.asarray(gender_weight))\n",
    "            age_sub_weight += age_tmp_sub / skf.n_splits * np.asarray(age_weight)\n",
    "            gender_sub_weight += gender_tmp_sub / skf.n_splits * np.asarray(gender_weight)\n",
    "            age_sub += age_tmp_sub / skf.n_splits\n",
    "            gender_sub += gender_tmp_sub / skf.n_splits\n",
    "            logger.info(np.min(hist.history['val_loss']))\n",
    "            age_score.append(np.max(hist.history['val_age_out_accuracy']))\n",
    "            gender_score.append(np.max(hist.history['val_gender_out_accuracy']))\n",
    "            age_score_weight.append(age_flag_score)\n",
    "            gender_score_weight.append(gender_flag_score)\n",
    "            count += 1\n",
    "\n",
    "            del model, data_generator, hist, fold_data_train_x, fold_data_train_y, fold_data_val_x, fold_data_val_y\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "        logger.info(\"age score: %s\" % age_score)\n",
    "        logger.info(\"age acc: %s\" % np.mean(age_score))\n",
    "        logger.info(\"age score weight: %s\" % age_score_weight)\n",
    "        logger.info(\"age weight acc: %s\" % np.mean(age_score_weight))\n",
    "\n",
    "        logger.info(\"gender score: %s\" % gender_score)\n",
    "        logger.info(\"gender acc: %s\" % np.mean(gender_score))\n",
    "        logger.info(\"gender score weight: %s\" % gender_score_weight)\n",
    "        logger.info(\"gender weight acc: %s\" % np.mean(gender_score_weight))\n",
    "        return age_sub_weight, age_sub, gender_sub_weight, gender_sub\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(str(e))\n",
    "        logger.error(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger.info('train start')\n",
    "    word_indexs = pickle.load(open(\"./model/cache/word_index.pkl\", \"rb\"))\n",
    "    fe_df = pd.read_parquet('./model/cache/fe_df_150.parquet')\n",
    "    logger.info('data has been loaded')\n",
    "\n",
    "    # load w2v\n",
    "    wv_group = {}\n",
    "    for col in agg_col:\n",
    "        wv_group[col] = KeyedVectors.load(f\"./martin_model/model/word2vec_{col}_128.model\", mmap='r')\n",
    "    #       wv_group[col] = KeyedVectors.load(f\"./model/word2vec_{col}_200.model\", mmap='r')\n",
    "\n",
    "    # divide data into train and test\n",
    "    df_test_final = fe_df[fe_df.age.isna()]\n",
    "    df_train_val_final = fe_df[~fe_df.age.isna()]\n",
    "    assert df_train_val_final.shape[0] == 3000000\n",
    "\n",
    "    del fe_df\n",
    "    gc.collect()\n",
    "\n",
    "    # combine age and gender into one label\n",
    "    df_train_val_final['y'] = list(zip(df_train_val_final['gender'], df_train_val_final['age']))\n",
    "    label_map_dict = {\n",
    "        0: (1, 1),\n",
    "        1: (1, 2),\n",
    "        2: (1, 3),\n",
    "        3: (1, 4),\n",
    "        4: (1, 5),\n",
    "        5: (1, 6),\n",
    "        6: (1, 7),\n",
    "        7: (1, 8),\n",
    "        8: (1, 9),\n",
    "        9: (1, 10),\n",
    "        10: (2, 1),\n",
    "        11: (2, 2),\n",
    "        12: (2, 3),\n",
    "        13: (2, 4),\n",
    "        14: (2, 5),\n",
    "        15: (2, 6),\n",
    "        16: (2, 7),\n",
    "        17: (2, 8),\n",
    "        18: (2, 9),\n",
    "        19: (2, 10)\n",
    "    }\n",
    "    reverse_label_map_dict = dict([(value, key) for key, value in label_map_dict.items()])\n",
    "    df_train_val_final['y'] = df_train_val_final['y'].apply(lambda x: reverse_label_map_dict[x])\n",
    "    label_age = to_categorical(df_train_val_final['age'] - 1)\n",
    "    label_gender = to_categorical(df_train_val_final['gender'] - 1)\n",
    "    label_y = to_categorical(df_train_val_final['y'])\n",
    "\n",
    "    # 转成numpy. 用于kfold\n",
    "    dataset_train_x = {}\n",
    "    dataset_test_x = {}\n",
    "\n",
    "    for col in agg_col + ['attention_mask'] + ['click_times_length'] + ['click_times_prob'] + ['stats_feat']:\n",
    "        dataset_train_x[col] = np.stack(df_train_val_final[col].values)\n",
    "        dataset_test_x[col] = np.stack(df_test_final[col].values)\n",
    "\n",
    "    dataset_train_y = {\n",
    "        'age_out': label_age,\n",
    "        'gender_out': label_gender,\n",
    "        'y_out': label_y\n",
    "    }\n",
    "\n",
    "    embedding_group = {}\n",
    "    for col in agg_col:\n",
    "        nb_words = len(word_indexs[col]) + 1\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "        count = 0\n",
    "        for word, i in tqdm(word_indexs[col].items()):\n",
    "            try:\n",
    "                embedding_vector = wv_group[col][word]\n",
    "            except KeyError:\n",
    "                embedding_vector = np.zeros(embed_size)\n",
    "                count += 1\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        embedding_group[col] = embedding_matrix\n",
    "        logger.info(\"col: %s, null cnt: %s\" % (col, count))\n",
    "\n",
    "    del wv_group, word_indexs\n",
    "    gc.collect()\n",
    "\n",
    "    # generate folds\n",
    "    folds = []\n",
    "    if not os.path.exists('./temp/folder.pkl'):\n",
    "        skf = StratifiedKFold(n_splits=5, random_state=1011, shuffle=True)\n",
    "        count = 0\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(df_train_val_final, df_train_val_final['y'])):\n",
    "            folds.append((train_index, test_index))\n",
    "        f = open(\"./temp/folder.pkl\", \"wb\")\n",
    "        pickle.dump(folds, f)\n",
    "        f.close()\n",
    "    else:\n",
    "        f = open(\"./temp/folder.pkl\", \"rb\")\n",
    "        folds = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    # train\n",
    "    # model params\n",
    "    config = BertConfig(\n",
    "        vocab_size=None,\n",
    "        hidden_size=128,  # for transformer, should be same as emb_dim\n",
    "        num_hidden_layers=1,  # for transformer\n",
    "        num_attention_heads=8,\n",
    "        intermediate_size=256,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=max_len,\n",
    "        type_vocab_size=1,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "    )\n",
    "    cur_time = datetime.now().isoformat()\n",
    "    age_sub_weight, age_sub, gender_sub_weight, gender_sub = train(\n",
    "                                                                    dataset_train_x,\n",
    "                                                                    dataset_test_x,\n",
    "                                                                    df_test_final,\n",
    "                                                                    label_age,\n",
    "                                                                    label_gender,\n",
    "                                                                    folds,\n",
    "                                                                    config,\n",
    "                                                                    embedding_group,\n",
    "                                                                   )\n",
    "    age_weight_result = age_sub_weight\n",
    "    age_result = age_sub\n",
    "    gender_weight_result = gender_sub_weight\n",
    "    gender_result = gender_sub\n",
    "    np.save('./model/age_result_5zhe_weight_transformer_%s' % cur_time, age_weight_result)\n",
    "    np.save('./model/age_result_5zhe_transformer_%s' % cur_time, age_result)\n",
    "    np.save('./model/gender_result_5zhe_weight_transformer_%s' % cur_time, gender_weight_result)\n",
    "    np.save('./model/gender_result_5zhe_transformer_%s' % cur_time, gender_result)\n",
    "    logger.info('train has been completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

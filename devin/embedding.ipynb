{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: tensorflow-gpu==2.2 in /opt/conda/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (1.17.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (3.12.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (1.29.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (0.2.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (1.4.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (3.2.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (0.3.3)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (2.2.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (0.33.6)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (0.9.0)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==2.2) (2.2.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow-gpu==2.2) (41.4.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.6.0.post3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (0.4.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (4.1.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.24.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.5.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow-gpu==2.2 -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root_path ='./data/train_preliminary/'\n",
    "train_ad_path = os.path.join(train_root_path,'ad.csv')\n",
    "train_click_path = os.path.join(train_root_path,'click_log.csv')\n",
    "train_user_path = os.path.join(train_root_path,'user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root_path = './data/test/'\n",
    "test_ad_path = os.path.join(test_root_path,'ad.csv')\n",
    "test_click_path = os.path.join(test_root_path,'click_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_click_train = pd.read_csv(train_click_path,dtype= {'creative_id':str,'click_times':str,'time':str})\n",
    "df_click_test = pd.read_csv(test_click_path,dtype= {'creative_id':str,'click_times':str,'time':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ad_train = pd.read_csv(train_ad_path,na_values='\\\\N', \n",
    "                          dtype= {'creative_id':str,'ad_id': str, 'product_id': str, 'product_category': str,'advertiser_id': str,'industry': str} )\n",
    "df_ad_test = pd.read_csv(test_ad_path,na_values='\\\\N',\n",
    "                         dtype= {'creative_id':str,'ad_id': str, 'product_id': str, 'product_category': str,'advertiser_id': str,'industry': str} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_click_ad_train = df_click_train.merge(df_ad_train,on=['creative_id'],how='inner')\n",
    "df_click_ad_test = df_click_test.merge(df_ad_test,on=['creative_id'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_click_all =  pd.concat([df_click_ad_train,df_click_ad_test], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_click_all = df_click_all.sort_values('time', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_click_all.fillna('un',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=10\n",
    "# embed_size = 128\n",
    "embed_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "class epoch_logger(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "        \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcol = ['creative_id','ad_id','product_id','product_category','advertiser_id','industry','click_times','time']\n",
    "func = lambda x: ' '.join(x.tolist())\n",
    "doc = df_click_all.groupby(['user_id']).agg(\n",
    "    {'creative_id':func,'ad_id':func,'product_id':func,'product_category':func,'advertiser_id':func,'industry':func,'click_times':func,'time':func}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [19:59<00:00, 149.95s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "new_df ={}\n",
    "new_df['user_id'] = doc['user_id']\n",
    "word_index ={}\n",
    "tokenizer = None\n",
    "for col in tqdm(aggcol):\n",
    "    tokenizer = Tokenizer(lower=False, char_level=False, split=' ')\n",
    "    tokenizer.fit_on_texts(doc[col])\n",
    "    train_x = tokenizer.texts_to_sequences(doc[col])\n",
    "    train_x = pad_sequences(train_x, maxlen=maxlen, value=0,padding='post')\n",
    "    new_df[col] = list(train_x)\n",
    "    word_index[col] =tokenizer.word_index\n",
    "df_final = pd.DataFrame(new_df)\n",
    "df_final.to_parquet('./model/cache/final_padding.parquet_8input',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"./model/cache/word_index.pkl\",\"wb\")\n",
    "pickle.dump(word_index,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: list(x)\n",
    "# group_df = df_click_all.groupby(['user_id']).agg({'ad_id':func,'product_id':func,'product_category':func,'advertiser_id':func,'industry':func,'click_times':func,'time':func})\n",
    "group_df = df_click_all.groupby(['user_id']).agg({'creative_id':func})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creative_id embeding:\n",
      "Epoch #0 start\n",
      "Loss after epoch 0: 38568428.0\n",
      "Epoch #1 start\n",
      "Loss after epoch 1: 19379080.0\n",
      "Epoch #2 start\n",
      "Loss after epoch 2: 10448660.0\n",
      "Epoch #3 start\n",
      "Loss after epoch 3: 2495392.0\n",
      "Epoch #4 start\n",
      "Loss after epoch 4: 2431224.0\n",
      "Epoch #5 start\n",
      "Loss after epoch 5: 2368592.0\n",
      "Epoch #6 start\n",
      "Loss after epoch 6: 2228960.0\n",
      "Epoch #7 start\n",
      "Loss after epoch 7: 2179912.0\n",
      "Epoch #8 start\n",
      "Loss after epoch 8: 2111304.0\n",
      "Epoch #9 start\n",
      "Loss after epoch 9: 2049736.0\n"
     ]
    }
   ],
   "source": [
    "#product_docs = df_click_all.groupby(['user_id'])['product_id'].apply(list).reset_index(name='product_list')\n",
    "# agg_col = ['ad_id','product_id','product_category','advertiser_id','industry','click_times','time']\n",
    "# agg_col = ['ad_id','product_id','product_category','advertiser_id','industry']\n",
    "agg_col = ['creative_id']\n",
    "for col in agg_col:\n",
    "    print(f'Start {col} embeding:')\n",
    "#     w2v_model = Word2Vec(group_df[col], sg=1,window=window,min_count=1,size=embed_size,iter=5,compute_loss=True,callbacks=[epoch_logger()])\n",
    "#     w2v_model.wv.save(f\"./model/word2vec_{col}_128.model\")\n",
    "    w2v_model = Word2Vec(group_df[col], sg=1,window=window,min_count=1,size=embed_size,iter=10,workers=20, compute_loss=True,callbacks=[epoch_logger()])\n",
    "    w2v_model.wv.save(f\"./model/word2vec_{col}_200.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# click_time 分箱处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_click_all['click_times'] = df_click_all['click_times'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[1, 2, 3, 6, 10,19]\n",
    "def buckize(x):\n",
    "    if x ==1:\n",
    "        return 1\n",
    "    if x ==2:\n",
    "        return 2\n",
    "    if x >=3 and x<6:\n",
    "        return 3\n",
    "    if x >=6 and x<10:\n",
    "        return 4\n",
    "    if x >=10 and x<19:\n",
    "        return 5\n",
    "    return 6\n",
    "df_click_all['click_times_buck'] = df_click_all['click_times'].apply(buckize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_click_time = df_click_all.groupby(['user_id'])['click_times_buck'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>click_times_buck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1899995</td>\n",
       "      <td>3999996</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1899996</td>\n",
       "      <td>3999997</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1899997</td>\n",
       "      <td>3999998</td>\n",
       "      <td>[1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1899998</td>\n",
       "      <td>3999999</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1899999</td>\n",
       "      <td>4000000</td>\n",
       "      <td>[2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1900000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id                                   click_times_buck\n",
       "0              1            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n",
       "1              2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "2              3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "3              4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "4              5  [1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "...          ...                                                ...\n",
       "1899995  3999996  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, ...\n",
       "1899996  3999997  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "1899997  3999998  [1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "1899998  3999999  [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, ...\n",
       "1899999  4000000  [2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "\n",
       "[1900000 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_click_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_click = pad_sequences(df_click_time['click_times_buck'], maxlen=80, value=0,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df ={}\n",
    "new_df['user_id'] = df_click_time['user_id']\n",
    "new_df['click_times_buck'] =  list(train_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_parquet('./model/cache/click_times_buck.parquet',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepwalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepwalk(log,f1,f2,flag,L):\n",
    "    #Deepwalk算法，\n",
    "    print(\"deepwalk:\",f1,f2)\n",
    "    #构建图\n",
    "    dic={}\n",
    "    for item in log[[f1,f2]].values:\n",
    "        try:\n",
    "            str(int(item[1]))\n",
    "            str(int(item[0]))\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            dic['item_'+str(int(item[1]))].add('user_'+str(int(item[0])))\n",
    "        except:\n",
    "            dic['item_'+str(int(item[1]))]=set(['user_'+str(int(item[0]))])\n",
    "        try:\n",
    "            dic['user_'+str(int(item[0]))].add('item_'+str(int(item[1])))\n",
    "        except:\n",
    "            dic['user_'+str(int(item[0]))]=set(['item_'+str(int(item[1]))])\n",
    "    dic_cont={}\n",
    "    for key in dic:\n",
    "        dic[key]=list(dic[key])\n",
    "        dic_cont[key]=len(dic[key])\n",
    "    print(\"creating\")     \n",
    "    #构建路径\n",
    "    path_length=10        \n",
    "    sentences=[]\n",
    "    length=[]\n",
    "    for key in dic:\n",
    "        sentence=[key]\n",
    "        while len(sentence)!=path_length:\n",
    "            key=dic[sentence[-1]][random.randint(0,dic_cont[sentence[-1]]-1)]\n",
    "            if len(sentence)>=2 and key == sentence[-2]:\n",
    "                break\n",
    "            else:\n",
    "                sentence.append(key)\n",
    "        sentences.append(sentence)\n",
    "        length.append(len(sentence))\n",
    "        if len(sentences)%100000==0:\n",
    "            print(len(sentences))\n",
    "    print(np.mean(length))\n",
    "    print(len(sentences))\n",
    "    #训练Deepwalk模型\n",
    "    print('training...')\n",
    "    random.shuffle(sentences)\n",
    "    model = Word2Vec(sentences, size=L, window=4,min_count=1,sg=1, workers=10,iter=20)\n",
    "    print('outputing...')\n",
    "    #输出\n",
    "    values=set(log[f1].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        try:\n",
    "            a=[int(v)]\n",
    "            a.extend(model['user_'+str(int(v))])\n",
    "            w2v.append(a)\n",
    "        except:\n",
    "            pass\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f1]\n",
    "    for i in range(L):\n",
    "        names.append(f1+'_'+ f2+'_'+names[0]+'_deepwalk_embedding_'+str(L)+'_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/' +f1+'_'+ f2+'_'+f1 +'_'+flag +'_deepwalk_'+str(L)+'.pkl') \n",
    "    ########################\n",
    "    values=set(log[f2].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        try:\n",
    "            a=[int(v)]\n",
    "            a.extend(model['item_'+str(int(v))])\n",
    "            w2v.append(a)\n",
    "        except:\n",
    "            pass\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f2]\n",
    "    for i in range(L):\n",
    "        names.append(f1+'_'+ f2+'_'+names[0]+'_deepwalk_embedding_'+str(L)+'_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/' +f1+'_'+ f2+'_'+f2 +'_'+flag +'_deepwalk_'+str(L)+'.pkl') \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
